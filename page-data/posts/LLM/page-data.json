{"componentChunkName":"component---src-templates-category-template-js","path":"/posts/LLM","result":{"pageContext":{"currentCategory":"LLM","categories":["All","NLP","LLM","ASAC","Paper","Reflections(ML/DL)","Algorithm"],"edges":[{"node":{"id":"d023c61d-c7ed-5cce-a75f-62754442ed31","excerpt":"참고 : 테디노트의 RAG 비법노트 소스코드: https://github.com/teddylee777/langchain-kr   오늘도 작성하는데 Pydantic 파싱 따로 공부한 것 좀 끄적이고 가겠습니다. 레츠기릿!   PydanticOutputParser BaseModel/Field, partial_variables, 결과 포맷 지시사항, parser 등을 알아야 합니다. 예시로 바로 봅시다. 1)  / 는 무엇인가? : “출력 데이터는 반드시 이 구조여야 한다”를 정의하는 스키마(계약서) 필드 누락, 타입 불일치, 구조 깨진다? → 즉시 에러 발생하도록 합니다. : 각 필드의 의미/제약(설명, 기본값 등)을 붙이는 메타데이터 LLM에게 “이 필드는 이런 의미”라고 알려줘서 출력 품질을 올려줄 수 있습니다. 2) 를 넣으면 항상 형식이 맞나? 아닙니다. 프롬프트에 지침 문자열을 미리 채워 넣어주는 기능이라, LLM이 지침을 따를 확률만 높여줄 뿐입니다. ⭐️ 진짜 강제는 par…","fields":{"slug":"/25-12-13_1/"},"frontmatter":{"categories":"NLP LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-3","date":"December 13, 2025"}},"next":{"fields":{"slug":"/25-12-12_1/"}},"previous":null},{"node":{"id":"16028044-feb8-52fb-8dfc-47bb74183808","excerpt":"참고 : 테디노트의 RAG 비법노트 소스코드: https://github.com/teddylee777/langchain-kr   오늘도 가즈아..!!!!\n새로운 걸 알아가는 건 정말 즐겁다~ 다만 알게된 것을 {정리}하는 것은 별개일 뿐… O—< 거두절미. 레츠고~!   LCEL 인터페이스 지난 글에서는 invoke(), stream() 메서드를 살펴봤는데\n사실 batch() 메서드도 있다. batch(): 배치 처리 리스트에 input_variables의 값들을 갖는 딕셔너리를 모아서 각각 배치로 chain에 던진다.\nmodel을 거치고 파싱을 거치면서 각 배치 결과의 content들을 리스트에 묶어서 저장된다. 참고로 config 딕셔너리에서 max_concurrency 키의 값을 설정하여 동시 처리할 수 있는 최대 작업 수를 정해줄 수 있다. 즉, 한번에 배치를 여러 개 처리를 할 수 있고 결과는 똑같이 모든 배치를 묶어서 정리되어 있다.   비동기 메소드 어떤 작업을 기다리…","fields":{"slug":"/25-12-12_1/"},"frontmatter":{"categories":"NLP LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-2","date":"December 12, 2025"}},"next":{"fields":{"slug":"/25-12-11_1/"}},"previous":{"fields":{"slug":"/25-12-13_1/"}}},{"node":{"id":"66d916a8-fe23-5bc2-a843-b9c1486dac94","excerpt":"참고 : 테디노트의 RAG 비법노트 소스코드: https://github.com/teddylee777/langchain-kr   지난 번 토큰화 관련해서 공부하고 블로그도 썼지만, 관련 내용 이해와 내면화 사이의 괴리가 있었다.\n그런데, 갓 테디노트님 강의 들으면서 그 괴리를 좁혀나갈 수 있었던 것 같다^^< 굳👍🏻 (오늘 게시글은 진짜 두서없이 깨알 Tip 넣을 것임다… 너무 뜬금 없어도 이해부탁…)   서브워드 기반 토큰화 왜 하는 것인가? 문자 기반 토큰화(한국어는 자모 단위로 토큰화)는 너무 쪼개니까 편집거리를 이용해 검색어 추천, 오타 교정 정도에 활용할 수 있는데, 텍스트 문장 생성에서는 너무 많이 분절해놓으니 이렇게 쪼갠 것을 모델이 학습하고 또 생성해내기가 어려워진다.(모델이 생성해야하는 묶음이 더 많아짐!!)\n(우리한테 자모음들을 막 뿌려 주고, 텍스트 문장 만들라고 시켜보면 얼마나 힘든가…) 단어 기반 토큰화는 텍스트 문장 생성에 효과적일 수 있다지만,지난 블로그 …","fields":{"slug":"/25-12-11_1/"},"frontmatter":{"categories":"NLP LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-1","date":"December 11, 2025"}},"next":{"fields":{"slug":"/25-09-09_1/"}},"previous":{"fields":{"slug":"/25-12-12_1/"}}}]}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}