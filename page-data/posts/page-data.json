{"componentChunkName":"component---src-templates-category-template-js","path":"/posts","result":{"pageContext":{"currentCategory":"All","edges":[{"node":{"id":"0dfe15fa-7a6e-5ab9-8ce0-881477c61b57","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘은 LangGraph 기능들 좀 더 알아보고자 합니다.[26-01-17 게시글][https://byeonwoojung.github.io/26-01-17_1/]에서 좀 얘기를 하긴 했었지만 너무 줄줄이 적어나간 글인 것 같아 오늘은 좀 더 정리해서 올리고자 합니다.   Human-in-the-loop 3가지만 기억합시다. HumanRequest 툴과 human_node 노드(실제 실행되는 함수) 생성 interrupt_before 설정 그래프 상태 업데이트 & 계속 호출   HumanRequest 툴과 human_node 노드(실제 실행되는 함수) 생성 HumanRequest는 LLM에게 docstring을 주…","fields":{"slug":"/26-02-15_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-15","date":"February 15, 2026"}},"next":{"fields":{"slug":"/26-01-30_1/"}},"previous":null},{"node":{"id":"43e4eb72-86c7-5bf4-b823-9d74506fda6f","excerpt":"오늘은 제가 진행하고 있는 프로젝트에서RAG 성능 평가를 진행했던 부분을 다루고자 합니다.   우선 저는, 유튜브 콘텐츠의 자동 생성 자막 등을 이용한Agent 프로젝트 (자세한 건 아직 비밀 🤫)를 진행 중에 있습니다. 이 데이터를 벡터 임베딩을 통해 RAG 개발을 하고 있는데,개발하면서 했던 여러 고민들을 정리해보고자 합니다.   첫번째 고민: ASR 오류 어떻게 해결하지? 유튜브 콘텐츠 자동 생성 자막은 ASR(Automatic Speech Recognition, 자동 음성 인식)을 통해 만들어지기 때문에 오류가 있습니다. 이는 주변 배경음이나 발화자의 발음, STT 모델의 성능 등의 이유로 발생하는 오류인데이를 곧바로 RAG 개발에 이용하면 문제가 차암 많아지겠죠. 그래서 LLM을 이용해 데이터 전처리를 해보자 마음을 먹었습니다. Phase 1. 자막 교정 시도해보자. ASR 오류에서 ”문자의 형태학적 오류로 인한 의미적 차이를 분명하게 차이가 있었을 때” 문제가 생깁니다.…","fields":{"slug":"/26-01-30_1/"},"frontmatter":{"categories":"LLM RAG","title":"[LLM] Contextual Retrieval과 RAG 평가 💡 (feat. RAGAS, 쯔동 프로젝트)","date":"January 30, 2026"}},"next":{"fields":{"slug":"/26-01-17_1/"}},"previous":{"fields":{"slug":"/26-02-15_1/"}}},{"node":{"id":"9873a81c-6667-5bd3-856c-57c33af2f27a","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   글이 자주 올라오지 않는 것은RAG와 Agent의 내용들은 프로젝트에 녹여서 끄적이고자 그렇습니다,, 오늘은 아아아주 정말 메모장st 느낌의 글입니다. 다소 짧고 구조적이지 않습니다. 레츠기릿,,   create_sql_query_chain DB에서 특정 값을 조회하기 위한 SQL 쿼리를 생성하고, 쿼리를 실행하는 방법에 대해 정리하고자 합니다. 간단합니다. SQL 쿼리를 생성하는 chain 생성 (DB를 연결하고, 쿼리 생성하는 LLM 선정) 쿼리를 실행하는 도구 생성 두 체인 연결 create_query_chain의 결과가 execute_query의 query 변수에 들어가서 db에 조회합니다.     추가 …","fields":{"slug":"/26-01-17_1/"},"frontmatter":{"categories":"LLM RAG","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-14","date":"January 17, 2026"}},"next":{"fields":{"slug":"/26-01-13_1/"}},"previous":{"fields":{"slug":"/26-01-30_1/"}}},{"node":{"id":"34d120bf-0552-506b-8640-779280088570","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘은 정말 뜬금없이 갖가지 내용들을 끄적이고자 합니다..@chain 데코레이터, Configurable, Route 등에 대해 정리하고자 합니다. 레츠기릿   @chain 데코레이터 으로 가져와서 데코레이터를 이용해 함수를 으로 변환하도록 합시다. 즉, 은  객체이기 때문에 LCEL 인터페이스에 따라  메서드 등을 활용할 수 있습니다. chain.get_graph().print_ascii()로 체인의 그래프 출력 가능( 그래프 그리는 라이브러리 설치해야 함) 로 체인의 프롬프트 확인 가능 Helper 함수와 Wrapper 함수 Configurable 에 로 전달하는 딕셔너리()에서 주로 쓰는 약속된 키가 몇 가…","fields":{"slug":"/26-01-13_1/"},"frontmatter":{"categories":"LLM RAG","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-13","date":"January 13, 2026"}},"next":{"fields":{"slug":"/26-01-11_1/"}},"previous":{"fields":{"slug":"/26-01-17_1/"}}},{"node":{"id":"73d60c32-da40-5d8c-adea-c9b7352ed91c","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오랜만에 왔습니다!! (열심히 공부하고 왔습니다 🫡) Text Embedding와 Vectorstore, Retriver, Reranker는 워낙 모델이 많아서 모두 각각 정리하는 것보다는RAG 개발 시 각 단계별로 고려할 점들을 정리해보는 것이 좋지 않을까 해서 가져왔습니다. 고럼,, 레츠기릿~!   RAG 개발 단계별 고려할 점 각 단계에서 고려할 점들을 제 생각대로 정리해보았습니다. 떠오르는 부분들만 적어두었는데, 추가적으로 고려할 점들이 있다면 알려주시면 감사하겠습니다 😊 1. 데이터 로드 데이터에 따라 로드 방식 설정 PDF 등의 이미지의 문서의 경우 텍스트 OCR 모델 선정   2. 텍스트 데이터 임베…","fields":{"slug":"/26-01-11_1/"},"frontmatter":{"categories":"LLM RAG","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-12 (RAG 개발 고려할 점)","date":"January 11, 2026"}},"next":{"fields":{"slug":"/26-01-01_1/"}},"previous":{"fields":{"slug":"/26-01-13_1/"}}},{"node":{"id":"69725161-4cf6-5273-b75f-c77780d8edb4","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘도 와쓰요~오늘은 문서 로드 방식과 청킹에 대해 정리해보고자 합니다. 바로 레츠기릿~!   문서 로더 (Loader) 문서 로드 방식을 간단하게 정리하고자 합니다. Document 생성 문서 로드  메서드  메서드: 를 설정해 줌  메서드: 각 문서를 generator 방식으로 읽으면서 버림(문서의 양이 많을 때 유용함)  메서드: 비동기 방식으로 문서를 로드함   문서 로드 방식은 파일 형식이 어떠하든 그 방식이 비슷합니다. 파일 형식에 맞게 loader 설정 후,,  등의 메서드를 이용하여 로드하면 됩니다.     청킹 (Chunking) RAG에서 텍스트를 청킹하는 것은 중요합니다. 아. 청킹은 문서 내…","fields":{"slug":"/26-01-01_1/"},"frontmatter":{"categories":"LLM RAG","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-11","date":"January 01, 2026"}},"next":{"fields":{"slug":"/25-12-31_1/"}},"previous":{"fields":{"slug":"/26-01-11_1/"}}},{"node":{"id":"16d4151c-c0f0-5202-8651-703654b9bb3c","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   이번에는 ’LCEL에서 메모리 사용하는 방법‘을 간단히 정리하고,’SQLite DB를 이용한 메모리 저장하는 방법‘을 정리해보고자 합니다. 상당히 복잡해보여서 연습이 중요할 것 같습니다. (주의‼️ 마음의 준비를 하고, 심호흡 한번 크게 하고 들어가야 합니다.) 레츠기릿~!   LCEL에서 메모리 사용하는 방법 프롬프트 템플릿에  박아두기!  메모리 생성하면서의 값과 에 들어가는  값을 동일하게 하기 메모리에서 지정한 는 프롬프트 템플릿에서 이전 대화 자리에 박아두는 변수명과 같아야 함 에서 을 에 지정한 을 만들어 chain에서 이용함 는 와 같은 것임 즉,  키의 값에 대화들 리스트를 담은 딕셔너리일 것임 그리…","fields":{"slug":"/25-12-31_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-10","date":"December 31, 2025"}},"next":{"fields":{"slug":"/25-12-30_1/"}},"previous":{"fields":{"slug":"/26-01-01_1/"}}},{"node":{"id":"e502e6d7-98be-5c28-a38e-553cae829030","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘은 대화 메모리를 저장하는 방법에 대해 끄적끄적 하고자 하는데몇 가지만 다루고자 합니다.   그 전에, chain에서 retriever에 커스텀 함수 연결도 가능하다는 적어두고 가겠습니다. 💡깨알 Tip chain에서 retriever에 커스텀 함수 연결도 가능합니다. 이때 함수에 전달하는 인자는 1개여야 하는데,인자가 여러 개일 때는 wrapper 함수를 통해 전달 가능합니다. 이런 식으로  결과를 커스텀 함수 에 연결해줄 수 있습니다. 이때 전달하는 인자는   1개만 있기 때문에 연결이 가능합니다.   그럼, 본격적으로 레츠기릿~!     ConversationKGMemory: 지식 그래프 형태 기억 우선…","fields":{"slug":"/25-12-30_1/"},"frontmatter":{"categories":"LLM RAG","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-9","date":"December 30, 2025"}},"next":{"fields":{"slug":"/25-12-25_1/"}},"previous":{"fields":{"slug":"/25-12-31_1/"}}},{"node":{"id":"cd4dc7a1-3e81-5f0a-aec7-f809be6b0269","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘은 맥 사용자(저요,,,)의 허깅페이스 모델을 로컬로 가져와mps를 이용해 GPU 가속화하는 방법을 먼저 보고 가겠습니다~ 레츠기리잇~!   Mac(MPS) 환경에서 HuggingFacePipeline 사용 시 주의사항  vs  직접 주입 LangChain을 사용하여 로컬 LLM을 구동할 때, 특히 Apple Silicon(M1/M2/M3) 환경에서 GPU 가속(MPS)을 설정하는 과정에서 가 발생하는 경우가 많습니다. 이는 LangChain이 내부적으로 Hugging Face 모델을 로드하는 방식()과 실제  라이브러리의 이 장치(Device)를 처리하는 방식의 차이 때문입니다.    소속 라이브러리:  (…","fields":{"slug":"/25-12-25_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-8","date":"December 25, 2025"}},"next":{"fields":{"slug":"/25-12-24_1/"}},"previous":{"fields":{"slug":"/25-12-30_1/"}}},{"node":{"id":"cc4dedd9-03dd-5854-bb4d-e9e42ae1ce11","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘은 캐시 방법에 대해 정리해보고자 합니다. 레츠기륏~!     Cache 1. InMemory Cache (인메모리 캐시) LLM 호출에서 인메모리 캐시를 사용한다면,동일한 질문이 들어왔을 때 LLM(OpenAI 등) 서버로 요청을 전달하지 않고, 메모리에 미리 저장해둔 답변을 즉시 꺼내어 응답합니다. (노트북 커널 재시작하면 캐시가 삭제됩니다.) 그렇기에, LLM 호출 비용은 들지 않습니다. 하지만, 질문이 약간 바뀌면(띄어쓰기 하나라도) 다시 호출하게 됩니다. 그 이유는 는 내부적으로 Dictionary(Hash Map) 구조를 사용하기 때문입니다. 위 코드를 작성한 후에 LLM 호출하게 되면 질문과 답변…","fields":{"slug":"/25-12-24_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-7","date":"December 24, 2025"}},"next":{"fields":{"slug":"/25-12-23_1/"}},"previous":{"fields":{"slug":"/25-12-25_1/"}}},{"node":{"id":"0a5507c6-4f1e-5505-b7fb-c556a92f5d07","excerpt":"오늘은 Dense Retriever와 Sparse Retriever에 대해 정리해보고자 합니다. 레츠기릿~!     Dense Retriever vs Sparse Retriever 임베딩(Embedding) Dense Retriever는 텍스트를 고차원 공간의 **밀집 벡터(Dense Vector)**로 변환합니다. Sparse Vector: 단어 사전 전체 크기의 벡터 중 대부분이 0인 형태 (단어 중복 위주) Dense Vector: 보통 768차원이나 1024차원의 고정된 크기 안에 모든 숫자가 의미 있는 수치로 채워진 형태 구분 Sparse Retriever (BM25 등) Dense Retriever (DPR 등) 매칭 방식 키워드 중심 (Exact Match) 문맥 및 의미 중심 (Semantic Match) 특징 “사과”가 포함된 문서를 잘 찾음 “애플”이나 “과일”이라는 단어도 문맥상 이해 장점 빠르고, 도메인 지식 없이도 안정적 동의어 처리와 추상적인 질문 답변에 …","fields":{"slug":"/25-12-23_1/"},"frontmatter":{"categories":"LLM RAG","title":"Dense Retriever와 Sparse Retriever","date":"December 23, 2025"}},"next":{"fields":{"slug":"/25-12-22_1/"}},"previous":{"fields":{"slug":"/25-12-24_1/"}}},{"node":{"id":"bdf921ea-25f0-5e43-8d59-d6039447df3e","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   지난 번에 OutputParser는 필요 시 다루고자 했는데제 성격상 바로 또 남겨놔야해서.. 오늘은 나머지 OutputParser 끄적이고 가겠습니다.. 레츠고오!!   이외의 OutputParser 요약해서 적어두고자 합니다. 를 chain에 달아 스트리밍 출력할 때는 각 응답 1개씩을 리스트로 변환하는 경우가 있다. (모든 응답을 묶어서 리스트로 반환하지 않음)   는  클래스를 이용해 정의하고  메서드를 이용해 파서 초기화를 해준다. 로컬과 덜 강력한 모델에 유용한 파서라고 한다.   는 Pydantic 파서에서 했던 방식과 같이 데이터 구조를 클래스를 이용해 정의해주고  파라미터 값에 해당 클래스명 값을…","fields":{"slug":"/25-12-22_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-6","date":"December 22, 2025"}},"next":{"fields":{"slug":"/25-12-21_1/"}},"previous":{"fields":{"slug":"/25-12-23_1/"}}},{"node":{"id":"905bdc07-3be5-56b6-b025-f3fdad7ea637","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘은 출력 파서에 대해 알아보고자 합니다. 며칠 글이 올라오지 않았던 것은streamlit 활용한 UI에서 사용자가 프롬프트 템플릿을 선택하고 업로드한 PDF를 기반해 질문하면, PDF를 파싱해서 RAG를 구현하는 미니 프로젝트를 했었습니다. RAG 개발 관련한 게시글은 좀 더 깊이 있게 공부한 후에 올리고자 합니다. 고럼 레츠고~!     LLM은 수렴보다 발산을 더 잘하기 떄문에 응답의 포맷을 정해주는 것이 중요합니다.그래서 LLM의 답변을 원하는 구조대로 강제하기 위해 출력 파서를 이용하는 것이 좋습니다. 저는 Pydantic 파싱 방법을 많이 이용했던 것 같습니다.   PydanticOutputParse…","fields":{"slug":"/25-12-21_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-5","date":"December 21, 2025"}},"next":{"fields":{"slug":"/25-12-14_1/"}},"previous":{"fields":{"slug":"/25-12-22_1/"}}},{"node":{"id":"fe49cd49-a75e-5770-ae92-ec14391a764a","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘도 갓 테디노트님 끄적 레츠기릿.   그 전에 잠시!!딕셔너리 키워드 인자 전달하는 파이썬 문법 확인하고 가고자 합니다~ example의 첫번째 에서 언패킹하는 방법으로 PromptTemplate.from_template에 키워드 인자를 전달하고, 그 키워드를 직접 사용해 프롬프트 템플릿을 만들 수 있습니다.   그럼 진짜 레츠기륏!   FewShotPromptTemplate LLM에게 예시 몇 가지를 을 활용해 던져주는 방법을 알아봅시다. 사실, 프롬프트에 예시를 직접 포함해도 되지만 예시를 선택적으로 삽입하는 모듈과 함께 쓰면 좋기에 활용성이 괜찮습니다. 에는  예시들을 예시 프롬프트 템플릿에 각 채워 나…","fields":{"slug":"/25-12-14_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-4","date":"December 14, 2025"}},"next":{"fields":{"slug":"/25-12-13_1/"}},"previous":{"fields":{"slug":"/25-12-21_1/"}}},{"node":{"id":"d023c61d-c7ed-5cce-a75f-62754442ed31","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘도 작성하는데 Pydantic 파싱 따로 공부한 것 좀 끄적이고 가겠습니다. 레츠기릿!   PydanticOutputParser BaseModel/Field, partial_variables, 결과 포맷 지시사항, parser 등을 알아야 합니다. 예시로 바로 봅시다. 1)  / 는 무엇인가? : 상속받는 부모 모델 (여기서는 pydantic 라이브러리의 BaseModel 모듈을 상속 받는 것임) : 각 필드의 의미/제약(설명, 기본값 등)을 붙이는 메타데이터 LLM에게 “이 필드는 이런 의미”라고 알려줘서 출력 품질을 올려줄 수 있습니다. 보통 아래와 같이 작성합니다.   2) 는 무엇인가? 미리 프롬프트에…","fields":{"slug":"/25-12-13_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-3","date":"December 13, 2025"}},"next":{"fields":{"slug":"/25-12-12_1/"}},"previous":{"fields":{"slug":"/25-12-14_1/"}}},{"node":{"id":"16028044-feb8-52fb-8dfc-47bb74183808","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘도 가즈아..!!!!\n새로운 걸 알아가는 건 정말 즐겁다~ 다만 알게된 것을 {정리}하는 것은 별개일 뿐… O—< 거두절미. 레츠고~!   LCEL 인터페이스 지난 글에서는 ,  메서드를 살펴봤는데\n사실  메서드도 있다. batch(): 배치 처리 리스트에 input_variables의 값들을 갖는 딕셔너리를 모아서 각각 배치로 chain에 던진다.\nmodel을 거치고 파싱을 거치면서 각 배치 결과의 content들을 리스트에 묶어서 저장된다. 참고로 config 딕셔너리에서  키의 값을 설정하여 동시 처리할 수 있는 최대 작업 수를 정해줄 수 있다.즉, 한번에 배치를 여러 개 처리를 할 수 있고, 결과는 똑…","fields":{"slug":"/25-12-12_1/"},"frontmatter":{"categories":"LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-2","date":"December 12, 2025"}},"next":{"fields":{"slug":"/25-12-11_1/"}},"previous":{"fields":{"slug":"/25-12-13_1/"}}},{"node":{"id":"66d916a8-fe23-5bc2-a843-b9c1486dac94","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   지난 번 토큰화 관련해서 공부하고 블로그도 썼지만, 관련 내용 이해와 내면화 사이의 괴리가 있었다.\n그런데, 갓 테디노트님 강의 들으면서 그 괴리를 좁혀나갈 수 있었던 것 같다^^< 굳👍🏻 (오늘 게시글은 진짜 두서없이 깨알 Tip 넣을 것임다… 너무 뜬금 없어도 이해부탁…)   서브워드 기반 토큰화 왜 하는 것인가? 문자 기반 토큰화(한국어는 자모 단위로 토큰화)는 너무 쪼개니까 편집거리를 이용해 검색어 추천, 오타 교정 정도에 활용할 수 있는데, 텍스트 문장 생성에서는 너무 많이 분절해놓으니 이렇게 쪼갠 것을 모델이 학습하고 또 생성해내기가 어려워진다.(모델이 생성해야하는 묶음이 더 많아짐!!)\n(우리한테 자모…","fields":{"slug":"/25-12-11_1/"},"frontmatter":{"categories":"NLP LLM","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-1","date":"December 11, 2025"}},"next":{"fields":{"slug":"/25-09-09_1/"}},"previous":{"fields":{"slug":"/25-12-12_1/"}}},{"node":{"id":"510447e4-7933-5a0e-b6aa-8ea9e7284203","excerpt":"참고 : 윤대희 외. (2023). 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습. 위키북스. 소스코드: https://github.com/wikibook/pytorchtrf 위의 교재와 소스코드를 참고하였으며, 대부분의 내용은 직접 찾아보며 학습하였습니다.   자연어 처리에서 토큰화하는 것에 대해 알아보자. 토큰을 나누는 기준은 공백 분할, 정규표현식 사용, 어휘사전 적용, 머신러닝 활용하는 방법이 있다. 어휘 사전을 구축할 때, 너무 크게 구축하면 차원의 저주에 빠지고, 너무 작게 구축하면 OOV(Out of Vocab) 존재 가능성이 있으므로 그 크기를 적절히 정해야 하며, 출현빈도는 고려되지만 순서관계는 표현하지 못한다는 점을 기억하자.   토큰화 라이브러리 jamo 라이브러리 h2j (Hangul to Jamo): 한글 → 자모(자음과 모음 / 한글 글자를 초성,중성,종성 단위로 쪼갬) 변환한다.\n자모 단위로 쪼개었을 때 → 음운단위 학습 가능해져, 희귀단…","fields":{"slug":"/25-09-09_1/"},"frontmatter":{"categories":"NLP","title":"[NLP] 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습 -토큰화","date":"September 09, 2025"}},"next":{"fields":{"slug":"/25-07-04_1/"}},"previous":{"fields":{"slug":"/25-12-11_1/"}}},{"node":{"id":"1dd3c51a-53a4-5cbb-9e43-79be0b45f237","excerpt":"GitHub 레포: https://github.com/byeonwoojung/youtube-playlist-MLproject   들어가며(?) 2025년 7월 3일, 생애 처음으로 학회에서 논문 발표를 했다. ”섬네일·제목·오디오 기반 통합적 유튜브 플레이리스트 조회수 예측“이라는 주제로 한국디지털콘텐츠학회 하계종합학술대회에 참가했다. ASAC 교육과정에서 발표를 위해 1개월 정도 준비했고, 이후 딥러닝 프로젝트, 기업연계 프로젝트와 병행하면서(진짜 너무 힘듦 ㅠ) 약 5~6주 동안 디벨롭해 학회 논문 발표를 진행했다.. 프로젝트 2개씩 병행하니 잠을 거의 못 자며 준비했었다,,, 정말 발표 때가 오니, 약간 긴장도 되고 설레기도 했다.(사실 긴장 안함🥶) 진짜 마감 직전까지 수정한…ㅠ 학회 논문은 내겐 아픈 손가락이다.. 더 잘할 수 있었을 텐데 아쉬움이 많이 남는다..\n(다른 분들은 원치 않으실 수 있으니 가렸음다,,ㅎ)   프로젝트를 시작한 계기 평소 유튜브 플레이리스트 영상…","fields":{"slug":"/25-07-04_1/"},"frontmatter":{"categories":"ASAC","title":"[ASAC 회고] 한국디지털콘텐츠학회 논문 발표 후기 (2025.07.03)","date":"July 04, 2025"}},"next":{"fields":{"slug":"/25-04-06_1/"}},"previous":{"fields":{"slug":"/25-09-09_1/"}}},{"node":{"id":"e7fe0ccc-c7d8-5340-a138-7745c28d065c","excerpt":"이번 주에는 많은 일들이 있었다(매주 많을 것 같지만..?). 그동안 파이썬 알고리즘 문제를 풀면서 그 문법들을 익혔고, 이번 주부터는 파이썬 알고리즘을 마무리한 후 MySQL에서 원하는 데이터를 읽어보는 방법을 학습했다. 그런데 MySQL까지 진도를 다 나갔고… 무려 금요일에 Python 알고리즘과 MySQL 데이터로 결과값 찾는 테스트까지 봤다!!! (미친 속도🔥) 이번 주부터 나의 주도로 만든 Python 알고리즘 스터디도 시작되었다. 스터디는 일주일에 2번 진행됐고 스터디 내에서 선정한 알고리즘 문제를 자신의 난이도에 맞게 선택하여 풀어오는 방식으로 진행되었다. 몇몇 분들과 함께 공공데이터를 활용한 데이터 분석 및 AI 관련 공모전도 나가기로 해서 관련 회의와 데이터 서칭을 했다. 정신 없는 일주일. 회고를 시작해보자.   ‘시점’에 대한 정의로 알고리즘 문제해결 그동안 구현과 그리디, DFS/BFS 위주로 알고리즘 문제를 풀고 학습했다. 먼저 문제에 대한 이해를 완벽히 한…","fields":{"slug":"/25-04-06_1/"},"frontmatter":{"categories":"ASAC","title":"[ASAC 회고] 3주차: 도전","date":"April 06, 2025"}},"next":{"fields":{"slug":"/25-03-29_1/"}},"previous":{"fields":{"slug":"/25-07-04_1/"}}},{"node":{"id":"18dbe677-fcc3-5b08-b72c-e5e49614e26b","excerpt":"시작 25년 3월 19일 수요일 ASAC 빅데이터 분석가 교육 과정이 시작됐다. 그동안 혼자 파이썬 기본 문법부터 시작해서 SQL, 머신러닝의 전통적 모델들을 하루 9~12시간(?)씩 공부해 왔다. ’내 속도에 맞게, 궁금한 것은 깊이 파면서’ 공부하는 습관대로 공부를 해왔는데 이 방향성이 맞을지에 대한 고민이 많았다. (아래는 공부했던 내용 중의 일부다.)   엉덩이 무겁게 공부하는 것에는 자신이 있었지만 조금 더 방향성을 잘 잡고, 효율적으로 공부하기 위해 SK 플래닛에서 진행하는 ASAC 과정에 지원하게 되었다.   1주차 공부하는 것이 즐겁다. (미친 소리 같겠지만..) 정확히는 무언가에 대해 공부할 때 깊이 파는 걸 선호한다. aka. 대충하지 않는다. 간단한 예시를 들면 [Hands-on machine learning] (이하 생략)을 공부할 때 이해한 것을 모두 다 적느라 주석이 좀 많다..  또 다른 예시를 보여주면…  이해가 안 되면, GPT를 괴롭히거나 그림을 직…","fields":{"slug":"/25-03-29_1/"},"frontmatter":{"categories":"ASAC","title":"[ASAC 회고] 1-2주차: 미러링","date":"March 29, 2025"}},"next":{"fields":{"slug":"/25-03-15_1/"}},"previous":{"fields":{"slug":"/25-04-06_1/"}}},{"node":{"id":"432191ef-a5a1-521e-9966-40d179d9dfe6","excerpt":"논문 : Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery, 2(2), 121-167.   최근 핸즈온 머신러닝 교재에서 SVM(Support Vector Machine)에 대해 공부하면서 논문을 읽어보고 싶었다. Survey 논문으로 시작하는 것이 좋을 것 같아 논문을 서칭해 ‘패턴 인식을 위한 서포트 벡터 머신 튜토리얼’이라는 주제의 논문을 선택했다. 논문을 읽었을 때 해당 기술에 대한 배경과 의도를 알 수 있어서 좋다. 그 배경을 알면 그 활용법을 조금 더 이해하지 않을까 기대를 한다. 바로 레츠고오.   Abstract 이 논문은 키워드를 ’서포트 벡터 머신’, ’통계적 학습 이론’, ’VC 차원’, ’패턴 인식‘로 선택했다. 이 중에 VC 차원에 대해 먼저 알고 가는 것이 좋을 것 같다.   VC 차원(…","fields":{"slug":"/25-03-15_1/"},"frontmatter":{"categories":"Paper","title":"[논문 리뷰] A Tutorial on Support Vector Machines for Pattern Recognition (1)","date":"March 16, 2025"}},"next":{"fields":{"slug":"/25-03-14_1/"}},"previous":{"fields":{"slug":"/25-03-29_1/"}}},{"node":{"id":"afd5a24d-7f38-5ab5-a413-5f8ff667a157","excerpt":"참고 : Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow 2판, 오렐리앙 제롱, 한빛미디어. 소스코드 참고 : https://github.com/codingalzi/handson-ml2   빅데이터 관련 곧 박사 졸업하는 지인께서 기본부터 잘 쌓아야 이 분야에서 오래 살아남을 수 있다는 이야기로,\nHands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (앞으로 핸즈온이라고 부를 것임) 이라는 교재를 정독하게 되었다. 교재를 보고 공부를 시작한지는 3달이 넘어가는데, 깃허브 블로그에 대해 잘 알지 못해서 노션에 정리만 해두고 그동안 묵혀 두었다.(본 교재는 현재 5장 읽는 중이다..) 최근 자격증 시험(ADsP, SQLD), 알고리즘 공부, ML 인터넷 강의 수강 등으로 정리 속도가 느려졌는데, 더 속도를 내어 완독하자아아앗!!   참고로, 노션에 정리한 모든…","fields":{"slug":"/25-03-14_1/"},"frontmatter":{"categories":"Reflections(ML/DL)","title":"Hands-on Machine learning 1장 회고(24년 말 콘퍼런스 이야기를 곁들인)","date":"March 14, 2025"}},"next":{"fields":{"slug":"/25-03-12_1/"}},"previous":{"fields":{"slug":"/25-03-15_1/"}}},{"node":{"id":"d5d5bef9-d525-50f6-84fe-53507b823a15","excerpt":"출처 : https://www.acmicpc.net/problem/7576 백준 7576번 토마토   문제 철수의 토마토 농장에서는 토마토를 보관하는 큰 창고를 가지고 있다. 토마토는 아래의 그림과 같이 격자 모양 상자의 칸에 하나씩 넣어서 창고에 보관한다. img 창고에 보관되는 토마토들 중에는 잘 익은 것도 있지만, 아직 익지 않은 토마토들도 있을 수 있다. 보관 후 하루가 지나면, 익은 토마토들의 인접한 곳에 있는 익지 않은 토마토들은 익은 토마토의 영향을 받아 익게 된다. 하나의 토마토의 인접한 곳은 왼쪽, 오른쪽, 앞, 뒤 네 방향에 있는 토마토를 의미한다. 대각선 방향에 있는 토마토들에게는 영향을 주지 못하며, 토마토가 혼자 저절로 익는 경우는 없다고 가정한다. 철수는 창고에 보관된 토마토들이 며칠이 지나면 다 익게 되는지, 그 최소 일수를 알고 싶어 한다. 토마토를 창고에 보관하는 격자모양의 상자들의 크기와 익은 토마토들과 익지 않은 토마토들의 정보가 주어졌을 때, 며…","fields":{"slug":"/25-03-12_1/"},"frontmatter":{"categories":"Algorithm","title":"[백준] 7576번 토마토 (Gold 5, Python 파이썬)","date":"March 12, 2025"}},"next":{"fields":{"slug":"/25-03-11_1/"}},"previous":{"fields":{"slug":"/25-03-14_1/"}}},{"node":{"id":"d8a0e52d-1659-50f3-829a-8a40745bcc7c","excerpt":"출처 : https://www.acmicpc.net/problem/14502 백준 14502번 연구소   문제 인체에 치명적인 바이러스를 연구하던 연구소에서 바이러스가 유출되었다. 다행히 바이러스는 아직 퍼지지 않았고, 바이러스의 확산을 막기 위해서 연구소에 벽을 세우려고 한다. 연구소는 크기가 N×M인 직사각형으로 나타낼 수 있으며, 직사각형은 1×1 크기의 정사각형으로 나누어져 있다. 연구소는 빈 칸, 벽으로 이루어져 있으며, 벽은 칸 하나를 가득 차지한다. 일부 칸은 바이러스가 존재하며, 이 바이러스는 상하좌우로 인접한 빈 칸으로 모두 퍼져나갈 수 있다. 새로 세울 수 있는 벽의 개수는 3개이며, 꼭 3개를 세워야 한다. 예를 들어, 아래와 같이 연구소가 생긴 경우를 살펴보자. 이때, 0은 빈 칸, 1은 벽, 2는 바이러스가 있는 곳이다. 아무런 벽을 세우지 않는다면, 바이러스는 모든 빈 칸으로 퍼져나갈 수 있다. 2행 1열, 1행 2열, 4행 6열에 벽을 세운다면 지도의 모…","fields":{"slug":"/25-03-11_1/"},"frontmatter":{"categories":"Algorithm","title":"[백준] 14502번 연구소 (Gold 4, Python 파이썬)","date":"March 11, 2025"}},"next":null,"previous":{"fields":{"slug":"/25-03-12_1/"}}}],"categories":["All","LLM","RAG","NLP","ASAC","Paper","Reflections(ML/DL)","Algorithm"]}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}