{"componentChunkName":"component---src-templates-blog-template-js","path":"/25-12-24_1/","result":{"data":{"cur":{"id":"cc4dedd9-03dd-5854-bb4d-e9e42ae1ce11","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy\">https://fastcampus.co.kr/data_online_teddy</a>)</p>\n<p>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a></p>\n<p> </p>\n<p>오늘은 캐시 방법에 대해 정리해보고자 합니다.</p>\n<p>레츠기륏~!</p>\n<p> </p>\n<p> </p>\n<h2 id=\"cache\" style=\"position:relative;\"><a href=\"#cache\" aria-label=\"cache permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cache</h2>\n<hr>\n<h3 id=\"1-inmemory-cache-인메모리-캐시\" style=\"position:relative;\"><a href=\"#1-inmemory-cache-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BA%90%EC%8B%9C\" aria-label=\"1 inmemory cache 인메모리 캐시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. InMemory Cache (인메모리 캐시)</h3>\n<p>LLM 호출에서 인메모리 캐시를 사용한다면,<br>동일한 질문이 들어왔을 때 LLM(OpenAI 등) 서버로 요청을 전달하지 않고, <strong>메모리에 미리 저장해둔 답변을 즉시 꺼내어 응답합니다.</strong> (노트북 커널 재시작하면 캐시가 삭제됩니다.)</p>\n<p>그렇기에, <strong>LLM 호출 비용은 들지 않습니다</strong>.</p>\n<p>하지만, <strong>질문이 약간 바뀌면(띄어쓰기 하나라도) 다시 호출하게 됩니다.</strong> 그 이유는 <code class=\"language-text\">InMemoryCache</code>는 내부적으로 <strong>Dictionary(Hash Map) 구조를 사용</strong>하기 때문입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> InMemoryCache\n\nset_llm_cache<span class=\"token punctuation\">(</span>InMemoryCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 인메모리 캐시 설정</span></code></pre></div>\n<p>위 코드를 작성한 후에 LLM 호출하게 되면 질문과 답변을 메모리에 저장해두게 됩니다.</p>\n<p> </p>\n<h3 id=\"2-semantic-cache-code-classlanguage-textredissemanticcachecode-등\" style=\"position:relative;\"><a href=\"#2-semantic-cache-code-classlanguage-textredissemanticcachecode-%EB%93%B1\" aria-label=\"2 semantic cache code classlanguage textredissemanticcachecode 등 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Semantic Cache (<code class=\"language-text\">RedisSemanticCache</code> 등)</h3>\n<p><code class=\"language-text\">InMemoryCache</code>는 질문이 약간 바뀔 때 LLM을 다시 호출해야 하는 단점이 있었습니다.</p>\n<p>반면에, <strong><code class=\"language-text\">Semantic Cache</code>는 질문이 바뀌어도 유사한 질문을 찾아 저장된 캐시를 사용한다는 특징을 가집니다.</strong></p>\n<p>다만, <strong>유사도 계산 시 임베딩 모델을 이용해야 하므로 비용이 들 수 있습니다.</strong></p>\n<table>\n<thead>\n<tr>\n<th><strong>구분</strong></th>\n<th><strong>일반 인메모리 캐시 (InMemoryCache)</strong></th>\n<th><strong>시맨틱 캐시 (RedisSemanticCache 등)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>비교 방식</strong></td>\n<td>문자열 완전 일치 (Exact Match)</td>\n<td><strong>벡터 유사도 비교 (Similarity Match)</strong></td>\n</tr>\n<tr>\n<td><strong>유연성</strong></td>\n<td>띄어쓰기, 조사 하나만 틀려도 실패</td>\n<td>“날씨 어때?”와 “날씨 알려줘”를 같게 인식 가능</td>\n</tr>\n<tr>\n<td><strong>비용</strong></td>\n<td>없음</td>\n<td>유사도 계산을 위한 임베딩 모델 호출 비용 발생</td>\n</tr>\n</tbody>\n</table>\n<p>아래 코드와 같이 작성 후, LLM 호출하게 되면 캐시를 저장하고 이후에는 저장된 캐시를 통해 답변을 하게 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> RedisSemanticCache\n<span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token comment\"># 1. 시맨틱 캐시는 '유사도'를 측정해야 하므로 임베딩 모델이 반드시 필요합니다.</span>\nembeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 시맨틱 캐시 설정 (Redis 사용 예시)</span>\nset_llm_cache<span class=\"token punctuation\">(</span>RedisSemanticCache<span class=\"token punctuation\">(</span>\n    redis_url<span class=\"token operator\">=</span><span class=\"token string\">\"redis://localhost:6379\"</span><span class=\"token punctuation\">,</span>\n    embedding<span class=\"token operator\">=</span>embeddings<span class=\"token punctuation\">,</span>\n    score_threshold<span class=\"token operator\">=</span><span class=\"token number\">0.1</span>  <span class=\"token comment\"># 이 점수가 낮을수록 더 '비슷해야' 캐시를 반환합니다.</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>여기서는 <strong>Localhost에 설치된 Redis 데이터베이스</strong>의 메모리에 저장하게 됩니다.</p>\n<p> </p>\n<h3 id=\"3-sqlite-cache\" style=\"position:relative;\"><a href=\"#3-sqlite-cache\" aria-label=\"3 sqlite cache permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. SQLite Cache</h3>\n<p>SQLite Cache는  별도의 데이터베이스 서버 설치 없이, <strong>내 컴퓨터의 하드디스크에 파일(<code class=\"language-text\">.db</code>) 형태로 데이터를 저장</strong>하는 비휘발성 캐싱 방식입니다. (당연히 비휘발성입니다.)</p>\n<p>LangChain에서 <code class=\"language-text\">InMemoryCache</code>의 휘발성 문제를 해결하면서도, <code class=\"language-text\">Redis</code>처럼 복잡한 서버 설정이 부담스러울 때 가장 많이 사용하는 <strong>가장 간편한 영구 저장용 캐시</strong>입니다.</p>\n<p><code class=\"language-text\">InMemoryCache</code>와 동일하게 질문이 <strong>완전히 동일</strong>해야 캐시를 사용합니다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> SQLiteCache\n<span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">import</span> os\n\n<span class=\"token comment\"># 캐시 디렉토리를 생성합니다.</span>\n<span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span><span class=\"token string\">\"cache\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span><span class=\"token string\">\"cache\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># SQLiteCache를 사용합니다.</span>\nset_llm_cache<span class=\"token punctuation\">(</span>SQLiteCache<span class=\"token punctuation\">(</span>database_path<span class=\"token operator\">=</span><span class=\"token string\">\"cache/llm_cache.db\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 코드를 작성 후,  LLM 호출하게 되면 로컬에 db 형태로 캐시를 저장하고, 이후 저장된 캐시를 통해 답변을 하게 됩니다.</p>\n<p> </p>\n<p> </p>\n<p>여기서 끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#cache\">Cache</a></p>\n<ul>\n<li><a href=\"#1-inmemory-cache-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BA%90%EC%8B%9C\">1. InMemory Cache (인메모리 캐시)</a></li>\n<li><a href=\"#2-semantic-cache-redissemanticcache-%EB%93%B1\">2. Semantic Cache (<code class=\"language-text\">RedisSemanticCache</code> 등)</a></li>\n<li><a href=\"#3-sqlite-cache\">3. SQLite Cache</a></li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy) 소스코드: https://github.com/teddylee777/langchain-kr   오늘은 캐시 방법에 대해 정리해보고자 합니다. 레츠기륏~!     Cache 1. InMemory Cache (인메모리 캐시) LLM 호출에서 인메모리 캐시를 사용한다면,동일한 질문이 들어왔을 때 LLM(OpenAI 등) 서버로 요청을 전달하지 않고, 메모리에 미리 저장해둔 답변을 즉시 꺼내어 응답합니다. (노트북 커널 재시작하면 캐시가 삭제됩니다.) 그렇기에, LLM 호출 비용은 들지 않습니다. 하지만, 질문이 약간 바뀌면(띄어쓰기 하나라도) 다시 호출하게 됩니다. 그 이유는 는 내부적으로 Dictionary(Hash Map) 구조를 사용하기 때문입니다. 위 코드를 작성한 후에 LLM 호출하게 되면 질문과 답변을 메모리에 저장해두게 됩니다.   2. Semantic Cach…","frontmatter":{"date":"December 24, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-7","categories":"LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-24_1/"}},"next":{"id":"0a5507c6-4f1e-5505-b7fb-c556a92f5d07","html":"<p>오늘은 Dense Retriever와 Sparse Retriever에 대해 정리해보고자 합니다.</p>\n<p>레츠기릿~!</p>\n<p> </p>\n<p> </p>\n<h2 id=\"dense-retriever-vs-sparse-retriever\" style=\"position:relative;\"><a href=\"#dense-retriever-vs-sparse-retriever\" aria-label=\"dense retriever vs sparse retriever permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dense Retriever vs Sparse Retriever</h2>\n<hr>\n<h3 id=\"임베딩embedding\" style=\"position:relative;\"><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9embedding\" aria-label=\"임베딩embedding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>임베딩(Embedding)</h3>\n<p>Dense Retriever는 텍스트를 고차원 공간의 **밀집 벡터(Dense Vector)**로 변환합니다.</p>\n<ul>\n<li><strong>Sparse Vector:</strong> 단어 사전 전체 크기의 벡터 중 대부분이 0인 형태 (단어 중복 위주)</li>\n<li><strong>Dense Vector:</strong> 보통 768차원이나 1024차원의 고정된 크기 안에 모든 숫자가 의미 있는 수치로 채워진 형태</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th><strong>구분</strong></th>\n<th><strong>Sparse Retriever (BM25 등)</strong></th>\n<th><strong>Dense Retriever (DPR 등)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>매칭 방식</strong></td>\n<td>키워드 중심 (Exact Match)</td>\n<td>문맥 및 의미 중심 (Semantic Match)</td>\n</tr>\n<tr>\n<td><strong>특징</strong></td>\n<td>“사과”가 포함된 문서를 잘 찾음</td>\n<td>“애플”이나 “과일”이라는 단어도 문맥상 이해</td>\n</tr>\n<tr>\n<td><strong>장점</strong></td>\n<td>빠르고, 도메인 지식 없이도 안정적</td>\n<td>동의어 처리와 추상적인 질문 답변에 강함</td>\n</tr>\n<tr>\n<td><strong>단점</strong></td>\n<td>오타나 유의어 처리에 취약함</td>\n<td>학습 데이터가 많이 필요하고 계산 비용이 큼</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"왜-dense-retriever를-쓸까\" style=\"position:relative;\"><a href=\"#%EC%99%9C-dense-retriever%EB%A5%BC-%EC%93%B8%EA%B9%8C\" aria-label=\"왜 dense retriever를 쓸까 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>왜 Dense Retriever를 쓸까?</h3>\n<ol>\n<li><strong>미세한 의미 차이 파악:</strong> “파이썬 설치 방법”과 “Python 인스톨 가이드”가 같은 의미임을 이해합니다.</li>\n<li><strong>질문-답변 성능 향상:</strong> RAG(검색 증강 생성) 시스템에서 LLM이 답변하기 가장 좋은 문맥을 가져오는 데 탁월합니다.</li>\n</ol>\n<h3 id=\"대표적인-모델-dpr-dense-passage-retrieval\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%AA%A8%EB%8D%B8-dpr-dense-passage-retrieval\" aria-label=\"대표적인 모델 dpr dense passage retrieval permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표적인 모델: DPR (Dense Passage Retrieval)</h3>\n<p>Meta(구 Facebook)에서 제안한 모델로, 두 개의 BERT 모델을 각각 질문용과 문서용 인코더로 학습시켜 성능을 극대화한 방식이 가장 유명합니다.</p>\n<h3 id=\"한계와-보완-hybrid-search\" style=\"position:relative;\"><a href=\"#%ED%95%9C%EA%B3%84%EC%99%80-%EB%B3%B4%EC%99%84-hybrid-search\" aria-label=\"한계와 보완 hybrid search permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>한계와 보완 (Hybrid Search)</h3>\n<p>Dense Retriever는 학습 데이터에 없는 특수한 고유명사나 품번(예: “A-1234”) 같은 수치 매칭에는 약할 수 있습니다. 그래서 최근에는 <strong>Sparse와 Dense를 섞은 Hybrid Search</strong> 방식을 실무에서 가장 많이 사용합니다.</p>\n<blockquote>\n<p>최근에는 고성능 벡터 데이터베이스(Chroma, Pinecone, FAISS 등)를 사용하여 수백만 개의 Dense Vector 중에서 가장 유사한 것을 0.1초 내외로 검색할 수 있게 되었습니다.</p>\n</blockquote>\n<p> </p>\n<p> </p>\n<h3 id=\"예제로-확인\" style=\"position:relative;\"><a href=\"#%EC%98%88%EC%A0%9C%EB%A1%9C-%ED%99%95%EC%9D%B8\" aria-label=\"예제로 확인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>예제로 확인</h3>\n<p>Sparse Retriever와 Dense Retriever를 예제를 통해서 직접 비교해보겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>documents <span class=\"token keyword\">import</span> Document\n\n<span class=\"token comment\"># 1. 예시 데이터 (문서 청크)</span>\ntexts <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">\"아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"갤럭시 S24 울트라는 AI 번역 기능을 제공합니다.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"점심 메뉴로 김치찌개와 제육볶음이 인기입니다.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># '아이폰' 단어 없음</span>\n<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># LangChain Document 객체로 변환</span>\ndocuments <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>Document<span class=\"token punctuation\">(</span>page_content<span class=\"token operator\">=</span>t<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> texts<span class=\"token punctuation\">]</span>\ndocuments\n<span class=\"token triple-quoted-string string\">\"\"\"출력:\n[Document(metadata={}, page_content='아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.'),\n Document(metadata={}, page_content='갤럭시 S24 울트라는 AI 번역 기능을 제공합니다.'),\n Document(metadata={}, page_content='점심 메뉴로 김치찌개와 제육볶음이 인기입니다.'),\n Document(metadata={}, page_content='애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.')]\n\"\"\"</span></code></pre></div>\n<p>LangChain Document 객체로 변환한 후에,<br>검색어 ”<strong>아이폰 15 프로 충전</strong>“에 대한 <strong>두 Retriever 결과를 비교</strong>해보겠습니다.</p>\n<p> </p>\n<ol>\n<li>\n<p><strong>BM25Retriever - Sparse Retriever 예시</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>retrievers <span class=\"token keyword\">import</span> BM25Retriever\n\n<span class=\"token comment\"># 1. Sparse Retriever (BM25)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"--- [A. Sparse Retriever (BM25)] ---\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># BM25Retriever 객체 생성</span>\nsparse_retriever <span class=\"token operator\">=</span> BM25Retriever<span class=\"token punctuation\">.</span>from_documents<span class=\"token punctuation\">(</span>documents<span class=\"token punctuation\">)</span>\nsparse_retriever<span class=\"token punctuation\">.</span>k <span class=\"token operator\">=</span> <span class=\"token number\">1</span>  <span class=\"token comment\"># 상위 1개만 검색</span>\n\n<span class=\"token comment\"># 검색어: \"아이폰 15 프로 충전\"</span>\nsparse_result <span class=\"token operator\">=</span> sparse_retriever<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"아이폰 15 프로 충전\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"검색어: '아이폰 15 프로 충전'\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"결과: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>sparse_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>page_content<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"출력:\n--- [A. Sparse Retriever (BM25)] ---\n검색어: '아이폰 15 프로 충전'\n결과: 아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\n\"\"\"</span></code></pre></div>\n</li>\n</ol>\n<ul>\n<li>\n<p><strong>검색어 분해(토큰화):</strong> <code class=\"language-text\">[\"아이폰\", \"15\", \"프로\", \"충전\"]</code> (4개의 키워드)</p>\n</li>\n<li>\n<p><strong>첫번째와 네번째 문서가 유력한 후보</strong>인데</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\"아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\",\n...\n\"애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.\"</code></pre></div>\n</li>\n<li>\n<p><strong>BM25</strong>는 <strong>1) 토큰이 문서 전체에서 얼마나 희귀한지, 2) 토큰이 각 문서에서 얼마나 자주 등장하는지(빈도 포화 계산을 통해 너무 많이 등장하면 더이상 점수가 오르지 않음), 3) 각 문서 길이는 얼마나 짧은지</strong> 를 계산하여 점수를 매깁니다.</p>\n<ul>\n<li>이때 첫번째 문서 <code class=\"language-text\">\"아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\"</code>가 <code class=\"language-text\">아이폰</code>, <code class=\"language-text\">15</code>, <code class=\"language-text\">프로</code>가 여러 개 겹치면서 다른 문서들과 비교했을 때 토큰들의 희귀성들이 큰 차이 없고 문서 길이도 큰 차이 없기 때문에</li>\n<li><strong>Sparse Retriever인 BM25Retriever는 첫번째 문서를 가져오게 됩니다.</strong></li>\n</ul>\n</li>\n</ul>\n<p> </p>\n<ol start=\"2\">\n<li><strong>허깅페이스의 jhgan/ko-sroberta-multitask 모델 - Dense Retriever 예시</strong></li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>vectorstores <span class=\"token keyword\">import</span> FAISS\n<span class=\"token keyword\">from</span> langchain_huggingface <span class=\"token keyword\">import</span> HuggingFaceEmbeddings\n\n<span class=\"token comment\"># 2. Dense Retriever (Vector Store)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"--- [B. Dense Retriever (Vector/FAISS)] ---\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 임베딩 모델 로드 (한국어 성능 좋은 모델)</span>\nembedding_model <span class=\"token operator\">=</span> HuggingFaceEmbeddings<span class=\"token punctuation\">(</span>model_name<span class=\"token operator\">=</span><span class=\"token string\">\"jhgan/ko-sroberta-multitask\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 벡터 저장소(VectorStore) 생성 후 Retriever로 변환</span>\nvectorstore <span class=\"token operator\">=</span> FAISS<span class=\"token punctuation\">.</span>from_documents<span class=\"token punctuation\">(</span>documents<span class=\"token punctuation\">,</span> embedding_model<span class=\"token punctuation\">)</span>\ndense_retriever <span class=\"token operator\">=</span> vectorstore<span class=\"token punctuation\">.</span>as_retriever<span class=\"token punctuation\">(</span>search_kwargs<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"k\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 검색어: \"아이폰 15 프로 충전\"</span>\ndense_result <span class=\"token operator\">=</span> dense_retriever<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"아이폰 15 프로 충전\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"검색어: '아이폰 15 프로 충전'\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"결과: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>dense_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>page_content<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token triple-quoted-string string\">\"\"\"출력:\n--- [B. Dense Retriever (Vector/FAISS)] ---\n검색어: '아이폰 15 프로 충전'\n결과: 애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.\n\"\"\"</span></code></pre></div>\n<ul>\n<li><strong>검색어 벡터:</strong> **<code class=\"language-text\">아이폰 15 프로 충전</code>**을 <strong>아이폰 스마트폰의 특정 기종의 충전과 관련한 의미</strong>를 가진 좌표로 변환할 것임</li>\n<li><strong>네번째 문서가 첫번째 문서보다 특별하게 <code class=\"language-text\">충전</code>이라는 핵심 의도가 가까우므로 네번째 문서를 가져오게 됩니다.</strong></li>\n</ul>\n<p> </p>\n<p> </p>\n<p>Dense Retriever과 같은 Semantic Retriever은<br>벡터 검색은 “의미”를 너무 중시한 나머지 “정확성”을 놓칠 때가 많습니다.</p>\n<p>특히, 고유명사를 무시하는 경우가 존재하여<br>최근에는 하이브리드로 Retriever를 이용하는 경우가 많다고 합니다.</p>\n<p>이것은 다음에 다루도록 해보겠습니다.</p>\n<p>그럼 이번 글은 여기까지 끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#dense-retriever-vs-sparse-retriever\">Dense Retriever vs Sparse Retriever</a></p>\n<ul>\n<li><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9embedding\">임베딩(Embedding)</a></li>\n<li><a href=\"#%EC%99%9C-dense-retriever%EB%A5%BC-%EC%93%B8%EA%B9%8C\">왜 Dense Retriever를 쓸까?</a></li>\n<li><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%AA%A8%EB%8D%B8-dpr-dense-passage-retrieval\">대표적인 모델: DPR (Dense Passage Retrieval)</a></li>\n<li><a href=\"#%ED%95%9C%EA%B3%84%EC%99%80-%EB%B3%B4%EC%99%84-hybrid-search\">한계와 보완 (Hybrid Search)</a></li>\n<li><a href=\"#%EC%98%88%EC%A0%9C%EB%A1%9C-%ED%99%95%EC%9D%B8\">예제로 확인</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"December 23, 2025","title":"Dense Retriever와 Sparse Retriever","categories":"LLM RAG","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-23_1/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://www.zoomkoding.com","comments":{"utterances":{"repo":"https://github.com/byeonwoojung/byeonwoojung.github.io"}}}}},"pageContext":{"slug":"/25-12-24_1/","nextSlug":"/25-12-23_1/","prevSlug":""}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}