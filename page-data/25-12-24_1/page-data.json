{"componentChunkName":"component---src-templates-blog-template-js","path":"/25-12-24_1/","result":{"data":{"cur":{"id":"cc4dedd9-03dd-5854-bb4d-e9e42ae1ce11","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy\">https://fastcampus.co.kr/data_online_teddy</a>)</p>\n<p>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a></p>\n<p>위키독스: <a href=\"https://wikidocs.net/book/14314\">https://wikidocs.net/book/14314</a></p>\n<p> </p>\n<p>오늘은 캐시 방법에 대해 정리해보고자 합니다.</p>\n<p>레츠기륏~!</p>\n<p> </p>\n<p> </p>\n<h2 id=\"cache\" style=\"position:relative;\"><a href=\"#cache\" aria-label=\"cache permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cache</h2>\n<hr>\n<h3 id=\"1-inmemory-cache-인메모리-캐시\" style=\"position:relative;\"><a href=\"#1-inmemory-cache-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BA%90%EC%8B%9C\" aria-label=\"1 inmemory cache 인메모리 캐시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. InMemory Cache (인메모리 캐시)</h3>\n<p>LLM 호출에서 인메모리 캐시를 사용한다면,<br>동일한 질문이 들어왔을 때 LLM(OpenAI 등) 서버로 요청을 전달하지 않고, <strong>메모리에 미리 저장해둔 답변을 즉시 꺼내어 응답합니다.</strong> (노트북 커널 재시작하면 캐시가 삭제됩니다.)</p>\n<p>그렇기에, <strong>LLM 호출 비용은 들지 않습니다</strong>.</p>\n<p>하지만, <strong>질문이 약간 바뀌면(띄어쓰기 하나라도) 다시 호출하게 됩니다.</strong> 그 이유는 <code class=\"language-text\">InMemoryCache</code>는 내부적으로 <strong>Dictionary(Hash Map) 구조를 사용</strong>하기 때문입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> InMemoryCache\n\nset_llm_cache<span class=\"token punctuation\">(</span>InMemoryCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 인메모리 캐시 설정</span></code></pre></div>\n<p>위 코드를 작성한 후에 LLM 호출하게 되면 질문과 답변을 메모리에 저장해두게 됩니다.</p>\n<p> </p>\n<h3 id=\"2-semantic-cache-code-classlanguage-textredissemanticcachecode-등\" style=\"position:relative;\"><a href=\"#2-semantic-cache-code-classlanguage-textredissemanticcachecode-%EB%93%B1\" aria-label=\"2 semantic cache code classlanguage textredissemanticcachecode 등 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Semantic Cache (<code class=\"language-text\">RedisSemanticCache</code> 등)</h3>\n<p><code class=\"language-text\">InMemoryCache</code>는 질문이 약간 바뀔 때 LLM을 다시 호출해야 하는 단점이 있었습니다.</p>\n<p>반면에, <strong><code class=\"language-text\">Semantic Cache</code>는 질문이 바뀌어도 유사한 질문을 찾아 저장된 캐시를 사용한다는 특징을 가집니다.</strong></p>\n<p>다만, <strong>유사도 계산 시 임베딩 모델을 이용해야 하므로 비용이 들 수 있습니다.</strong></p>\n<table>\n<thead>\n<tr>\n<th><strong>구분</strong></th>\n<th><strong>일반 인메모리 캐시 (InMemoryCache)</strong></th>\n<th><strong>시맨틱 캐시 (RedisSemanticCache 등)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>비교 방식</strong></td>\n<td>문자열 완전 일치 (Exact Match)</td>\n<td><strong>벡터 유사도 비교 (Similarity Match)</strong></td>\n</tr>\n<tr>\n<td><strong>유연성</strong></td>\n<td>띄어쓰기, 조사 하나만 틀려도 실패</td>\n<td>“날씨 어때?”와 “날씨 알려줘”를 같게 인식 가능</td>\n</tr>\n<tr>\n<td><strong>비용</strong></td>\n<td>없음</td>\n<td>유사도 계산을 위한 임베딩 모델 호출 비용 발생</td>\n</tr>\n</tbody>\n</table>\n<p>아래 코드와 같이 작성 후, LLM 호출하게 되면 캐시를 저장하고 이후에는 저장된 캐시를 통해 답변을 하게 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> RedisSemanticCache\n<span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token comment\"># 1. 시맨틱 캐시는 '유사도'를 측정해야 하므로 임베딩 모델이 반드시 필요합니다.</span>\nembeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 시맨틱 캐시 설정 (Redis 사용 예시)</span>\nset_llm_cache<span class=\"token punctuation\">(</span>RedisSemanticCache<span class=\"token punctuation\">(</span>\n    redis_url<span class=\"token operator\">=</span><span class=\"token string\">\"redis://localhost:6379\"</span><span class=\"token punctuation\">,</span>\n    embedding<span class=\"token operator\">=</span>embeddings<span class=\"token punctuation\">,</span>\n    score_threshold<span class=\"token operator\">=</span><span class=\"token number\">0.1</span>  <span class=\"token comment\"># 이 점수가 낮을수록 더 '비슷해야' 캐시를 반환합니다.</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>여기서는 <strong>Localhost에 설치된 Redis 데이터베이스</strong>의 메모리에 저장하게 됩니다.</p>\n<p> </p>\n<h3 id=\"3-sqlite-cache\" style=\"position:relative;\"><a href=\"#3-sqlite-cache\" aria-label=\"3 sqlite cache permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. SQLite Cache</h3>\n<p>SQLite Cache는  별도의 데이터베이스 서버 설치 없이, <strong>내 컴퓨터의 하드디스크에 파일(<code class=\"language-text\">.db</code>) 형태로 데이터를 저장</strong>하는 비휘발성 캐싱 방식입니다. (당연히 비휘발성입니다.)</p>\n<p>LangChain에서 <code class=\"language-text\">InMemoryCache</code>의 휘발성 문제를 해결하면서도, <code class=\"language-text\">Redis</code>처럼 복잡한 서버 설정이 부담스러울 때 가장 많이 사용하는 <strong>가장 간편한 영구 저장용 캐시</strong>입니다.</p>\n<p><code class=\"language-text\">InMemoryCache</code>와 동일하게 질문이 <strong>완전히 동일</strong>해야 캐시를 사용합니다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> SQLiteCache\n<span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">import</span> os\n\n<span class=\"token comment\"># 캐시 디렉토리를 생성합니다.</span>\n<span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span><span class=\"token string\">\"cache\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span><span class=\"token string\">\"cache\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># SQLiteCache를 사용합니다.</span>\nset_llm_cache<span class=\"token punctuation\">(</span>SQLiteCache<span class=\"token punctuation\">(</span>database_path<span class=\"token operator\">=</span><span class=\"token string\">\"cache/llm_cache.db\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 코드를 작성 후,  LLM 호출하게 되면 로컬에 db 형태로 캐시를 저장하고, 이후 저장된 캐시를 통해 답변을 하게 됩니다.</p>\n<p> </p>\n<p> </p>\n<p>여기서 끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#cache\">Cache</a></p>\n<ul>\n<li><a href=\"#1-inmemory-cache-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BA%90%EC%8B%9C\">1. InMemory Cache (인메모리 캐시)</a></li>\n<li><a href=\"#2-semantic-cache-redissemanticcache-%EB%93%B1\">2. Semantic Cache (<code class=\"language-text\">RedisSemanticCache</code> 등)</a></li>\n<li><a href=\"#3-sqlite-cache\">3. SQLite Cache</a></li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy) 소스코드: https://github.com/teddylee777/langchain-kr 위키독스: https://wikidocs.net/book/14314   오늘은 캐시 방법에 대해 정리해보고자 합니다. 레츠기륏~!     Cache 1. InMemory Cache (인메모리 캐시) LLM 호출에서 인메모리 캐시를 사용한다면,동일한 질문이 들어왔을 때 LLM(OpenAI 등) 서버로 요청을 전달하지 않고, 메모리에 미리 저장해둔 답변을 즉시 꺼내어 응답합니다. (노트북 커널 재시작하면 캐시가 삭제됩니다.) 그렇기에, LLM 호출 비용은 들지 않습니다. 하지만, 질문이 약간 바뀌면(띄어쓰기 하나라도) 다시 호출하게 됩니다. 그 이유는 는 내부적으로 Dictionary(Hash Map) 구조를 사용하기 때문입니다. 위 코드를 작성한 후에 LLM 호출하게 되면 질문과 …","frontmatter":{"date":"December 24, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-7","categories":"LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-24_1/"}},"next":{"id":"0a5507c6-4f1e-5505-b7fb-c556a92f5d07","html":"<p>오늘은 Dense Retriever와 Sparse Retriever에 대해 정리해보고자 합니다.</p>\n<p>레츠기릿~!</p>\n<p> </p>\n<p> </p>\n<h2 id=\"dense-retriever-vs-sparse-retriever\" style=\"position:relative;\"><a href=\"#dense-retriever-vs-sparse-retriever\" aria-label=\"dense retriever vs sparse retriever permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dense Retriever vs Sparse Retriever</h2>\n<hr>\n<h3 id=\"임베딩embedding\" style=\"position:relative;\"><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9embedding\" aria-label=\"임베딩embedding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>임베딩(Embedding)</h3>\n<p>Dense Retriever는 텍스트를 고차원 공간의 **밀집 벡터(Dense Vector)**로 변환합니다.</p>\n<ul>\n<li><strong>Sparse Vector:</strong> 단어 사전 전체 크기의 벡터 중 대부분이 0인 형태 (단어 중복 위주)</li>\n<li><strong>Dense Vector:</strong> 보통 768차원이나 1024차원의 고정된 크기 안에 모든 숫자가 의미 있는 수치로 채워진 형태</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th><strong>구분</strong></th>\n<th><strong>Sparse Retriever (BM25 등)</strong></th>\n<th><strong>Dense Retriever (DPR 등)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>매칭 방식</strong></td>\n<td>키워드 중심 (Exact Match)</td>\n<td>문맥 및 의미 중심 (Semantic Match)</td>\n</tr>\n<tr>\n<td><strong>특징</strong></td>\n<td>“사과”가 포함된 문서를 잘 찾음</td>\n<td>“애플”이나 “과일”이라는 단어도 문맥상 이해</td>\n</tr>\n<tr>\n<td><strong>장점</strong></td>\n<td>빠르고, 도메인 지식 없이도 안정적</td>\n<td>동의어 처리와 추상적인 질문 답변에 강함</td>\n</tr>\n<tr>\n<td><strong>단점</strong></td>\n<td>오타나 유의어 처리에 취약함</td>\n<td>학습 데이터가 많이 필요하고 계산 비용이 큼</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"왜-dense-retriever를-쓸까\" style=\"position:relative;\"><a href=\"#%EC%99%9C-dense-retriever%EB%A5%BC-%EC%93%B8%EA%B9%8C\" aria-label=\"왜 dense retriever를 쓸까 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>왜 Dense Retriever를 쓸까?</h3>\n<ol>\n<li><strong>미세한 의미 차이 파악:</strong> “파이썬 설치 방법”과 “Python 인스톨 가이드”가 같은 의미임을 이해합니다.</li>\n<li><strong>질문-답변 성능 향상:</strong> RAG(검색 증강 생성) 시스템에서 LLM이 답변하기 가장 좋은 문맥을 가져오는 데 탁월합니다.</li>\n</ol>\n<h3 id=\"대표적인-모델-dpr-dense-passage-retrieval\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%AA%A8%EB%8D%B8-dpr-dense-passage-retrieval\" aria-label=\"대표적인 모델 dpr dense passage retrieval permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표적인 모델: DPR (Dense Passage Retrieval)</h3>\n<p>메타(구 Facebook !!)에서 제안한 모델로, 두 개의 BERT 모델을 각각 질문용과 문서용 인코더로 학습시켜 성능을 극대화한 방식이 가장 유명합니다.</p>\n<h3 id=\"한계와-보완-hybrid-search\" style=\"position:relative;\"><a href=\"#%ED%95%9C%EA%B3%84%EC%99%80-%EB%B3%B4%EC%99%84-hybrid-search\" aria-label=\"한계와 보완 hybrid search permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>한계와 보완 (Hybrid Search)</h3>\n<p>Dense Retriever는 학습 데이터에 없는 특수한 고유명사나 품번(예: “A-1234”) 같은 수치 매칭에는 약할 수 있습니다. 그래서 최근에는 <strong>Sparse와 Dense를 섞은 Hybrid Search</strong> 방식을 실무에서 가장 많이 사용합니다.</p>\n<blockquote>\n<p>최근에는 고성능 벡터 데이터베이스(Chroma, Pinecone, FAISS 등)를 사용하여 수백만 개의 Dense Vector 중에서 가장 유사한 것을 0.1초 내외로 검색할 수 있게 되었습니다.</p>\n</blockquote>\n<p> </p>\n<p> </p>\n<h3 id=\"예제로-확인\" style=\"position:relative;\"><a href=\"#%EC%98%88%EC%A0%9C%EB%A1%9C-%ED%99%95%EC%9D%B8\" aria-label=\"예제로 확인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>예제로 확인</h3>\n<p>Sparse Retriever와 Dense Retriever를 예제를 통해서 직접 비교해보겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>documents <span class=\"token keyword\">import</span> Document\n\n<span class=\"token comment\"># 1. 예시 데이터 (문서 청크)</span>\ntexts <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">\"아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"갤럭시 S24 울트라는 AI 번역 기능을 제공합니다.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"점심 메뉴로 김치찌개와 제육볶음이 인기입니다.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># '아이폰' 단어 없음</span>\n<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># LangChain Document 객체로 변환</span>\ndocuments <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>Document<span class=\"token punctuation\">(</span>page_content<span class=\"token operator\">=</span>t<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> texts<span class=\"token punctuation\">]</span>\ndocuments\n<span class=\"token triple-quoted-string string\">\"\"\"출력:\n[Document(metadata={}, page_content='아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.'),\n Document(metadata={}, page_content='갤럭시 S24 울트라는 AI 번역 기능을 제공합니다.'),\n Document(metadata={}, page_content='점심 메뉴로 김치찌개와 제육볶음이 인기입니다.'),\n Document(metadata={}, page_content='애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.')]\n\"\"\"</span></code></pre></div>\n<p>LangChain Document 객체로 변환한 후에,<br>검색어 ”<strong>아이폰 15 프로 충전</strong>“에 대한 <strong>두 Retriever 결과를 비교</strong>해보겠습니다.</p>\n<p> </p>\n<ol>\n<li>\n<p><strong>BM25Retriever - Sparse Retriever 예시</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>retrievers <span class=\"token keyword\">import</span> BM25Retriever\n\n<span class=\"token comment\"># 1. Sparse Retriever (BM25)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"--- [A. Sparse Retriever (BM25)] ---\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># BM25Retriever 객체 생성</span>\nsparse_retriever <span class=\"token operator\">=</span> BM25Retriever<span class=\"token punctuation\">.</span>from_documents<span class=\"token punctuation\">(</span>documents<span class=\"token punctuation\">)</span>\nsparse_retriever<span class=\"token punctuation\">.</span>k <span class=\"token operator\">=</span> <span class=\"token number\">1</span>  <span class=\"token comment\"># 상위 1개만 검색</span>\n\n<span class=\"token comment\"># 검색어: \"아이폰 15 프로 충전\"</span>\nsparse_result <span class=\"token operator\">=</span> sparse_retriever<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"아이폰 15 프로 충전\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"검색어: '아이폰 15 프로 충전'\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"결과: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>sparse_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>page_content<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"출력:\n--- [A. Sparse Retriever (BM25)] ---\n검색어: '아이폰 15 프로 충전'\n결과: 아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\n\"\"\"</span></code></pre></div>\n</li>\n</ol>\n<ul>\n<li>\n<p><strong>검색어 분해(토큰화):</strong> <code class=\"language-text\">[\"아이폰\", \"15\", \"프로\", \"충전\"]</code> (4개의 키워드)</p>\n</li>\n<li>\n<p><strong>첫번째와 네번째 문서가 유력한 후보</strong>인데</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\"아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\",\n...\n\"애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.\"</code></pre></div>\n</li>\n<li>\n<p><strong>BM25</strong>는 <strong>1) 토큰이 문서 전체에서 얼마나 희귀한지, 2) 토큰이 각 문서에서 얼마나 자주 등장하는지(빈도 포화 계산을 통해 너무 많이 등장하면 더이상 점수가 오르지 않음), 3) 각 문서 길이는 얼마나 짧은지</strong> 를 계산하여 점수를 매깁니다.</p>\n<ul>\n<li>이때 첫번째 문서 <code class=\"language-text\">\"아이폰 15 프로는 티타늄 소재를 사용하여 가볍습니다.\"</code>가 <code class=\"language-text\">아이폰</code>, <code class=\"language-text\">15</code>, <code class=\"language-text\">프로</code>가 여러 개 겹치면서 다른 문서들과 비교했을 때 토큰들의 희귀성들이 큰 차이 없고 문서 길이도 큰 차이 없기 때문에</li>\n<li><strong>Sparse Retriever인 BM25Retriever는 첫번째 문서를 가져오게 됩니다.</strong></li>\n</ul>\n</li>\n</ul>\n<p> </p>\n<ol start=\"2\">\n<li>\n<p><strong>허깅페이스의 jhgan/ko-sroberta-multitask 모델 - Dense Retriever 예시</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>vectorstores <span class=\"token keyword\">import</span> FAISS\n<span class=\"token keyword\">from</span> langchain_huggingface <span class=\"token keyword\">import</span> HuggingFaceEmbeddings\n\n<span class=\"token comment\"># 2. Dense Retriever (Vector Store)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"--- [B. Dense Retriever (Vector/FAISS)] ---\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 임베딩 모델 로드 (한국어 성능 좋은 모델)</span>\nembedding_model <span class=\"token operator\">=</span> HuggingFaceEmbeddings<span class=\"token punctuation\">(</span>model_name<span class=\"token operator\">=</span><span class=\"token string\">\"jhgan/ko-sroberta-multitask\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 벡터 저장소(VectorStore) 생성 후 Retriever로 변환</span>\nvectorstore <span class=\"token operator\">=</span> FAISS<span class=\"token punctuation\">.</span>from_documents<span class=\"token punctuation\">(</span>documents<span class=\"token punctuation\">,</span> embedding_model<span class=\"token punctuation\">)</span>\ndense_retriever <span class=\"token operator\">=</span> vectorstore<span class=\"token punctuation\">.</span>as_retriever<span class=\"token punctuation\">(</span>search_kwargs<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"k\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 검색어: \"아이폰 15 프로 충전\"</span>\ndense_result <span class=\"token operator\">=</span> dense_retriever<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"아이폰 15 프로 충전\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"검색어: '아이폰 15 프로 충전'\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"결과: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>dense_result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>page_content<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token triple-quoted-string string\">\"\"\"출력:\n--- [B. Dense Retriever (Vector/FAISS)] ---\n검색어: '아이폰 15 프로 충전'\n결과: 애플의 새로운 스마트폰은 C타입 충전 단자를 지원합니다.\n\"\"\"</span></code></pre></div>\n<ul>\n<li><strong>검색어 벡터:</strong> **<code class=\"language-text\">아이폰 15 프로 충전</code>**을 <strong>아이폰 스마트폰의 특정 기종의 충전과 관련한 의미</strong>를 가진 좌표로 변환할 것임</li>\n<li><strong>네번째 문서가 첫번째 문서보다 특별하게 <code class=\"language-text\">충전</code>이라는 핵심 의도가 가까우므로 네번째 문서를 가져오게 됩니다.</strong></li>\n</ul>\n</li>\n</ol>\n<p> </p>\n<p> </p>\n<p>Dense Retriever과 같은 Semantic Retriever은<br>벡터 검색은 “의미”를 너무 중시한 나머지 “정확성”을 놓칠 때가 많습니다.</p>\n<p>특히, 고유명사를 무시하는 경우가 존재하여<br>최근에는 하이브리드로 Retriever를 이용하는 경우가 많다고 합니다.</p>\n<p>이것은 다음에 다루도록 해보겠습니다.</p>\n<p>그럼 이번 글은 여기까지 끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#dense-retriever-vs-sparse-retriever\">Dense Retriever vs Sparse Retriever</a></p>\n<ul>\n<li><a href=\"#%EC%9E%84%EB%B2%A0%EB%94%A9embedding\">임베딩(Embedding)</a></li>\n<li><a href=\"#%EC%99%9C-dense-retriever%EB%A5%BC-%EC%93%B8%EA%B9%8C\">왜 Dense Retriever를 쓸까?</a></li>\n<li><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%AA%A8%EB%8D%B8-dpr-dense-passage-retrieval\">대표적인 모델: DPR (Dense Passage Retrieval)</a></li>\n<li><a href=\"#%ED%95%9C%EA%B3%84%EC%99%80-%EB%B3%B4%EC%99%84-hybrid-search\">한계와 보완 (Hybrid Search)</a></li>\n<li><a href=\"#%EC%98%88%EC%A0%9C%EB%A1%9C-%ED%99%95%EC%9D%B8\">예제로 확인</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"December 23, 2025","title":"Dense Retriever와 Sparse Retriever","categories":"LLM RAG","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-23_1/"}},"prev":{"id":"cd4dc7a1-3e81-5f0a-aec7-f809be6b0269","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy\">https://fastcampus.co.kr/data_online_teddy</a>)</p>\n<p>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a></p>\n<p>위키독스: <a href=\"https://wikidocs.net/book/14314\">https://wikidocs.net/book/14314</a></p>\n<p> </p>\n<p>오늘은 맥 사용자(저요,,,)의 허깅페이스 모델을 로컬로 가져와<br>mps를 이용해 GPU 가속화하는 방법을 먼저 보고 가겠습니다~</p>\n<p>레츠기리잇~!</p>\n<p> </p>\n<h2 id=\"macmps-환경에서-huggingfacepipeline-사용-시-주의사항\" style=\"position:relative;\"><a href=\"#macmps-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-huggingfacepipeline-%EC%82%AC%EC%9A%A9-%EC%8B%9C-%EC%A3%BC%EC%9D%98%EC%82%AC%ED%95%AD\" aria-label=\"macmps 환경에서 huggingfacepipeline 사용 시 주의사항 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Mac(MPS) 환경에서 HuggingFacePipeline 사용 시 주의사항</h2>\n<hr>\n<h3 id=\"code-classlanguage-textfrom_model_idcode-vs-code-classlanguage-textpipelinecode-직접-주입\" style=\"position:relative;\"><a href=\"#code-classlanguage-textfrom_model_idcode-vs-code-classlanguage-textpipelinecode-%EC%A7%81%EC%A0%91-%EC%A3%BC%EC%9E%85\" aria-label=\"code classlanguage textfrom_model_idcode vs code classlanguage textpipelinecode 직접 주입 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">from_model_id</code> vs <code class=\"language-text\">pipeline</code> 직접 주입</h3>\n<p>LangChain을 사용하여 로컬 LLM을 구동할 때, 특히 Apple Silicon(M1/M2/M3) 환경에서 GPU 가속(MPS)을 설정하는 과정에서 <code class=\"language-text\">ValueError</code>가 발생하는 경우가 많습니다.</p>\n<p><strong>이는 LangChain이 내부적으로 Hugging Face 모델을 로드하는 방식(<code class=\"language-text\">from_model_id</code>)과 실제 <code class=\"language-text\">transformers</code> 라이브러리의 <code class=\"language-text\">pipeline</code>이 장치(Device)를 처리하는 방식의 차이 때문입니다.</strong></p>\n<p> </p>\n<ol>\n<li>\n<p><code class=\"language-text\">transformers.pipeline</code></p>\n<ul>\n<li>\n<p><strong>소속 라이브러리:</strong> <code class=\"language-text\">transformers</code> (Hugging Face)</p>\n</li>\n<li>\n<p><strong>정체:</strong> 모델 추론(Inference)을 위한 <strong>End-to-End 실행 엔진</strong>입니다.</p>\n</li>\n<li>\n<p><strong>역할:</strong> 모델 다운로드, 토크나이저 로드, 입력 텍스트 전처리(Pre-processing), 모델 추론(Model Inference), 결과 후처리(Post-processing)의 전 과정을 수행합니다.</p>\n</li>\n<li>\n<p><strong>Device 처리 특징:</strong></p>\n<ul>\n<li>⭐️ <strong><code class=\"language-text\">device</code> 인자로 문자열(<code class=\"language-text\">\"cpu\"</code>, <code class=\"language-text\">\"cuda\"</code>, <code class=\"language-text\">\"mps\"</code>)과 정수(GPU ID)를 모두 지원합니다.</strong> ⭐️</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><code class=\"language-text\">HuggingFacePipeline.from_model_id</code></p>\n<ul>\n<li>\n<p><strong>소속 라이브러리:</strong> <code class=\"language-text\">langchain_huggingface</code></p>\n</li>\n<li>\n<p><strong>정체:</strong> LangChain에서 <code class=\"language-text\">HuggingFacePipeline</code> 객체를 쉽게 생성하기 위해 제공하는 **팩토리 메서드(Factory Method)이자 래퍼(Wrapper)**입니다.</p>\n</li>\n<li>\n<p><strong>역할:</strong> 내부적으로 <code class=\"language-text\">transformers.pipeline</code>을 호출하여 파이프라인을 생성하고, 이를 LangChain 객체로 감쌉니다.</p>\n</li>\n<li>\n<p><strong>Device 처리 특징:</strong></p>\n<ul>\n<li>사용자의 편의를 위해 <code class=\"language-text\">device</code> 파라미터를 주로 정수형(Integer)으로 입력받도록 설계되었습니다. (예: <code class=\"language-text\">-1</code>은 CPU, ⭐️ <strong><code class=\"language-text\">0</code>은 첫 번째 CUDA GPU</strong> ⭐️)</li>\n<li>이 과정에서 **유효성 검사 로직(Validation Logic)**이 포함되는데, 이 로직이 NVIDIA GPU(CUDA)를 기준으로 작성되어 있어 Mac(MPS) 환경과 호환성 충돌을 일으킬 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p> </p>\n<p>결국,</p>\n<p><strong>맥북 사용자는 mps 사용하기 위해서는<br><code class=\"language-text\">HuggingFacePipeline.from_model_id</code>가 아닌, <code class=\"language-text\">transformers.pipeline</code>을 이용하여야 합니다.</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># ✅ mps는 허깅페이스 라이브러리인 transformers의 pipeline을 사용하여야 한다.</span>\n\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> pipeline\n<span class=\"token keyword\">from</span> langchain_huggingface <span class=\"token keyword\">import</span> HuggingFacePipeline\n\n<span class=\"token comment\"># 1. Transformers의 pipeline을 직접 생성합니다.</span>\n<span class=\"token comment\"># 여기서 device=\"mps\"를 문자열로 명시하면 에러 없이 강제로 MPS를 잡습니다.</span>\npipe <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"text-generation\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"microsoft/Phi-3-mini-4k-instruct\"</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span><span class=\"token string\">\"mps\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># &lt;--- 핵심: 여기서 \"mps\"를 직접 지정</span>\n    model_kwargs<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"torch_dtype\"</span><span class=\"token punctuation\">:</span> torch<span class=\"token punctuation\">.</span>float16<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># Mac에서 메모리 절약 및 속도 향상</span>\n    max_new_tokens<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>\n    do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># 경고 메시지 해결</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 생성된 pipe를 LangChain 객체에 주입합니다.</span>\nllm <span class=\"token operator\">=</span> HuggingFacePipeline<span class=\"token punctuation\">(</span>pipeline<span class=\"token operator\">=</span>pipe<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 3. 실행</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>llm<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"Hugging Face is\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>맥북 사용자(mps 이용)는 위 코드를 이용해야 하며,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># CUDA 용</span>\n<span class=\"token keyword\">from</span> langchain_huggingface <span class=\"token keyword\">import</span> HuggingFacePipeline\n\nllm <span class=\"token operator\">=</span> HuggingFacePipeline<span class=\"token punctuation\">.</span>from_model_id<span class=\"token punctuation\">(</span>\n    model_id<span class=\"token operator\">=</span><span class=\"token string\">\"microsoft/Phi-3-mini-4k-instruct\"</span><span class=\"token punctuation\">,</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"text-generation\"</span><span class=\"token punctuation\">,</span>\n    pipeline_kwargs<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"max_new_tokens\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"top_k\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"temperature\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"do_sample\"</span><span class=\"token punctuation\">:</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span><span class=\"token number\">0</span>   <span class=\"token comment\"># from_model_id은 cuda를 0으로 설정되어 있어서 mps는 사용 불가능</span>\n<span class=\"token punctuation\">)</span>\nllm<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"Hugging Face is\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>위 코드는 맥북 사용자(mps 이용)는 사용할 수 없습니다.</p>\n<p>OK.</p>\n<p> </p>\n<p> </p>\n<h2 id=\"ollama\" style=\"position:relative;\"><a href=\"#ollama\" aria-label=\"ollama permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ollama</h2>\n<hr>\n<p>Ollama는 복잡한 설정 없이 CLI(명령줄) 한 줄로 모델을 다운로드하고 실행할 수 있게 해줍니다. Hugging Face <code class=\"language-text\">transformers</code> 라이브러리를 사용하여 직접 모델을 로드해올 수 있지만 Ollama를 이용하면 편리하게 모델을 사용할 수 있습니다.</p>\n<p>물론 AWS, GCP에서 EC2를 빌려와서 그 안에서 Ollama를 설치해서 사용 가능합니다.</p>\n<p> </p>\n<h3 id=\"ollama를-사용하는-방법\" style=\"position:relative;\"><a href=\"#ollama%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\" aria-label=\"ollama를 사용하는 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ollama를 사용하는 방법</h3>\n<p><strong>gguf 파일(허깅페이스에서 gguf를 제공하는 LLM)을 models 폴더에 넣어줍니다</strong>.<br></p>\n<p><strong>models 폴더에 Modelfile 파일을 작성해주는데</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">FROM (허깅페이스에서 가져온 모델명).gguf\n\nTEMPLATE \"\"\"(모델이 학습한 템플릿 내용)\n\"\"\"\n\nSYSTEM \"\"\"(시스템 프롬프트)\"\"\"\n\nPARAMETER stop &lt;s>\nPARAMETER stop &lt;/s></code></pre></div>\n<p>이런 식으로 작성해줍니다. 그러면 허깅페이스에서 가져온 모델을 읽고 모델이 학습했던 템플릿과 시스템 프롬프트를 특정 명령어를 통해서 알아서 읽게 됩니다.</p>\n<p>(&#x3C;s>, &#x3C;/s>과 같은 것들을 스페셜 토큰이라 하며 모델이 어디까지 무엇인지 알아들을 수 있는 키워드입니다. <code class=\"language-text\">PARAMETER stop &lt;/s></code>을 작성하지 않으면 혼잣말을 계속하거나 횡설수설하는 현상이 일어날 때가 있습니다.)</p>\n<p> </p>\n<h3 id=\"ollama-관련-터미널-사용법-가상환경과-무관\" style=\"position:relative;\"><a href=\"#ollama-%EA%B4%80%EB%A0%A8-%ED%84%B0%EB%AF%B8%EB%84%90-%EC%82%AC%EC%9A%A9%EB%B2%95-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD%EA%B3%BC-%EB%AC%B4%EA%B4%80\" aria-label=\"ollama 관련 터미널 사용법 가상환경과 무관 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ollama 관련 터미널 사용법 (가상환경과 무관)</h3>\n<ol>\n<li><code class=\"language-text\">ollama</code> : Ollama가 작동하는지 알 수 있음</li>\n<li><code class=\"language-text\">ollama list</code>(또는 <code class=\"language-text\">ollama ls</code>): 내 로컬 Ollama 시스템 전용 폴더에 저장된 모델 목록을 보여줌</li>\n<li><code class=\"language-text\">ollama ps</code>: 현재 실행 중인(메모리에 올라간) 모델 목록을 보여줌</li>\n<li><code class=\"language-text\">ollama rm [모델명]</code>: 모델을 삭제하는 명령어임</li>\n<li><code class=\"language-text\">ollama pull [모델명:모델크기b]</code>: Ollama 라이브러리에 있는 모델을 내 로컬 <code class=\"language-text\">ollama list</code>에 등록</li>\n<li><code class=\"language-text\">ollama create [내가부르고싶은모델명] -f Modelfile</code><br>: 내 로컬에 있는 모델을 내 로컬  <code class=\"language-text\">ollama list</code>에 등록<br>(<code class=\"language-text\">Modelfile</code> 파일에 있는 모델을 읽어 [내가부르고 싶은 모델명]으로 등록함)</li>\n<li><code class=\"language-text\">ollama run [모델명:모델크기b]</code>: 내 로컬  <code class=\"language-text\">ollama list</code>에 있는 모델을 메모리에 올림</li>\n</ol>\n<p> </p>\n<h3 id=\"langchain에서-ollama-사용법\" style=\"position:relative;\"><a href=\"#langchain%EC%97%90%EC%84%9C-ollama-%EC%82%AC%EC%9A%A9%EB%B2%95\" aria-label=\"langchain에서 ollama 사용법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>langchain에서 ollama 사용법</h3>\n<p><code class=\"language-text\">langchain_ollama</code>의 <code class=\"language-text\">ChatOllama</code> 모듈을 로컬 Ollama 시스템 전용 폴더에 저장된 모델명을 가져와 이용하면 됩니다.</p>\n<p>먼저, 터미널에서 <code class=\"language-text\">ollama ls</code>로 모델명을 파악한 후</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_ollama <span class=\"token keyword\">import</span> ChatOllama\n\nllm <span class=\"token operator\">=</span> ChatOllama<span class=\"token punctuation\">(</span>model<span class=\"token operator\">=</span><span class=\"token string\">\"모델명\"</span><span class=\"token punctuation\">,</span> temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">format</span><span class=\"token operator\">=</span><span class=\"token string\">'json'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>이렇게 <strong><code class=\"language-text\">model</code> 파라미터에 해당 모델명을 넣어 생성</strong>하면 해당 모델을 손쉽게 사용할 수 있습니다. (Chain 연결도 쌉가능.)</p>\n<p> </p>\n<p>그리고,</p>\n<p><strong><code class=\"language-text\">temperature</code>(답변의 창의성 정도)와 <code class=\"language-text\">format</code>(출력 형식), <code class=\"language-text\">num_predict</code>(최대 출력 토큰 수),  <code class=\"language-text\">top_k</code>(단어 선택의 다양성 정도),  <code class=\"language-text\">top_p</code>(누적 확률이 P가 될 때까지의 단어들만 후보군 넣음),  <code class=\"language-text\">repeat_penalty</code>(반복 방지),  <code class=\"language-text\">seed</code>(결과 재현성 확보),  <code class=\"language-text\">stop</code>(모델이 생성을 멈춰야 하는 문자열 지정), <code class=\"language-text\">keep_alive</code>(모델 메모리 유지 시간) 등을 설정</strong>할 수도 있습니다.</p>\n<p> </p>\n<p>만약,</p>\n<p>GPU를 잘 활용하고 있는지 보고 싶다면<br>코드가 스트리밍 중일 때 터미널에서 <code class=\"language-text\">ollama ps</code>를 입력해서 ‘100% GPU’인지 확인 가능합니다.</p>\n<p>참고로, Ollama는 자동적으로 환경을 감지해서 GPU 가속을 수행합니다.</p>\n<p> </p>\n<p> </p>\n<p>오늘은 여기까지 작성하는데,<br>Ollama 외에도 GPT4ALL 모델을 이용하여 LLM 모델을 로드하여 사용할 수 있습니다.(프로그램과 라이브러리 모두 존재함) 취향껏 써보면 될 듯 합니다.</p>\n<p>끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#macmps-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-huggingfacepipeline-%EC%82%AC%EC%9A%A9-%EC%8B%9C-%EC%A3%BC%EC%9D%98%EC%82%AC%ED%95%AD\">Mac(MPS) 환경에서 HuggingFacePipeline 사용 시 주의사항</a></p>\n<ul>\n<li><a href=\"#from_model_id-vs-pipeline-%EC%A7%81%EC%A0%91-%EC%A3%BC%EC%9E%85\"><code class=\"language-text\">from_model_id</code> vs <code class=\"language-text\">pipeline</code> 직접 주입</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#ollama\">Ollama</a></p>\n<ul>\n<li><a href=\"#ollama%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\">Ollama를 사용하는 방법</a></li>\n<li><a href=\"#ollama-%EA%B4%80%EB%A0%A8-%ED%84%B0%EB%AF%B8%EB%84%90-%EC%82%AC%EC%9A%A9%EB%B2%95-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD%EA%B3%BC-%EB%AC%B4%EA%B4%80\">Ollama 관련 터미널 사용법 (가상환경과 무관)</a></li>\n<li><a href=\"#langchain%EC%97%90%EC%84%9C-ollama-%EC%82%AC%EC%9A%A9%EB%B2%95\">langchain에서 ollama 사용법</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"December 25, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-8","categories":"LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-25_1/"}},"site":{"siteMetadata":{"siteUrl":"https://www.zoomkoding.com","comments":{"utterances":{"repo":"https://github.com/byeonwoojung/byeonwoojung.github.io"}}}}},"pageContext":{"slug":"/25-12-24_1/","nextSlug":"/25-12-23_1/","prevSlug":"/25-12-25_1/"}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}