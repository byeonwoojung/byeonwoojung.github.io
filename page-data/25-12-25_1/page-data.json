{"componentChunkName":"component---src-templates-blog-template-js","path":"/25-12-25_1/","result":{"data":{"cur":{"id":"cd4dc7a1-3e81-5f0a-aec7-f809be6b0269","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy\">https://fastcampus.co.kr/data_online_teddy</a>)</p>\n<p>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a></p>\n<p> </p>\n<p>오늘은 맥 사용자(저요,,,)의 허깅페이스 모델을 로컬로 가져와<br>mps를 이용해 GPU 가속화하는 방법을 먼저 보고 가겠습니다~</p>\n<p>레츠기리잇~!</p>\n<p> </p>\n<h2 id=\"macmps-환경에서-huggingfacepipeline-사용-시-주의사항\" style=\"position:relative;\"><a href=\"#macmps-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-huggingfacepipeline-%EC%82%AC%EC%9A%A9-%EC%8B%9C-%EC%A3%BC%EC%9D%98%EC%82%AC%ED%95%AD\" aria-label=\"macmps 환경에서 huggingfacepipeline 사용 시 주의사항 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Mac(MPS) 환경에서 HuggingFacePipeline 사용 시 주의사항</h2>\n<hr>\n<h3 id=\"code-classlanguage-textfrom_model_idcode-vs-code-classlanguage-textpipelinecode-직접-주입\" style=\"position:relative;\"><a href=\"#code-classlanguage-textfrom_model_idcode-vs-code-classlanguage-textpipelinecode-%EC%A7%81%EC%A0%91-%EC%A3%BC%EC%9E%85\" aria-label=\"code classlanguage textfrom_model_idcode vs code classlanguage textpipelinecode 직접 주입 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><code class=\"language-text\">from_model_id</code> vs <code class=\"language-text\">pipeline</code> 직접 주입</h3>\n<p>LangChain을 사용하여 로컬 LLM을 구동할 때, 특히 Apple Silicon(M1/M2/M3) 환경에서 GPU 가속(MPS)을 설정하는 과정에서 <code class=\"language-text\">ValueError</code>가 발생하는 경우가 많습니다.</p>\n<p><strong>이는 LangChain이 내부적으로 Hugging Face 모델을 로드하는 방식(<code class=\"language-text\">from_model_id</code>)과 실제 <code class=\"language-text\">transformers</code> 라이브러리의 <code class=\"language-text\">pipeline</code>이 장치(Device)를 처리하는 방식의 차이 때문입니다.</strong></p>\n<p>이 글에서는 두 방식의 기술적 차이점과 Mac 환경에서의 올바른 설정 방법을 정리합니다.</p>\n<h4 id=\"개념-및-역할-정의\" style=\"position:relative;\"><a href=\"#%EA%B0%9C%EB%85%90-%EB%B0%8F-%EC%97%AD%ED%95%A0-%EC%A0%95%EC%9D%98\" aria-label=\"개념 및 역할 정의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>개념 및 역할 정의</h4>\n<ol>\n<li>\n<p><code class=\"language-text\">transformers.pipeline</code></p>\n<ul>\n<li>\n<p><strong>소속 라이브러리:</strong> <code class=\"language-text\">transformers</code> (Hugging Face)</p>\n</li>\n<li>\n<p><strong>정체:</strong> 모델 추론(Inference)을 위한 <strong>End-to-End 실행 엔진</strong>입니다.</p>\n</li>\n<li>\n<p><strong>역할:</strong> 모델 다운로드, 토크나이저 로드, 입력 텍스트 전처리(Pre-processing), 모델 추론(Model Inference), 결과 후처리(Post-processing)의 전 과정을 수행합니다.</p>\n</li>\n<li>\n<p><strong>Device 처리 특징:</strong></p>\n<ul>\n<li><code class=\"language-text\">device</code> 인자로 문자열(<code class=\"language-text\">\"cpu\"</code>, <code class=\"language-text\">\"cuda\"</code>, <code class=\"language-text\">\"mps\"</code>)과 정수(GPU ID)를 모두 지원합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><code class=\"language-text\">HuggingFacePipeline.from_model_id</code></p>\n<ul>\n<li>\n<p><strong>소속 라이브러리:</strong> <code class=\"language-text\">langchain_huggingface</code></p>\n</li>\n<li>\n<p><strong>정체:</strong> LangChain에서 <code class=\"language-text\">HuggingFacePipeline</code> 객체를 쉽게 생성하기 위해 제공하는 **팩토리 메서드(Factory Method)이자 래퍼(Wrapper)**입니다.</p>\n</li>\n<li>\n<p><strong>역할:</strong> 내부적으로 <code class=\"language-text\">transformers.pipeline</code>을 호출하여 파이프라인을 생성하고, 이를 LangChain 객체로 감쌉니다.</p>\n</li>\n<li>\n<p><strong>Device 처리 특징:</strong></p>\n<ul>\n<li>사용자의 편의를 위해 <code class=\"language-text\">device</code> 파라미터를 주로 정수형(Integer)으로 입력받도록 설계되었습니다. (예: <code class=\"language-text\">-1</code>은 CPU, <strong><code class=\"language-text\">0</code>은 첫 번째 CUDA GPU</strong>)</li>\n<li>이 과정에서 **유효성 검사 로직(Validation Logic)**이 포함되는데, 이 로직이 NVIDIA GPU(CUDA)를 기준으로 작성되어 있어 Mac(MPS) 환경과 호환성 충돌을 일으킬 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p> </p>\n<p>결국,</p>\n<p><strong>맥북 사용자는 mps 사용하기 위해서는<br><code class=\"language-text\">HuggingFacePipeline.from_model_id</code>가 아닌, <code class=\"language-text\">transformers.pipeline</code>을 이용하여야 합니다.</strong></p>\n<p> </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># ✅ mps는 허깅페이스 라이브러리인 transformers의 pipeline을 사용하여야 합니다!!</span>\n\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> pipeline\n<span class=\"token keyword\">from</span> langchain_huggingface <span class=\"token keyword\">import</span> HuggingFacePipeline\n\n<span class=\"token comment\"># 1. Transformers의 pipeline을 직접 생성합니다.</span>\n<span class=\"token comment\"># 여기서 device=\"mps\"를 문자열로 명시하면 에러 없이 강제로 MPS를 잡습니다.</span>\npipe <span class=\"token operator\">=</span> pipeline<span class=\"token punctuation\">(</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"text-generation\"</span><span class=\"token punctuation\">,</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"microsoft/Phi-3-mini-4k-instruct\"</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span><span class=\"token string\">\"mps\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># &lt;--- 핵심: 여기서 \"mps\"를 직접 지정</span>\n    model_kwargs<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"torch_dtype\"</span><span class=\"token punctuation\">:</span> torch<span class=\"token punctuation\">.</span>float16<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># Mac에서 메모리 절약 및 속도 향상</span>\n    max_new_tokens<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>\n    top_k<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>\n    do_sample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># 경고 메시지 해결</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 생성된 pipe를 LangChain 객체에 주입합니다.</span>\nllm <span class=\"token operator\">=</span> HuggingFacePipeline<span class=\"token punctuation\">(</span>pipeline<span class=\"token operator\">=</span>pipe<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 3. 실행</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>llm<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"Hugging Face is\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>맥북 사용자(mps 이용)는 위 코드를 이용해야 하며,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># CUDA 용</span>\n<span class=\"token keyword\">from</span> langchain_huggingface <span class=\"token keyword\">import</span> HuggingFacePipeline\n\nllm <span class=\"token operator\">=</span> HuggingFacePipeline<span class=\"token punctuation\">.</span>from_model_id<span class=\"token punctuation\">(</span>\n    model_id<span class=\"token operator\">=</span><span class=\"token string\">\"microsoft/Phi-3-mini-4k-instruct\"</span><span class=\"token punctuation\">,</span>\n    task<span class=\"token operator\">=</span><span class=\"token string\">\"text-generation\"</span><span class=\"token punctuation\">,</span>\n    pipeline_kwargs<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"max_new_tokens\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"top_k\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"temperature\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"do_sample\"</span><span class=\"token punctuation\">:</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span><span class=\"token number\">0</span>   <span class=\"token comment\"># from_model_id은 cuda를 0으로 설정되어 있어서 mps는 사용 불가능</span>\n<span class=\"token punctuation\">)</span>\nllm<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token string\">\"Hugging Face is\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>위 코드는 맥북 사용자(mps 이용)는 사용할 수 없습니다.</p>\n<p>OK.</p>\n<p> </p>\n<p> </p>\n<h2 id=\"ollama\" style=\"position:relative;\"><a href=\"#ollama\" aria-label=\"ollama permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ollama</h2>\n<hr>\n<p>Ollama는 복잡한 설정 없이 CLI(명령줄) 한 줄로 모델을 다운로드하고 실행할 수 있게 해줍니다. Hugging Face <code class=\"language-text\">transformers</code> 라이브러리를 사용하여 직접 모델을 로드해올 수 있지만 Ollama를 이용하면 편리하게 모델을 사용할 수 있습니다.</p>\n<p>물론 AWS, GCP에서 EC2를 빌려와서 그 안에서 Ollama를 설치해서 사용 가능합니다.</p>\n<p> </p>\n<h3 id=\"ollama를-사용하는-방법\" style=\"position:relative;\"><a href=\"#ollama%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\" aria-label=\"ollama를 사용하는 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ollama를 사용하는 방법</h3>\n<p>gguf 파일(허깅페이스에서 gguf를 제공하는 LLM)을 models 폴더에 넣어줍니다.<br>models 폴더에 Modelfile 파일을 작성해주는데</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">FROM (허깅페이스에서 가져온 모델명).gguf\n\nTEMPLATE \"\"\"(모델이 학습한 템플릿 내용)\n\"\"\"\n\nSYSTEM \"\"\"(시스템 프롬프트)\"\"\"\n\nPARAMETER stop &lt;s>\nPARAMETER stop &lt;/s></code></pre></div>\n<p>이런 식으로 작성해줍니다. 그러면 허깅페이스에서 가져온 모델을 읽고 모델이 학습했던 템플릿과 시스템 프롬프트를 특정 명령어를 통해서 알아서 읽게 됩니다.</p>\n<p>(&#x3C;s>, &#x3C;/s>과 같은 것들을 스페셜 토큰이라 하며 모델이 어디까지 무엇인지 알아들을 수 있는 키워드입니다. <code class=\"language-text\">PARAMETER stop &lt;/s></code>을 작성하지 않으면 혼잣말을 계속하거나 횡설수설하는 현상이 일어날 때가 있습니다.)</p>\n<h3 id=\"ollama-관련-터미널-사용법-가상환경과-무관\" style=\"position:relative;\"><a href=\"#ollama-%EA%B4%80%EB%A0%A8-%ED%84%B0%EB%AF%B8%EB%84%90-%EC%82%AC%EC%9A%A9%EB%B2%95-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD%EA%B3%BC-%EB%AC%B4%EA%B4%80\" aria-label=\"ollama 관련 터미널 사용법 가상환경과 무관 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ollama 관련 터미널 사용법 (가상환경과 무관)</h3>\n<ol>\n<li><code class=\"language-text\">ollama</code> : Ollama가 작동하는지 알 수 있음</li>\n<li><code class=\"language-text\">ollama list</code>(또는 <code class=\"language-text\">ollama ls</code>): 내 로컬 Ollama 시스템 전용 폴더에 저장된 모델 목록을 보여줌</li>\n<li><code class=\"language-text\">ollama ps</code>: 현재 실행 중인(메모리에 올라간) 모델 목록을 보여줌</li>\n<li><code class=\"language-text\">ollama rm [모델명]</code>: 모델을 삭제하는 명령어임</li>\n<li><code class=\"language-text\">ollama pull [모델명:모델크기b]</code>: Ollama 라이브러리에 있는 모델을 내 로컬 <code class=\"language-text\">ollama list</code>에 등록</li>\n<li><code class=\"language-text\">ollama create [내가부르고싶은모델명] -f Modelfile</code><br>: 내 로컬에 있는 모델을 내 로컬  <code class=\"language-text\">ollama list</code>에 등록<br>(<code class=\"language-text\">Modelfile</code> 파일에 있는 모델을 읽어 [내가부르고 싶은 모델명]으로 등록함)</li>\n<li><code class=\"language-text\">ollama run [모델명:모델크기b]</code>: 내 로컬  <code class=\"language-text\">ollama list</code>에 있는 모델을 메모리에 올림</li>\n</ol>\n<p> </p>\n<h3 id=\"langchain에서-ollama-사용법\" style=\"position:relative;\"><a href=\"#langchain%EC%97%90%EC%84%9C-ollama-%EC%82%AC%EC%9A%A9%EB%B2%95\" aria-label=\"langchain에서 ollama 사용법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>langchain에서 ollama 사용법</h3>\n<p><code class=\"language-text\">langchain_ollama</code>의 <code class=\"language-text\">ChatOllama</code> 모듈을 로컬 Ollama 시스템 전용 폴더에 저장된 모델명을 가져와 이용하면 됩니다.</p>\n<p>먼저, 터미널에서 <code class=\"language-text\">ollama ls</code>로 모델명을 파악한 후</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_ollama <span class=\"token keyword\">import</span> ChatOllama\n\nllm <span class=\"token operator\">=</span> ChatOllama<span class=\"token punctuation\">(</span>model<span class=\"token operator\">=</span><span class=\"token string\">\"모델명\"</span><span class=\"token punctuation\">,</span> temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">format</span><span class=\"token operator\">=</span><span class=\"token string\">'json'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>이렇게 <strong><code class=\"language-text\">model</code> 파라미터에 해당 모델명을 넣어 생성</strong>하면 해당 모델을 손쉽게 사용할 수 있습니다. (Chain 연결도 쌉가능.)</p>\n<p> </p>\n<p>그리고,</p>\n<p><strong><code class=\"language-text\">temperature</code>(답변의 창의성 정도)와 <code class=\"language-text\">format</code>(출력 형식), <code class=\"language-text\">num_predict</code>(최대 출력 토큰 수),  <code class=\"language-text\">top_k</code>(단어 선택의 다양성 정도),  <code class=\"language-text\">top_p</code>(누적 확률이 P가 될 때까지의 단어들만 후보군 넣음),  <code class=\"language-text\">repeat_penalty</code>(반복 방지),  <code class=\"language-text\">seed</code>(결과 재현성 확보),  <code class=\"language-text\">stop</code>(모델이 생성을 멈춰야 하는 문자열 지정), <code class=\"language-text\">keep_alive</code>(모델 메모리 유지 시간) 등을 설정</strong>할 수도 있습니다.</p>\n<p> </p>\n<p>만약,</p>\n<p>GPU를 잘 활용하고 있는지 보고 싶다면<br>코드가 스트리밍 중일 때 터미널에서 <code class=\"language-text\">ollama ps</code>를 입력해서 ‘100% GPU’인지 확인 가능합니다.</p>\n<p>참고로, Ollama는 자동적으로 환경을 감지해서 GPU 가속을 수행합니다.</p>\n<p> </p>\n<p> </p>\n<p>오늘은 여기까지 작성하는데,<br>Ollama 외에도 GPT4ALL 모델을 이용하여 LLM 모델을 로드하여 사용할 수 있습니다.(프로그램과 라이브러리 모두 존재함) 취향껏 써보면 될 듯 합니다.</p>\n<p>끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#macmps-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-huggingfacepipeline-%EC%82%AC%EC%9A%A9-%EC%8B%9C-%EC%A3%BC%EC%9D%98%EC%82%AC%ED%95%AD\">Mac(MPS) 환경에서 HuggingFacePipeline 사용 시 주의사항</a></p>\n<ul>\n<li>\n<p><a href=\"#from_model_id-vs-pipeline-%EC%A7%81%EC%A0%91-%EC%A3%BC%EC%9E%85\"><code class=\"language-text\">from_model_id</code> vs <code class=\"language-text\">pipeline</code> 직접 주입</a></p>\n<ul>\n<li><a href=\"#%EA%B0%9C%EB%85%90-%EB%B0%8F-%EC%97%AD%ED%95%A0-%EC%A0%95%EC%9D%98\">개념 및 역할 정의</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#ollama\">Ollama</a></p>\n<ul>\n<li><a href=\"#ollama%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\">Ollama를 사용하는 방법</a></li>\n<li><a href=\"#ollama-%EA%B4%80%EB%A0%A8-%ED%84%B0%EB%AF%B8%EB%84%90-%EC%82%AC%EC%9A%A9%EB%B2%95-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD%EA%B3%BC-%EB%AC%B4%EA%B4%80\">Ollama 관련 터미널 사용법 (가상환경과 무관)</a></li>\n<li><a href=\"#langchain%EC%97%90%EC%84%9C-ollama-%EC%82%AC%EC%9A%A9%EB%B2%95\">langchain에서 ollama 사용법</a></li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy) 소스코드: https://github.com/teddylee777/langchain-kr   오늘은 맥 사용자(저요,,,)의 허깅페이스 모델을 로컬로 가져와mps를 이용해 GPU 가속화하는 방법을 먼저 보고 가겠습니다~ 레츠기리잇~!   Mac(MPS) 환경에서 HuggingFacePipeline 사용 시 주의사항  vs  직접 주입 LangChain을 사용하여 로컬 LLM을 구동할 때, 특히 Apple Silicon(M1/M2/M3) 환경에서 GPU 가속(MPS)을 설정하는 과정에서 가 발생하는 경우가 많습니다. 이는 LangChain이 내부적으로 Hugging Face 모델을 로드하는 방식()과 실제  라이브러리의 이 장치(Device)를 처리하는 방식의 차이 때문입니다. 이 글에서는 두 방식의 기술적 차이점과 Mac 환경에서의 올바른 설정 방법을 정리합니다. 개…","frontmatter":{"date":"December 25, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-8","categories":"LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-25_1/"}},"next":{"id":"cc4dedd9-03dd-5854-bb4d-e9e42ae1ce11","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy\">https://fastcampus.co.kr/data_online_teddy</a>)</p>\n<p>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a></p>\n<p> </p>\n<p>오늘은 캐시 방법에 대해 정리해보고자 합니다.</p>\n<p>레츠기륏~!</p>\n<p> </p>\n<p> </p>\n<h2 id=\"cache\" style=\"position:relative;\"><a href=\"#cache\" aria-label=\"cache permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cache</h2>\n<hr>\n<h3 id=\"1-inmemory-cache-인메모리-캐시\" style=\"position:relative;\"><a href=\"#1-inmemory-cache-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BA%90%EC%8B%9C\" aria-label=\"1 inmemory cache 인메모리 캐시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. InMemory Cache (인메모리 캐시)</h3>\n<p>LLM 호출에서 인메모리 캐시를 사용한다면,<br>동일한 질문이 들어왔을 때 LLM(OpenAI 등) 서버로 요청을 전달하지 않고, <strong>메모리에 미리 저장해둔 답변을 즉시 꺼내어 응답합니다.</strong> (노트북 커널 재시작하면 캐시가 삭제됩니다.)</p>\n<p>그렇기에, <strong>LLM 호출 비용은 들지 않습니다</strong>.</p>\n<p>하지만, <strong>질문이 약간 바뀌면(띄어쓰기 하나라도) 다시 호출하게 됩니다.</strong> 그 이유는 <code class=\"language-text\">InMemoryCache</code>는 내부적으로 <strong>Dictionary(Hash Map) 구조를 사용</strong>하기 때문입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> InMemoryCache\n\nset_llm_cache<span class=\"token punctuation\">(</span>InMemoryCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 인메모리 캐시 설정</span></code></pre></div>\n<p>위 코드를 작성한 후에 LLM 호출하게 되면 질문과 답변을 메모리에 저장해두게 됩니다.</p>\n<p> </p>\n<h3 id=\"2-semantic-cache-code-classlanguage-textredissemanticcachecode-등\" style=\"position:relative;\"><a href=\"#2-semantic-cache-code-classlanguage-textredissemanticcachecode-%EB%93%B1\" aria-label=\"2 semantic cache code classlanguage textredissemanticcachecode 등 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Semantic Cache (<code class=\"language-text\">RedisSemanticCache</code> 등)</h3>\n<p><code class=\"language-text\">InMemoryCache</code>는 질문이 약간 바뀔 때 LLM을 다시 호출해야 하는 단점이 있었습니다.</p>\n<p>반면에, <strong><code class=\"language-text\">Semantic Cache</code>는 질문이 바뀌어도 유사한 질문을 찾아 저장된 캐시를 사용한다는 특징을 가집니다.</strong></p>\n<p>다만, <strong>유사도 계산 시 임베딩 모델을 이용해야 하므로 비용이 들 수 있습니다.</strong></p>\n<table>\n<thead>\n<tr>\n<th><strong>구분</strong></th>\n<th><strong>일반 인메모리 캐시 (InMemoryCache)</strong></th>\n<th><strong>시맨틱 캐시 (RedisSemanticCache 등)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>비교 방식</strong></td>\n<td>문자열 완전 일치 (Exact Match)</td>\n<td><strong>벡터 유사도 비교 (Similarity Match)</strong></td>\n</tr>\n<tr>\n<td><strong>유연성</strong></td>\n<td>띄어쓰기, 조사 하나만 틀려도 실패</td>\n<td>“날씨 어때?”와 “날씨 알려줘”를 같게 인식 가능</td>\n</tr>\n<tr>\n<td><strong>비용</strong></td>\n<td>없음</td>\n<td>유사도 계산을 위한 임베딩 모델 호출 비용 발생</td>\n</tr>\n</tbody>\n</table>\n<p>아래 코드와 같이 작성 후, LLM 호출하게 되면 캐시를 저장하고 이후에는 저장된 캐시를 통해 답변을 하게 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> RedisSemanticCache\n<span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token comment\"># 1. 시맨틱 캐시는 '유사도'를 측정해야 하므로 임베딩 모델이 반드시 필요합니다.</span>\nembeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 시맨틱 캐시 설정 (Redis 사용 예시)</span>\nset_llm_cache<span class=\"token punctuation\">(</span>RedisSemanticCache<span class=\"token punctuation\">(</span>\n    redis_url<span class=\"token operator\">=</span><span class=\"token string\">\"redis://localhost:6379\"</span><span class=\"token punctuation\">,</span>\n    embedding<span class=\"token operator\">=</span>embeddings<span class=\"token punctuation\">,</span>\n    score_threshold<span class=\"token operator\">=</span><span class=\"token number\">0.1</span>  <span class=\"token comment\"># 이 점수가 낮을수록 더 '비슷해야' 캐시를 반환합니다.</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>여기서는 <strong>Localhost에 설치된 Redis 데이터베이스</strong>의 메모리에 저장하게 됩니다.</p>\n<p> </p>\n<h3 id=\"3-sqlite-cache\" style=\"position:relative;\"><a href=\"#3-sqlite-cache\" aria-label=\"3 sqlite cache permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. SQLite Cache</h3>\n<p>SQLite Cache는  별도의 데이터베이스 서버 설치 없이, <strong>내 컴퓨터의 하드디스크에 파일(<code class=\"language-text\">.db</code>) 형태로 데이터를 저장</strong>하는 비휘발성 캐싱 방식입니다. (당연히 비휘발성입니다.)</p>\n<p>LangChain에서 <code class=\"language-text\">InMemoryCache</code>의 휘발성 문제를 해결하면서도, <code class=\"language-text\">Redis</code>처럼 복잡한 서버 설정이 부담스러울 때 가장 많이 사용하는 <strong>가장 간편한 영구 저장용 캐시</strong>입니다.</p>\n<p><code class=\"language-text\">InMemoryCache</code>와 동일하게 질문이 <strong>완전히 동일</strong>해야 캐시를 사용합니다</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> SQLiteCache\n<span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span><span class=\"token builtin\">globals</span> <span class=\"token keyword\">import</span> set_llm_cache\n<span class=\"token keyword\">import</span> os\n\n<span class=\"token comment\"># 캐시 디렉토리를 생성합니다.</span>\n<span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span><span class=\"token string\">\"cache\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span><span class=\"token string\">\"cache\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># SQLiteCache를 사용합니다.</span>\nset_llm_cache<span class=\"token punctuation\">(</span>SQLiteCache<span class=\"token punctuation\">(</span>database_path<span class=\"token operator\">=</span><span class=\"token string\">\"cache/llm_cache.db\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 코드를 작성 후,  LLM 호출하게 되면 로컬에 db 형태로 캐시를 저장하고, 이후 저장된 캐시를 통해 답변을 하게 됩니다.</p>\n<p> </p>\n<p> </p>\n<p>여기서 끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#cache\">Cache</a></p>\n<ul>\n<li><a href=\"#1-inmemory-cache-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BA%90%EC%8B%9C\">1. InMemory Cache (인메모리 캐시)</a></li>\n<li><a href=\"#2-semantic-cache-redissemanticcache-%EB%93%B1\">2. Semantic Cache (<code class=\"language-text\">RedisSemanticCache</code> 등)</a></li>\n<li><a href=\"#3-sqlite-cache\">3. SQLite Cache</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"December 24, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-7","categories":"LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-24_1/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://www.zoomkoding.com","comments":{"utterances":{"repo":"https://github.com/byeonwoojung/byeonwoojung.github.io"}}}}},"pageContext":{"slug":"/25-12-25_1/","nextSlug":"/25-12-24_1/","prevSlug":""}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}