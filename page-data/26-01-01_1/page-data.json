{"componentChunkName":"component---src-templates-blog-template-js","path":"/26-01-01_1/","result":{"data":{"cur":{"id":"69725161-4cf6-5273-b75f-c77780d8edb4","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy)\">https://fastcampus.co.kr/data_online_teddy)</a><br>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a><br>위키독스: <a href=\"https://wikidocs.net/book/14314\">https://wikidocs.net/book/14314</a></p>\n<p> </p>\n<p>오늘도 와쓰요~<br>오늘은 문서 로드 방식과 청킹에 대해 정리해보고자 합니다.</p>\n<p>바로 레츠기릿~!</p>\n<p> </p>\n<h2 id=\"문서-로더-loader\" style=\"position:relative;\"><a href=\"#%EB%AC%B8%EC%84%9C-%EB%A1%9C%EB%8D%94-loader\" aria-label=\"문서 로더 loader permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>문서 로더 (Loader)</h2>\n<hr>\n<p>문서 로드 방식을 간단하게 정리하고자 합니다.</p>\n<ol>\n<li>\n<p>Document 생성</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>documents <span class=\"token keyword\">import</span> Document\n\n<span class=\"token comment\"># 도큐먼트 생성 (내용, 메타데이터 주입)</span>\ndocument <span class=\"token operator\">=</span> Document<span class=\"token punctuation\">(</span>page_content<span class=\"token operator\">=</span><span class=\"token string\">\"내용\"</span><span class=\"token punctuation\">,</span> metadata<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"키\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"값\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 메타데이터 추가</span>\ndocument<span class=\"token punctuation\">.</span>metadata<span class=\"token punctuation\">[</span><span class=\"token string\">\"키\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"값\"</span></code></pre></div>\n</li>\n<li>\n<p>문서 로드</p>\n<ul>\n<li>\n<p><code class=\"language-text\">load()</code> 메서드</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_community<span class=\"token punctuation\">.</span>document_loaders <span class=\"token keyword\">import</span> PyPDFLoader\n\n<span class=\"token comment\"># PyPDFLoader 외에 다른 로더도 가능합니다.</span>\nloader <span class=\"token operator\">=</span> PyPDFLoader<span class=\"token punctuation\">(</span>File_path<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># List[Document] 형식으로 반환합니다.</span>\ndocs <span class=\"token operator\">=</span> loader<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>\n<p><code class=\"language-text\">load_and_split()</code> 메서드: <code class=\"language-text\">text_splitter</code>를 설정해 줌</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_text_splitters <span class=\"token keyword\">import</span> RecursiveCharacterTextSplitter\n\n<span class=\"token comment\"># 분할기(splitter) 생성</span>\ntext_splitter <span class=\"token operator\">=</span> RecursiveCharacterTextSplitter<span class=\"token punctuation\">(</span>chunk_size<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span> chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 로더 설정</span>\nloader <span class=\"token operator\">=</span> PyPDFLoader<span class=\"token punctuation\">(</span>File_path<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 문서 분할</span>\nsplit_docs <span class=\"token operator\">=</span> loader<span class=\"token punctuation\">.</span>load_and_split<span class=\"token punctuation\">(</span>text_splitter<span class=\"token operator\">=</span>text_splitter<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>\n<p><code class=\"language-text\">lazy_load()</code> 메서드: 각 문서를 generator 방식으로 읽으면서 버림<br>(문서의 양이 많을 때 유용함)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># generator 방식으로 문서 로드</span>\n<span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> loader<span class=\"token punctuation\">.</span>lazy_load<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">.</span>metadata<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>\n<p><code class=\"language-text\">aload()</code> 메서드: 비동기 방식으로 문서를 로드함</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">adocs <span class=\"token operator\">=</span> loader<span class=\"token punctuation\">.</span>aload<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">await</span> adocs</code></pre></div>\n</li>\n</ul>\n</li>\n</ol>\n<p> </p>\n<p>문서 로드 방식은 파일 형식이 어떠하든 그 방식이 비슷합니다.</p>\n<p>파일 형식에 맞게 loader 설정 후,<br><code class=\"language-text\">load()</code>, <code class=\"language-text\">load_and_split()</code> 등의 메서드를 이용하여 로드하면 됩니다.</p>\n<p> </p>\n<p> </p>\n<h2 id=\"청킹-chunking\" style=\"position:relative;\"><a href=\"#%EC%B2%AD%ED%82%B9-chunking\" aria-label=\"청킹 chunking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>청킹 (Chunking)</h2>\n<hr>\n<p>RAG에서 텍스트를 청킹하는 것은 중요합니다. 아. 청킹은 문서 내용을 chunk(뭉치?) 단위로 묶는 것이라 생각하면 됩니다.</p>\n<h3 id=\"1-청킹을-하는-이유\" style=\"position:relative;\"><a href=\"#1-%EC%B2%AD%ED%82%B9%EC%9D%84-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0\" aria-label=\"1 청킹을 하는 이유 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 청킹을 하는 이유</h3>\n<ul>\n<li><strong>답변 품질·정확성 향상</strong>\n문서를 적절한 길이로 나누면 질문과의 <strong>유사도 계산이 정확해짐</strong>.\n청크가 너무 길면 핵심 정보가 희석돼 유사도가 낮게 나올 수 있음.</li>\n<li><strong>효율성 향상</strong>\n모델의 <strong>최대 토큰 제한</strong>을 넘지 않게 하고,\n<strong>비용 절감 + 불필요한 문맥 감소로 할루시네이션 위험 완화</strong>.</li>\n</ul>\n<p>문서 내용이 길다면 청킹은 반드시 필수겠죠.</p>\n<p>가장 유명한 청킹 방식은 Recursive Character 기반 Chunking, Sentence 기반 Chunking, Semantic Chunking 등이 있습니다.</p>\n<p>우선 Recursive Character 기반 chunking을 손쉽게 눈으로 확인할 수 있는 사이트를 가져와봤습니다.</p>\n<p><a href=\"https://chunkviz.up.railway.app/\">chunkviz</a></p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 91.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAADN0lEQVQ4y22UW1MbRxCF9f8fA8lDXlzxSx6CTVxJgIgE7ICJuTkgyQhJWOxKmr3fL9qV2N0vNSOLomy66tTs7nT3nj7dM62maZD42uS3TqfD0dERx8fHXFxc0OlcYxjG2uMbf/mttX5JkoQoitS6hu/7mKaJ4zgKQRCQJNLnAd+vCMMltt1QFM1j0taaYVEUZFlGnucq0HVdPM9TkIklwjAkDANsO8J1C5XYdWvKslbsVMLlcvmYaLFYKMgksjTLsrBtR0H+QLKUe45jk+cpDw/Sf858njOfz1cJJTP5Z8lKYp1IBstyXVc+mwSBi2WZj/uGYWKYJnGcqPg4jqnreqXhUyuLgrIslQRpmiJEhq4nzGY5UZSRpYmqSCaQVX1tLTfM+Dyx0Q13BeEytXymps/MDukNPK5vHLq3LiNttWd6MW6UK9hBiu2n2EHGfFHRsvyEmTCURlIr05Qluo/wPKmbjec7WIZATKeEQUgUx6pcyVZORJYXJEVNSxcO3W4PXdcZDoeMRiO1jsdjJpMJg9sB/X5fabeGbdtomoYQQuks/fJ5QVbWK4aGaX/pnuzmapVBkrWEDPL9gFRqF0XEYai6GkWxYpdnKU1Tk0qGbpRxL1b6CSdENzw04XA/c5iY/qO2E2Ey0gR3mmCsm0xMj5FmqL2zjsbHW4e8fKbL0iQrWdLd3Z2SYiYEV0MTyxJkWYqcXYm1JVmFE5TU9ZOT8hRynvwg+DLUNkEYMXEy1YCyXFBVlTrJz90DzzBsoKnVk5y5bueawe0N42Gfu+GApq6eXA7NNxdFa1HVpAuJhmzRkC8hW0L+AF5SMrEChBsrGG5MWjZqb+0n41aoqZuK1iwr+eCnXPoJHxyb3c9t/poesDfe58B4x9/iHw5mh+zrB7T1Q/anh+xPDti7/5O3xkcuw4xzP+bCTyiqnFblRwy2XnL75iWDX3/i+pcNeq++59PrTfqvN+lsfcen7U16rzZW2Nqgv/2D8ulv/4i28zM32y+Y7vwOC3nbhBnXZxr/di3OOianVyZnXYu9kzY7R9u0z3bZPfmN9ukftE932Hn/hneXl5x3fU7/E5z3bE6uTHp9jbpa8D9ik0DY84e+PQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"img 1\" title=\"img 1\" src=\"/static/d1704980c59673ca549cd40023a0bb8f/37523/img_1.png\" srcset=\"/static/d1704980c59673ca549cd40023a0bb8f/e9ff0/img_1.png 180w,\n/static/d1704980c59673ca549cd40023a0bb8f/f21e7/img_1.png 360w,\n/static/d1704980c59673ca549cd40023a0bb8f/37523/img_1.png 720w,\n/static/d1704980c59673ca549cd40023a0bb8f/302a4/img_1.png 1080w,\n/static/d1704980c59673ca549cd40023a0bb8f/07a9c/img_1.png 1440w,\n/static/d1704980c59673ca549cd40023a0bb8f/ce0a7/img_1.png 1590w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p> </p>\n<p>청크 사이즈와 청크 오버랩(청크 간 겹치는 글자수)을 설정하여<br>화면에서 바로 확인해볼 수 있는 사이트입니다.</p>\n<p>간단하고 빠르게 청크 사이즈 정할 때 사용해볼 수 있겠네요!</p>\n<p> </p>\n<p>이제 더 딥하게 한번 봅시다.</p>\n<p> </p>\n<h3 id=\"2-text-splitter와-tokenizer\" style=\"position:relative;\"><a href=\"#2-text-splitter%EC%99%80-tokenizer\" aria-label=\"2 text splitter와 tokenizer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Text Splitter와 Tokenizer</h3>\n<p>먼저, Text Splitter와 Tokenizer를 명확히 구분하고 갑시다.<br>(제가 헷갈렸었거든요..)</p>\n<p><strong>Text Splitter는 말 그대로 텍스트를 분할하는 방식</strong>입니다. 문서나 텍스트가 들어왔을 때, Text Splitter의 방식대로 이들을 분할합니다.</p>\n<p><strong>Tokenizer는 토큰을 분할하는 방식</strong>입니다.</p>\n<p><strong>엥 그래서 둘의 차이는 뭐냐?</strong></p>\n<blockquote>\n<p><strong>청킹(Chunking)에서는</strong></p>\n<p><strong>Text Splitter</strong>를 이용해서 텍스트를 분할하고<br><strong>Tokenizer</strong>의 토큰화 방식을 이용하여 <strong>청크 사이즈가 넘는지 계산</strong>을 대신할 수 있습니다.</p>\n<p>즉,</p>\n<ol>\n<li><strong>문서를 Text Splitter의 특정 기준(\\n, \\n\\n, 특정 모델의 토큰화 방식 등등)으로 분할한 후에</strong></li>\n<li><strong>Tokenizer의 토큰 계산 방법을 이용해서 1개의 토큰이 청크 사이즈 1로 계산하여 청크 사이즈가 넘지 않을 만큼 분할한 문서를 합쳐줍니다.</strong></li>\n</ol>\n<p>OK!!! 이해 완.</p>\n</blockquote>\n<p>결국,</p>\n<p>⭐️ <strong>분할</strong>과 <strong>토큰 계산 방식</strong> 2가지만 기억하고 있으면 됩니다. ⭐️<br>(당연히 <strong>Text splitter만으로 분할하고 청크 사이즈 계산할 수도 있습니다</strong>!!!)</p>\n<p> </p>\n<p> </p>\n<h2 id=\"텍스트-분할기와-토크나이저\" style=\"position:relative;\"><a href=\"#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%ED%95%A0%EA%B8%B0%EC%99%80-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80\" aria-label=\"텍스트 분할기와 토크나이저 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>텍스트 분할기와 토크나이저</h2>\n<hr>\n<p>참고: 아래는 제가 정리한 내용을 지피티와 젬미니에게 정리하도록 하고, 재정리했습니다.<br>(좀 길어요.. 🫠)</p>\n<p>한 줄로 요약하면</p>\n<blockquote>\n<p>사용하는 모델의 입력 한계인 ‘토큰 수’를 정확히 계산하되<br>정보의 손실을 막기 위해 ‘의미 단위(문단·문장)‘의 경계를 최대한 지킬 수 있도록 자르는 것이 가장 중요합니다.</p>\n</blockquote>\n<p> </p>\n<h3 id=\"1-문자-및-구조-기반-분할기\" style=\"position:relative;\"><a href=\"#1-%EB%AC%B8%EC%9E%90-%EB%B0%8F-%EA%B5%AC%EC%A1%B0-%EA%B8%B0%EB%B0%98-%EB%B6%84%ED%95%A0%EA%B8%B0\" aria-label=\"1 문자 및 구조 기반 분할기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 문자 및 구조 기반 분할기</h3>\n<p>가장 기본이 되는 분할 방식으로, <strong>텍스트의 구조적 형태(문단, 개행 등)를 기준</strong>으로 나눕니다.</p>\n<h4 id=\"recursivecharactertextsplitter-️\" style=\"position:relative;\"><a href=\"#recursivecharactertextsplitter-%EF%B8%8F\" aria-label=\"recursivecharactertextsplitter ️ permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RecursiveCharacterTextSplitter ⭐️</h4>\n<p>가장 범용적으로 사용되는 방식입니다. 단순히 문자를 자르는 것이 아니라, 텍스트의 구조를 유지하며 재귀적으로 분할을 시도합니다.</p>\n<ul>\n<li><strong>동작 원리 (Algorithm):</strong>\n<ul>\n<li>**사용자가 지정한 구분자 리스트(기본값: <code class=\"language-text\">[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>)**의 순서대로 분할을 시도합니다.</li>\n<li><strong>재귀적 접근:</strong> 가장 큰 단위(<code class=\"language-text\">\\n\\n</code>: 문단)로 먼저 잘라보고, 청크 크기(Chunk Size)를 초과하면 그 다음 단위(<code class=\"language-text\">\\n</code>: 줄바꿈)로 내려가고, 그래도 크면 단어(<code class=\"language-text\"> </code>), 문자 순으로 쪼갭니다.</li>\n<li><strong>특이 사항 (Edge Case):</strong> 일반적으로 설정한 <code class=\"language-text\">chunk_size</code>를 준수하지만, <strong>절대적인 강제 사항은 아닙니다.</strong> 예를 들어, 구분자가 없는 매우 긴 문자열(예: 긴 URL, 띄어쓰기 없는 텍스트)이 들어오면 설정된 크기를 초과하더라도 자르지 않고 하나의 청크로 유지하는 경우가 발생할 수 있습니다.</li>\n</ul>\n</li>\n<li><strong>장점:</strong> 문단이나 문장의 경계를 최대한 보존하므로 문맥(Context)이 잘리지 않고 유지될 확률이 높습니다. 구조가 불명확한 텍스트에도 비교적 안정적입니다.</li>\n<li><strong>단점:</strong> 주제가 급격히 전환되는 지점을 완벽하게 잡아내지는 못하며, 텍스트의 의미(Semantic)보다는 <strong>구조적 형태에 의존</strong>합니다.</li>\n</ul>\n<ul>\n<li>\n<p>코드</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_text_splitters <span class=\"token keyword\">import</span> RecursiveCharacterTextSplitter\n\ntext_splitter <span class=\"token operator\">=</span> RecursiveCharacterTextSplitter<span class=\"token punctuation\">(</span>\n    <span class=\"token comment\"># 청크 사이즈</span>\n    chunk_size<span class=\"token operator\">=</span><span class=\"token number\">250</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 청크 간의 중복되는 문자 수</span>\n    chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 문자열 길이를 계산하는 함수</span>\n    length_function<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 구분자로 정규식을 사용할지 여부</span>\n    is_separator_regex<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># create_documents(): 원문(String) → 문서(Document) 변환</span>\n<span class=\"token comment\"># List[str] 인자 전달</span>\ntexts <span class=\"token operator\">=</span> text_splitter<span class=\"token punctuation\">.</span>create_documents<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>text_file<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>texts<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 분할된 문서의 첫 번째 문서를 출력합니다.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"===\"</span> <span class=\"token operator\">*</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>texts<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 분할된 문서의 두 번째 문서를 출력합니다.</span>\n\n\n<span class=\"token comment\"># split_documents(): 문서(Document) → 문서(Document) 재분할</span>\n<span class=\"token comment\"># List[Document] 인자 전달</span>\ndocuments <span class=\"token operator\">=</span> text_splitter<span class=\"token punctuation\">.</span>create_documents<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>document_file<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>documents<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>page_content<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 재분할된 첫번째 문서의 내용 출력</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"===\"</span> <span class=\"token operator\">*</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>documents<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>metadata<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 재분할된 첫번째 문서의 메타데이터 출력</span>\n\n\n<span class=\"token comment\"># split_text(): 원문(String) → 원문(String) 분할</span>\nsplit_texts <span class=\"token operator\">=</span> text_splitter<span class=\"token punctuation\">.</span>split_text<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<p> </p>\n<h4 id=\"charactertextsplitter\" style=\"position:relative;\"><a href=\"#charactertextsplitter\" aria-label=\"charactertextsplitter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CharacterTextSplitter</h4>\n<ul>\n<li><strong>동작 원리:</strong> 단일 구분자(기본값: <code class=\"language-text\">\\n\\n</code>) 하나만을 기준으로 텍스트를 분할합니다.</li>\n<li><strong>특징:</strong> <code class=\"language-text\">Recursive</code> 방식보다 단순하지만, 유연성이 떨어져 맥락이 끊길 가능성이 더 높습니다.</li>\n</ul>\n<p> </p>\n<h3 id=\"2-토큰-기반-분할기-tokentextsplitter\" style=\"position:relative;\"><a href=\"#2-%ED%86%A0%ED%81%B0-%EA%B8%B0%EB%B0%98-%EB%B6%84%ED%95%A0%EA%B8%B0-tokentextsplitter\" aria-label=\"2 토큰 기반 분할기 tokentextsplitter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 토큰 기반 분할기 (TokenTextSplitter)</h3>\n<p>LLM은 텍스트를 문자가 아닌 <strong>토큰(Token)</strong> 단위로 처리합니다. 따라서 LLM의 Context Window(입력 제한)를 효율적으로 관리하기 위해서는 토큰 기준 분할이 필수적입니다.</p>\n<ul>\n<li>\n<p>⭐️ <strong>동작 방식:</strong></p>\n<ol>\n<li>입력된 텍스트를 지정된 Tokenizer를 사용해 토큰(정수 ID)으로 변환합니다.</li>\n<li>설정된 <code class=\"language-text\">chunk_size</code> (토큰 개수) 만큼 자릅니다.</li>\n<li>잘린 토큰들을 다시 텍스트로 디코딩합니다.</li>\n</ol>\n</li>\n<li>\n<p>⭐️ <strong>역할 구분:</strong> 분할을 수행하는 주체는 <code class=\"language-text\">TextSplitter</code> 객체이며, 내부의 <code class=\"language-text\">Tokenizer</code>는 오직 **“길이 계산(Counting)“**과 <strong>“인코딩/디코딩”</strong> 역할만 수행합니다.</p>\n</li>\n<li>\n<p><strong>주의사항:</strong> 토큰 단위로 강제로 자르기 때문에, 단어 중간이나 문장 중간에서 끊길 수 있어 <code class=\"language-text\">RecursiveCharacterTextSplitter.from_tiktoken_encoder</code>와 같이 <strong>구조적 분할과 토큰 계산을 결합</strong>하여 사용하는 것이 일반적입니다.</p>\n</li>\n<li>\n<p>코드</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_text_splitters <span class=\"token keyword\">import</span> TokenTextSplitter\n\ntext_splitter <span class=\"token operator\">=</span> TokenTextSplitter<span class=\"token punctuation\">(</span>\n    chunk_size<span class=\"token operator\">=</span><span class=\"token number\">300</span><span class=\"token punctuation\">,</span>\n    chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 텍스트를 청크로 분할</span>\ntexts <span class=\"token operator\">=</span> text_splitter<span class=\"token punctuation\">.</span>split_text<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>texts<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 분할된 텍스트의 첫 번째 청크를 출력합니다.</span>\n\n<span class=\"token comment\"># ⭐️ '연�' 와 같이 글자가 깨져 보이는 것은 BPE(Bite Per Word)를 사용하여 토큰화하기 때문입니다. ⭐️</span></code></pre></div>\n</li>\n</ul>\n<p> </p>\n<h3 id=\"3-토크나이저tokenizer-상세-분류-및-통합-전략-️\" style=\"position:relative;\"><a href=\"#3-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80tokenizer-%EC%83%81%EC%84%B8-%EB%B6%84%EB%A5%98-%EB%B0%8F-%ED%86%B5%ED%95%A9-%EC%A0%84%EB%9E%B5-%EF%B8%8F\" aria-label=\"3 토크나이저tokenizer 상세 분류 및 통합 전략 ️ permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 토크나이저(Tokenizer) 상세 분류 및 통합 전략 ⭐️</h3>\n<p>어떤 모델을 쓰느냐에 따라 토큰 계산 방식이 다르므로, 적절한 토크나이저를 선택해야 합니다.</p>\n<h4 id=\"tiktoken-openai-계열\" style=\"position:relative;\"><a href=\"#tiktoken-openai-%EA%B3%84%EC%97%B4\" aria-label=\"tiktoken openai 계열 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tiktoken (OpenAI 계열)</h4>\n<p>OpenAI 모델을 사용한다면 사실상 **표준(Standard)**입니다.</p>\n<ul>\n<li>\n<p><strong>연결 방식:</strong> <code class=\"language-text\">from_tiktoken_encoder()</code> 메서드를 통해 <code class=\"language-text\">RecursiveCharacterTextSplitter</code> 등과 결합하여 사용합니다.</p>\n</li>\n<li>\n<p><strong>대상 모델:</strong> GPT-3.5, GPT-4, GPT-4o, GPT-5 계열 모델</p>\n</li>\n<li>\n<p><strong>핵심 특징:</strong></p>\n<ul>\n<li><strong>BPE(Bite Pair Encoding) 토크나이저</strong>: UTF-8 바이트 단위까지 쪼개어 인식하기 때문에 이모지, 특수 기호, 깨진 문자열, 바이너리 데이터가 섞인 텍스트도 에러 없이 토큰화 및 복원이 가능합니다.</li>\n<li><strong>정확성:</strong> OpenAI API가 계산하는 토큰 수와 100% 일치합니다. 비용 계산 및 컨텍스트 길이 관리에 오차가 없습니다.</li>\n<li><strong>성능:</strong> Rust 등으로 최적화되어 있어 매우 빠르고 메모리 효율적입니다.</li>\n<li><strong>강점:</strong> 영어 및 프로그래밍 코드(Code) 텍스트 처리에 최적화되어 있습니다.</li>\n</ul>\n</li>\n<li>\n<p>코드</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_text_splitters <span class=\"token keyword\">import</span> CharacterTextSplitter\n\n<span class=\"token comment\"># tiktoken 인코더를 사용하여 '토큰 수'를 기준으로 길이를 계산하는 CharacterTextSplitter를 생성합니다.</span>\ntext_splitter <span class=\"token operator\">=</span> CharacterTextSplitter<span class=\"token punctuation\">.</span>from_tiktoken_encoder<span class=\"token punctuation\">(</span>\n    chunk_size<span class=\"token operator\">=</span><span class=\"token number\">300</span><span class=\"token punctuation\">,</span>\n    chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 텍스트(file)를 분할하여 '문자열(String) 리스트'로 반환합니다.</span>\ntexts <span class=\"token operator\">=</span> text_splitter<span class=\"token punctuation\">.</span>split_text<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<p> </p>\n<h4 id=\"huggingface-tokenizer-오픈소스-llm-계열\" style=\"position:relative;\"><a href=\"#huggingface-tokenizer-%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-llm-%EA%B3%84%EC%97%B4\" aria-label=\"huggingface tokenizer 오픈소스 llm 계열 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HuggingFace Tokenizer (오픈소스 LLM 계열)</h4>\n<p>Llama, Mistral 등 오픈소스 모델을 로컬이나 별도 서버에 호스팅 할 때 필수적입니다.</p>\n<ul>\n<li>⭐️ <strong>연결 방식:</strong> <code class=\"language-text\">from_huggingface_tokenizer()</code> 메서드에 <code class=\"language-text\">AutoTokenizer</code> 객체 등을 넘겨서 사용합니다.</li>\n<li><strong>대표 클래스:</strong> <code class=\"language-text\">GPT2TokenizerFast</code>, <code class=\"language-text\">LlamaTokenizer</code>, <code class=\"language-text\">LlamaTokenizerFast</code>, <code class=\"language-text\">AutoTokenizer</code> emd</li>\n<li><strong>대상 모델:</strong> LLaMA 2/3, Mistral, Qwen, Gemma, Solar 등 대부분의 오픈소스 LLM</li>\n<li><strong>핵심 특징:</strong>\n<ul>\n<li>⭐️ <strong>Fast Tokenizer:</strong> 대부분 Rust 기반으로 작성된 ‘Fast’ 버전을 지원하여 속도가 매우 빠릅니다.</li>\n<li><strong>유연성:</strong> BPE, SentencePiece, Unigram 등 다양한 토큰화 알고리즘을 모델에 맞춰 자동으로 적용합니다(<code class=\"language-text\">AutoTokenizer</code> 사용 시).</li>\n</ul>\n</li>\n<li>⭐️ <strong>주의 :</strong> 모델마다 사용하는 어휘 사전(Vocab)이 다르므로, 반드시 <strong>사용하려는 LLM 모델과 동일한 토크나이저</strong>를 로드해야 정확한 토큰 수를 맞출 수 있습니다.</li>\n</ul>\n<p> </p>\n<h4 id=\"sentence-transformer-임베딩-특화\" style=\"position:relative;\"><a href=\"#sentence-transformer-%EC%9E%84%EB%B2%A0%EB%94%A9-%ED%8A%B9%ED%99%94\" aria-label=\"sentence transformer 임베딩 특화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Sentence Transformer (임베딩 특화)</h4>\n<ul>\n<li><strong>연결 방식:</strong> <code class=\"language-text\">SentenceTransformersTokenTextSplitter</code></li>\n<li><strong>특징:</strong> LLM의 입력 제한보다는, **임베딩 모델(Embedding Model)의 입력 한계(Max Sequence Length)**를 맞추기 위해 사용합니다. (예: BERT 계열은 보통 512 토큰 제한)</li>\n<li><strong>주의:</strong> 생성용 LLM(예: GPT-4)과 토크나이저가 다르므로, 여기서 계산한 토큰 수가 LLM 입력 토큰 수와 일치하지 않습니다. 주로 <strong>Semantic Chunking</strong>이나 임베딩 벡터 생성 전처리 단계에서 사용됩니다.</li>\n</ul>\n<p> </p>\n<h4 id=\"언어학적규칙-기반-토크나이저-linguisticrule-based\" style=\"position:relative;\"><a href=\"#%EC%96%B8%EC%96%B4%ED%95%99%EC%A0%81%EA%B7%9C%EC%B9%99-%EA%B8%B0%EB%B0%98-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80-linguisticrule-based\" aria-label=\"언어학적규칙 기반 토크나이저 linguisticrule based permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>언어학적/규칙 기반 토크나이저 (Linguistic/Rule-based)</h4>\n<p>토큰 개수보다는 **“문장의 의미적 완결성”**을 중시할 때 사용합니다.</p>\n<p>(코드는 이전과 비슷한 방식으로 진행하면 됩니다.)</p>\n<ul>\n<li><strong>spaCy (<code class=\"language-text\">SpacyTextSplitter</code>)</strong>\n<ul>\n<li><strong>특징:</strong> 정교한 NLP 파이프라인을 사용하여 문장 경계를 매우 정확하게 인식합니다.</li>\n<li><strong>단점:</strong> 딥러닝 모델 로드가 필요해 무겁고 속도가 느릴 수 있습니다.</li>\n</ul>\n</li>\n<li><strong>NLTK (<code class=\"language-text\">NLTKTextSplitter</code>)</strong>\n<ul>\n<li><strong>특징:</strong> 전통적인 규칙 기반 분할로 연구나 교육 목적으로 주로 쓰입니다.</li>\n<li><strong>주의:</strong> LLM 토큰 기준과는 호환성이 없습니다.</li>\n</ul>\n</li>\n<li><strong>KoNLPy (<code class=\"language-text\">KonlpyTextSplitter</code>)</strong>\n<ul>\n<li><strong>특징:</strong> 한국어의 특성(교착어)을 반영하여 형태소 단위로 분석합니다. 한국어의 의미적 단위를 보존하는 데 유리합니다.</li>\n<li><strong>주의 (Trade-off):</strong>\n<ol>\n<li><strong>속도:</strong> Tiktoken이나 HuggingFace Fast Tokenizer에 비해 분석 속도가 현저히 느립니다.</li>\n<li><strong>불일치:</strong> 형태소 개수와 LLM의 토큰 개수는 다릅니다. 따라서 이를 기준으로 자른 뒤, LLM에 넣을 때는 토큰 초과 여부를 다시 확인해야 할 수도 있습니다.</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<p> </p>\n<p> </p>\n<h2 id=\"시맨틱-청킹-semantic-chunking\" style=\"position:relative;\"><a href=\"#%EC%8B%9C%EB%A7%A8%ED%8B%B1-%EC%B2%AD%ED%82%B9-semantic-chunking\" aria-label=\"시맨틱 청킹 semantic chunking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>시맨틱 청킹 (Semantic Chunking)</h2>\n<hr>\n<p><code class=\"language-text\">Semantic Chunking</code> 방법은 별도로 정리하고자 합니다.</p>\n<p><strong><code class=\"language-text\">Semantic Chunking</code>은 앞 뒤 문장의 시맨틱 거리(Semantic Distance)를 계산하여 특정 임계치(<code class=\"language-text\">breakpoint_threshold_type</code>, <code class=\"language-text\">breakpoint_threshold_amount</code>)를 넘지 않을 때까지 한 청크로 묶는 방법입니다.</strong></p>\n<p>즉, <strong>연속된 유사한 문장들을 묶어주는 방법</strong>이라고 할 수 있습니다.</p>\n<p>바로 코드로 보자면</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_experimental<span class=\"token punctuation\">.</span>text_splitter <span class=\"token keyword\">import</span> SemanticChunker\n<span class=\"token keyword\">from</span> langchain_openai<span class=\"token punctuation\">.</span>embeddings <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token comment\"># OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.</span>\n<span class=\"token comment\"># ⭐️ 청크 사이즈, 청크 오버랩은 설정하지 않음 ⭐️</span>\ntext_splitter <span class=\"token operator\">=</span> text_splitter <span class=\"token operator\">=</span> SemanticChunker<span class=\"token punctuation\">(</span>\n    <span class=\"token comment\"># OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.</span>\n    OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 분할 기준점 유형을 백분위수로 설정합니다.</span>\n    breakpoint_threshold_type<span class=\"token operator\">=</span><span class=\"token string\">\"percentile\"</span><span class=\"token punctuation\">,</span>\n    breakpoint_threshold_amount<span class=\"token operator\">=</span><span class=\"token number\">95</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\ndocs <span class=\"token operator\">=</span> text_splitter<span class=\"token punctuation\">.</span>create_documents<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>docs<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>page_content<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>\n<p><strong><code class=\"language-text\">breakpoint_threshold_type</code>: 분할하는 기준 ⭐️</strong></p>\n<ul>\n<li><code class=\"language-text\">percentile</code>: 백분위수(디폴트)\n<ul>\n<li>현재 문장의 이전 문장과의 시맨틱 거리가 백분위수를 넘지 않는 곳까지 청크를 나눕니다.</li>\n</ul>\n</li>\n<li><code class=\"language-text\">standard_deviation</code>: 표준편차</li>\n<li><code class=\"language-text\">interquartile</code>: 사분위수 범위(IQR)\n<ul>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>임계값</mtext><mo stretchy=\"false\">(</mo><mi>T</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>Mean(평균)</mtext><mo>+</mo><mo stretchy=\"false\">(</mo><mtext>amount</mtext><mo>×</mo><mtext>IQR</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">임계값(Threshold) = \\text{Mean(평균)} + (\\text{amount} \\times \\text{IQR})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord hangul_fallback\">임계값</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">res</span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">d</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">Mean(</span><span class=\"mord hangul_fallback\">평균</span><span class=\"mord\">)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">amount</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">IQR</span></span><span class=\"mclose\">)</span></span></span></span></span><br>값이 <code class=\"language-text\">breakpoint_threshold_amount</code>을 넘지 않는 곳까지 청크를 나눕니다.</li>\n<li>평균과 IQR은 현재 문장의 이전 문장간의 시맨틱 거리를 구한 값들에서 구합니다.</li>\n</ul>\n</li>\n<li><code class=\"language-text\">gradient</code>: 기울기(그래디언트)</li>\n</ul>\n</li>\n<li>\n<p><strong><code class=\"language-text\">breakpoint_threshold_amount</code>: 그 기준의 임계치 ⭐️</strong></p>\n<ul>\n<li><strong>임계치가 높을수록, 시맨틱 거리가 먼 것(의미적 유사성이 덜한 것)도 한 청크로 묶이므로 청크 개수가 적어진다.</strong> ⭐️</li>\n<li><strong>임계치가 낮을수록, 시맨틱 거리가 짧은 것(의미적 유사성이 깊은 것)들끼리 청크로 묶이므로 청크 개수가 많아진다.</strong> ⭐️</li>\n</ul>\n</li>\n<li>\n<p>아래는 <code class=\"language-text\">percentile</code> 기준에서 <code class=\"language-text\">breakpoint_threshold_amount</code>에 대한 이해를 높이기 위한 그림입니다.<br>(임계치는 <code class=\"language-text\">percentile</code> 값이 95인 곳입니다.)</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 75.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAADzklEQVQ4yzWU7U9bBRTG+9eZ+HlqplNnjIkfZnQsJia+ZFmmbhJlbqsM5gYMxpqxUUov7W17720vK7S0wGBQ2tt7+/56W1p6+zKHDvIzMD1fnk/nl5PnyXNsAP1+n79evsLqvsQwdDK6Tr5QJJczyGYNEgkNwyiQzeXJpFIkEwlSuk5KS7Gn7ZHWNax2i+OjI2yHh3/TH3TJ5w7wqV0i2xLVlTAxrYxZUNhL+RDFGgH/a5RIhq4kUIysIVfqhDdVPGk/yoaL6PRvmDkNW61Wp9NpUcx38S8fsp4M0YnH2cw36TfCZHIyitIjFIRn6xWOVJHW5jaBWhN1XUEqq6hbbtYmr1M39t4ALeuArLHPE3cFOerG8ImI0W3KusjGcyeCUMOz9ApBSmEuOChHY9wPR7g5cRsxp+BRZ5FGr1DSdrA1Go1ToKE3cbjK+FZcxBfcOIIxstoSsc153EKdhcUBjzxJik9nKcfWGV+Ncf3eH3jzQdzLDqSxq2+AlUqVdnufvLGPe7GML+hk1+XBpT6nlhSIr83jcRZQ3H1mF17QmHfQCK8ytbbFnUczhNIiguxAtV+hnt49ubBJr29RLViI3hauoIDqlHGEkmR3/XjURR7P5Vj1vWZ6MYHpdpFXwtyOprnjEgkkRHzheWLjP7Of+c/DdrtJPt9h2tvD7pexz4b4eiHBQzWE3e/n6sMsM8IhY74CfqeIx73KBSHBN4+CXFODzK54CE2OUM5o2Kq1GlanhZZpMvK4ztC0n0tjEmfGorw17OGLKZGv7u7y/WSREafOJbvAhZt+zk2scX5U4f0JkR/mBL4dvstmMoPNbDTo9zokM/tcnslxbjTA28N+PhiP8I7dy7u3vXx+Z5uPbr3gQ/saZ6+5OXtT5ic5xSfjCu/dE/ns/hIf/zjFdtLAVq+b9Kw2kZ0SQxNJrrpkzvzqZ+hhnKmgxPkxkU/tcS4/KHNxcouhG26+mw5z/3mBLyckhhYCXJwRsN8aRUvrbzzsdlpsJuvc9TTZSK/yYE5lQk6jG8tMShJ/OgvMe4+YkfIEniwhS3GWik1+d6s8TQWZDXvw3btBydCwWZbFYNClkLOYE0yklXky6jMiaZOD6jIpQ8brayMGQImWGMgequtbhPYPkDfCBEoKy9suNh78gnmS8slzaLdbFPIVjGwbPatR2Etgti0qxT1yuV1KxR5bW1XypQZtPUV2N0HJ6pJI7aBVdYr1HDuREINuB9vx8fEJk//1ZA4si4ZpUjeblEqV066bZoVms0HVNCmWy3RaLQqFIoPe4HTn8J+jU/0XmIXn/KvtBCkAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"img 2\" title=\"img 2\" src=\"/static/024de6fd582fe8606ef7d325e1f68f42/37523/img_2.png\" srcset=\"/static/024de6fd582fe8606ef7d325e1f68f42/e9ff0/img_2.png 180w,\n/static/024de6fd582fe8606ef7d325e1f68f42/f21e7/img_2.png 360w,\n/static/024de6fd582fe8606ef7d325e1f68f42/37523/img_2.png 720w,\n/static/024de6fd582fe8606ef7d325e1f68f42/58213/img_2.png 902w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p>(출처: <a href=\"https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\">FullStackRetrieval-com Github의 gkamradt notebook</a>)</p>\n<ul>\n<li><code class=\"language-text\">x</code>축은 에세이의 첫 문장부터 마지막 문장까지 순서대로 나열한 것입니다.</li>\n<li><code class=\"language-text\">y</code>축은 <strong>현재 문장의 바로 이전 문장과의 시맨틱 거리</strong>(코사인 거리)입니다.<br>(<strong>거리가 높다는 것은 의미적 유사도가 낮다</strong>(즉, 주제가 급격히 바뀌었다)는 뜻입니다.)</li>\n<li><strong>빨간 줄(임계치)를 위로 넘는 부분에서 시맨틱 거리의 백분위수 95를 넘는 곳</strong>입니다. 그림에서 확인할 수 있듯이, <strong>빨간 줄 위로 넘는 부분들에 대해 그 이전 백분위수 95를 넘는 곳에서 해당 부분까지 Chunk가 형성되는 것</strong>입니다.</li>\n<li>결국, <strong>순서대로 나열된 문장들이 이전 문장과의 시맨틱 거리(코사인 거리)의 <code class=\"language-text\">percentile</code>이 95가 넘을 때 Chunk를 끊는 형태</strong>입니다.</li>\n<li>당연히 <strong>청크별 길이가 크게 차이가 날 수 있습니다.</strong></li>\n</ul>\n</li>\n</ul>\n<p> </p>\n<p>그런데,</p>\n<blockquote>\n<p><strong>Q. 계속 유사한 내용이 이어지면서 내용 주제가 서서히 바뀌고 있을 때는 어떡하는 게 좋을까요?</strong></p>\n<p>개인적으로 저는 <strong><code class=\"language-text\">SemanticChunker</code> 클래스를 부모로 상속받아서 최대 청크 사이즈(<code class=\"language-text\">max_chunk_size</code>)를 설정하는 것이 좋을 것으로 보입니다.</strong><br>(<code class=\"language-text\">min_chunk_size</code>는 현재 존재하지만 없더라고요..)</p>\n<p>아래처럼 클래스가 정의되어 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">SemanticChunker</span><span class=\"token punctuation\">(</span>BaseDocumentTransformer<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Split the text based on semantic similarity.\n\n    Taken from Greg Kamradt's wonderful notebook:\n    https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n\n    All credits to him.\n\n    At a high level, this splits into sentences, then groups into groups of 3\n    sentences, and then merges one that are similar in the embedding space.\n    \"\"\"</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>\n        self<span class=\"token punctuation\">,</span>\n        embeddings<span class=\"token punctuation\">:</span> Embeddings<span class=\"token punctuation\">,</span>\n        buffer_size<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n        add_start_index<span class=\"token punctuation\">:</span> <span class=\"token builtin\">bool</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n        breakpoint_threshold_type<span class=\"token punctuation\">:</span> BreakpointThresholdType <span class=\"token operator\">=</span> <span class=\"token string\">\"percentile\"</span><span class=\"token punctuation\">,</span>\n        breakpoint_threshold_amount<span class=\"token punctuation\">:</span> Optional<span class=\"token punctuation\">[</span><span class=\"token builtin\">float</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n        number_of_chunks<span class=\"token punctuation\">:</span> Optional<span class=\"token punctuation\">[</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n        sentence_split_regex<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span> <span class=\"token operator\">=</span> <span class=\"token string\">r\"(?&lt;=[.?!])\\s+\"</span><span class=\"token punctuation\">,</span>\n        min_chunk_size<span class=\"token punctuation\">:</span> Optional<span class=\"token punctuation\">[</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>_add_start_index <span class=\"token operator\">=</span> add_start_index\n        self<span class=\"token punctuation\">.</span>embeddings <span class=\"token operator\">=</span> embeddings\n        self<span class=\"token punctuation\">.</span>buffer_size <span class=\"token operator\">=</span> buffer_size\n        self<span class=\"token punctuation\">.</span>breakpoint_threshold_type <span class=\"token operator\">=</span> breakpoint_threshold_type\n        self<span class=\"token punctuation\">.</span>number_of_chunks <span class=\"token operator\">=</span> number_of_chunks\n        self<span class=\"token punctuation\">.</span>sentence_split_regex <span class=\"token operator\">=</span> sentence_split_regex\n        <span class=\"token keyword\">if</span> breakpoint_threshold_amount <span class=\"token keyword\">is</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>breakpoint_threshold_amount <span class=\"token operator\">=</span> BREAKPOINT_DEFAULTS<span class=\"token punctuation\">[</span>\n                breakpoint_threshold_type\n            <span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>breakpoint_threshold_amount <span class=\"token operator\">=</span> breakpoint_threshold_amount\n        self<span class=\"token punctuation\">.</span>min_chunk_size <span class=\"token operator\">=</span> min_chunk_size\n        \n        <span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span> 이하 생략<span class=\"token punctuation\">)</span></code></pre></div>\n<p>아래 함수가 정의되어 있는데, 이를 정의하면 되지 않을까 합니다.</p>\n</blockquote>\n<p> </p>\n<p> </p>\n<h2 id=\"코드-분할기-code-splitter\" style=\"position:relative;\"><a href=\"#%EC%BD%94%EB%93%9C-%EB%B6%84%ED%95%A0%EA%B8%B0-code-splitter\" aria-label=\"코드 분할기 code splitter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>코드 분할기 (Code Splitter)</h2>\n<hr>\n<p>cpp, go, java, python, js, ts, html markdown, powershell 등의 언어로 코드 분할도 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_text_splitters <span class=\"token keyword\">import</span> Language<span class=\"token punctuation\">,</span> RecursiveCharacterTextSplitter\n\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>e<span class=\"token punctuation\">.</span>value <span class=\"token keyword\">for</span> e <span class=\"token keyword\">in</span> Language<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>를 이용해 지원하는 모든 언어를 확인할 수 있습니다.</p>\n<p> </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">python_splitter <span class=\"token operator\">=</span> RecursiveCharacterTextSplitter<span class=\"token punctuation\">.</span>from_language<span class=\"token punctuation\">(</span>\n    language<span class=\"token operator\">=</span>Language<span class=\"token punctuation\">.</span>PYTHON<span class=\"token punctuation\">,</span> chunk_size<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">,</span> chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">0</span>\n<span class=\"token punctuation\">)</span>\n\npython_docs <span class=\"token operator\">=</span> python_splitter<span class=\"token punctuation\">.</span>create_documents<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>PYTHON_CODE<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\npython_docs</code></pre></div>\n<p>텍스트 분할기로 이용되었던 <strong><code class=\"language-text\">RecursiveCharacterTextSplitter</code>의 <code class=\"language-text\">from_language()</code> 메서드에 <code class=\"language-text\">language</code>, <code class=\"language-text\">chunk_size</code>, <code class=\"language-text\">chunk_overlap</code> 등의 값을 전달하여 spiltter 객체를 생성하고,<br>splitter 객체의 <code class=\"language-text\">create_documents()</code> 메서드를 통해 Document로 분할이 가능합니다.</strong></p>\n<p> </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">RecursiveCharacterTextSplitter<span class=\"token punctuation\">.</span>get_separators_for_language<span class=\"token punctuation\">(</span>Language<span class=\"token punctuation\">.</span>PYTHON<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 출력: ['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']</span></code></pre></div>\n<p>을 통해, 해당하는 언어의 구분자들은 어떻게 이루어져 있는지 확인 가능합니다.</p>\n<p> </p>\n<p> </p>\n<h2 id=\"json-분할기-recursivejsonsplitter\" style=\"position:relative;\"><a href=\"#json-%EB%B6%84%ED%95%A0%EA%B8%B0-recursivejsonsplitter\" aria-label=\"json 분할기 recursivejsonsplitter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>JSON 분할기 (RecursiveJsonSplitter)</h2>\n<hr>\n<p>JSON 분할기 <code class=\"language-text\">RecursiveJsonSplitter</code>은 데이터를 자를 때 <strong>JSON의 문법(Key-Value 쌍, 리스트 구조)을 최대한 유지하며 자릅니다.</strong> (그렇기 때문에 청크 사이즈가 서로 크게 다를 수 있습니다.)</p>\n<p>(안 그래도 JSON 파일을 분할할 경우가 있었는데 이걸 이용해야겠습니다 👍🏻)</p>\n<p> </p>\n<p><strong>재귀적 분할 (Recursive) 방식</strong>으로 JSON을 분할합니다.</p>\n<ol>\n<li>전체 JSON 객체를 봅니다.</li>\n<li>설정된 크기(<code class=\"language-text\">max_chunk_size</code>)보다 크면, 내부의 **키(Key)**나 <strong>리스트(List)의 요소</strong> 단위로 더 깊이 들어가서 쪼갭니다. (깊이우선탐색 DFS 알고리즘과 같은 방식입니다.)</li>\n<li>이 과정을 반복하여 각 조각이 제한 크기 안에 들어오도록 만듭니다.</li>\n</ol>\n<p> </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_text_splitters <span class=\"token keyword\">import</span> RecursiveJsonSplitter\n\n<span class=\"token comment\"># 재귀적 JSON 분할기 생성</span>\n<span class=\"token comment\"># 청크 최대 사이즈 300</span>\nsplitter <span class=\"token operator\">=</span> RecursiveJsonSplitter<span class=\"token punctuation\">(</span>max_chunk_size<span class=\"token operator\">=</span><span class=\"token number\">300</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># JSON -> JSON 분할</span>\njson_chunks <span class=\"token operator\">=</span> splitter<span class=\"token punctuation\">.</span>split_json<span class=\"token punctuation\">(</span>json_data<span class=\"token operator\">=</span>json_data<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># JSON -> Document 분할</span>\ndocs <span class=\"token operator\">=</span> splitter<span class=\"token punctuation\">.</span>create_documents<span class=\"token punctuation\">(</span>texts<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>json_data<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># JSON -> String 분할</span>\ntexts <span class=\"token operator\">=</span> splitter<span class=\"token punctuation\">.</span>split_text<span class=\"token punctuation\">(</span>json_data<span class=\"token operator\">=</span>json_data<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># JSON -> Document 분할 + ⭐️리스트 변형하여 분할⭐️</span>\ndocs <span class=\"token operator\">=</span> splitter<span class=\"token punctuation\">.</span>create_documents<span class=\"token punctuation\">(</span>texts<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>json_data<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> convert_lists<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li><code class=\"language-text\">RecursiveJsonSplitter</code> 객체 생성 시 2가지 설정이 가능합니다.\n<ul>\n<li><code class=\"language-text\">max_chunk_size</code></li>\n<li><code class=\"language-text\">min_chunk_size</code></li>\n</ul>\n</li>\n<li><strong><code class=\"language-text\">RecursiveJsonSplitter</code>의 <code class=\"language-text\">split_json()</code>, <code class=\"language-text\">create_documents()</code>, <code class=\"language-text\">split_text()</code> 메서드를 이용해 분할이 가능합니다.</strong>\n<ul>\n<li><strong><code class=\"language-text\">convert_lists=True</code> 설정을 통해 리스트를 각각 값을 나눠가지며 JSON 분할할 수 있습니다.<br>(기본적으로 깊이 탐색하다가 리스트를 만나면 그 묶음을 그대로 가져옵니다.)</strong></li>\n</ul>\n</li>\n</ul>\n<p> </p>\n<p> </p>\n<p>다른 것들도 더 있지만<br>이 정도까지만 정리하고자 합니다. 끄읕.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EB%AC%B8%EC%84%9C-%EB%A1%9C%EB%8D%94-loader\">문서 로더 (Loader)</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%B2%AD%ED%82%B9-chunking\">청킹 (Chunking)</a></p>\n<ul>\n<li><a href=\"#1-%EC%B2%AD%ED%82%B9%EC%9D%84-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0\">1. 청킹을 하는 이유</a></li>\n<li><a href=\"#2-text-splitter%EC%99%80-tokenizer\">2. Text Splitter와 Tokenizer</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%ED%95%A0%EA%B8%B0%EC%99%80-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80\">텍스트 분할기와 토크나이저</a></p>\n<ul>\n<li>\n<p><a href=\"#1-%EB%AC%B8%EC%9E%90-%EB%B0%8F-%EA%B5%AC%EC%A1%B0-%EA%B8%B0%EB%B0%98-%EB%B6%84%ED%95%A0%EA%B8%B0\">1. 문자 및 구조 기반 분할기</a></p>\n<ul>\n<li><a href=\"#recursivecharactertextsplitter-%EF%B8%8F\">RecursiveCharacterTextSplitter ⭐️</a></li>\n<li><a href=\"#charactertextsplitter\">CharacterTextSplitter</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-%ED%86%A0%ED%81%B0-%EA%B8%B0%EB%B0%98-%EB%B6%84%ED%95%A0%EA%B8%B0-tokentextsplitter\">2. 토큰 기반 분할기 (TokenTextSplitter)</a></p>\n</li>\n<li>\n<p><a href=\"#3-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80tokenizer-%EC%83%81%EC%84%B8-%EB%B6%84%EB%A5%98-%EB%B0%8F-%ED%86%B5%ED%95%A9-%EC%A0%84%EB%9E%B5-%EF%B8%8F\">3. 토크나이저(Tokenizer) 상세 분류 및 통합 전략 ⭐️</a></p>\n<ul>\n<li><a href=\"#tiktoken-openai-%EA%B3%84%EC%97%B4\">tiktoken (OpenAI 계열)</a></li>\n<li><a href=\"#huggingface-tokenizer-%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-llm-%EA%B3%84%EC%97%B4\">HuggingFace Tokenizer (오픈소스 LLM 계열)</a></li>\n<li><a href=\"#sentence-transformer-%EC%9E%84%EB%B2%A0%EB%94%A9-%ED%8A%B9%ED%99%94\">Sentence Transformer (임베딩 특화)</a></li>\n<li><a href=\"#%EC%96%B8%EC%96%B4%ED%95%99%EC%A0%81%EA%B7%9C%EC%B9%99-%EA%B8%B0%EB%B0%98-%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80-linguisticrule-based\">언어학적/규칙 기반 토크나이저 (Linguistic/Rule-based)</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%8B%9C%EB%A7%A8%ED%8B%B1-%EC%B2%AD%ED%82%B9-semantic-chunking\">시맨틱 청킹 (Semantic Chunking)</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%BD%94%EB%93%9C-%EB%B6%84%ED%95%A0%EA%B8%B0-code-splitter\">코드 분할기 (Code Splitter)</a></p>\n</li>\n<li>\n<p><a href=\"#json-%EB%B6%84%ED%95%A0%EA%B8%B0-recursivejsonsplitter\">JSON 분할기 (RecursiveJsonSplitter)</a></p>\n</li>\n</ul>\n</div>","excerpt":"참고 : 테디노트의 RAG 비법노트 (https://fastcampus.co.kr/data_online_teddy)소스코드: https://github.com/teddylee777/langchain-kr위키독스: https://wikidocs.net/book/14314   오늘도 와쓰요~오늘은 문서 로드 방식과 청킹에 대해 정리해보고자 합니다. 바로 레츠기릿~!   문서 로더 (Loader) 문서 로드 방식을 간단하게 정리하고자 합니다. Document 생성 문서 로드  메서드  메서드: 를 설정해 줌  메서드: 각 문서를 generator 방식으로 읽으면서 버림(문서의 양이 많을 때 유용함)  메서드: 비동기 방식으로 문서를 로드함   문서 로드 방식은 파일 형식이 어떠하든 그 방식이 비슷합니다. 파일 형식에 맞게 loader 설정 후,,  등의 메서드를 이용하여 로드하면 됩니다.     청킹 (Chunking) RAG에서 텍스트를 청킹하는 것은 중요합니다. 아. 청킹은 문서 내…","frontmatter":{"date":"January 01, 2026","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-11","categories":"LLM RAG","author":"변우중","emoji":"☀️"},"fields":{"slug":"/26-01-01_1/"}},"next":{"id":"16d4151c-c0f0-5202-8651-703654b9bb3c","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy)\">https://fastcampus.co.kr/data_online_teddy)</a><br>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a><br>위키독스: <a href=\"https://wikidocs.net/book/14314\">https://wikidocs.net/book/14314</a></p>\n<p> </p>\n<p>이번에는 ’<strong>LCEL에서 메모리 사용하는 방법</strong>‘을 간단히 정리하고,<br>’<strong>SQLite DB를 이용한 메모리 저장하는 방법</strong>‘을 정리해보고자 합니다.</p>\n<p>상당히 복잡해보여서 연습이 중요할 것 같습니다.</p>\n<p>(<strong>주의‼️ 마음의 준비를 하고, 심호흡 한번 크게 하고 들어가야 합니다.</strong>)</p>\n<p>레츠기릿~!</p>\n<p> </p>\n<h2 id=\"lcel에서-메모리-사용하는-방법\" style=\"position:relative;\"><a href=\"#lcel%EC%97%90%EC%84%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\" aria-label=\"lcel에서 메모리 사용하는 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LCEL에서 메모리 사용하는 방법</h2>\n<hr>\n<ol>\n<li>\n<p><strong>프롬프트 템플릿에 <code class=\"language-text\">MessagesPlaceholder(variable_name=\"chat_history\")</code> 박아두기</strong>!</p>\n</li>\n<li>\n<p><strong><code class=\"language-text\">memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")</code> 메모리 생성하면서<br><code class=\"language-text\">memory_key</code>의 값과 <code class=\"language-text\">MessagesPlaceholder</code>에 들어가는 <code class=\"language-text\">variable_name</code> 값을 동일하게 하기</strong></p>\n<ul>\n<li><strong>메모리에서 지정한 <code class=\"language-text\">memory_key</code>는 프롬프트 템플릿에서 이전 대화 자리에 박아두는 변수명과 같아야 함</strong></li>\n</ul>\n</li>\n<li>\n<p><strong><code class=\"language-text\">RunnablePassthrough.assign()</code>에서 <code class=\"language-text\">RunnableLambda(memory.load_memory_variables) | itemgetter(memory.memory_key)</code>을 <code class=\"language-text\">chat_history</code>에 지정한 <code class=\"language-text\">runnable</code>을 만들어 chain에서 이용함</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>runnables <span class=\"token keyword\">import</span> RunnableLambda<span class=\"token punctuation\">,</span> RunnablePassthrough\n\nrunnable <span class=\"token operator\">=</span> RunnablePassthrough<span class=\"token punctuation\">.</span>assign<span class=\"token punctuation\">(</span>  chat_history<span class=\"token operator\">=</span>RunnableLambda<span class=\"token punctuation\">(</span>memory<span class=\"token punctuation\">.</span>load_memory_variables<span class=\"token punctuation\">)</span>\n    <span class=\"token operator\">|</span> itemgetter<span class=\"token punctuation\">(</span><span class=\"token string\">\"chat_history\"</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># RunnableLambda(memory.load_memory_variables)의 반환값에서 chat_history만 추출</span>\n    <span class=\"token comment\"># | itemgetter(memory.memory_key) # 위와 똑같음</span>\n<span class=\"token punctuation\">)</span>\n\nrunnable<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"input\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"hi\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># {'input': 'hi'}과 chat_history 값이 합쳐져 아래의 프롬프트 템플릿의 변수값에 들어갈 것임 ({'input': 'hi', 'chat_history': [대화들]} 꼴)</span></code></pre></div>\n<ul>\n<li><strong><code class=\"language-text\">RunnableLambda(memory.load_memory_variables)</code>는 <code class=\"language-text\">memory.load_memory_variables({})</code>와 같은 것임</strong>\n<ul>\n<li>즉, <code class=\"language-text\">chat_history</code> 키의 값에 <strong>대화들 리스트</strong>를 담은 딕셔너리일 것임</li>\n</ul>\n</li>\n<li>그리고, ⭐️ <strong><code class=\"language-text\">memory.load_memory_variables({})</code>의 결과값(대화들 담긴 딕셔너리)에서 <code class=\"language-text\">memory.memory_key</code>값(‘chat_history’ 키의 값, 즉 대화들)만 추출해서 <code class=\"language-text\">chat_history</code> 변수에 전달함</strong> ⭐️\n<ul>\n<li><code class=\"language-text\">memory.load_memory_variables({})</code><br>: {‘chat_history’: [대화들]} 꼴</li>\n<li><code class=\"language-text\">memory.memory_key</code>값(‘chat_history’ 키의 값)만 추출<br>: [대화들] 꼴</li>\n<li><code class=\"language-text\">chat_history</code> 변수에 전달한 후의 최종 <code class=\"language-text\">RunnablePassthrough.assign()</code> 값<br>: {‘chat_history’: [대화들]}</li>\n<li><code class=\"language-text\">runnable.invoke({\"input\": \"hi\"})</code> 값<br>: <code class=\"language-text\">{'input': 'hi'}</code>과 <code class=\"language-text\">chat_history</code> 값이 합쳐져 <code class=\"language-text\">{'input': 'hi', 'chat_history': [대화들]}</code> 꼴이 됨</li>\n</ul>\n</li>\n</ul>\n<p> </p>\n<p>⭐️ **결국, runnable를 invoke 호출했을 때<br><code class=\"language-text\">{'input': 'hi', 'chat_history': [대화들]}</code>이 되므로 각각 사용자 입력값과 이전 대화로 프롬프트 템플릿에서 이용할 수 있습니다!!**⭐️</p>\n<p>(좀 복잡하지만 그 과정을 명확히 알고 있어야 본격적으로 chain에 연결할 때 헷갈리지 않을 것 같습니다..)</p>\n</li>\n</ol>\n<p> </p>\n<p>방금 만들었던 **이전 대화와 새로운 질문을 반환하는 <code class=\"language-text\">runnable</code>**을 가장 맨 앞에 붙여<br>그 결과를 프롬프트에 전달하는 ‘runnable-프롬프트-LLM-파서’ 이렇게 chain을 구성하면 될 듯 합니다.</p>\n<p> </p>\n<p> </p>\n<h2 id=\"sqlite-db를-이용한-메모리-저장-방법\" style=\"position:relative;\"><a href=\"#sqlite-db%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%A0%80%EC%9E%A5-%EB%B0%A9%EB%B2%95\" aria-label=\"sqlite db를 이용한 메모리 저장 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SQLite DB를 이용한 메모리 저장 방법</h2>\n<hr>\n<p>이전까지는 인메모리에 캐시를 저장하는 방법을 정리해봤는데<br>사실, 대화 세션을 일회성으로 이용하는 것이면 데이터베이스에 저장하지 않아도 되긴 합니다.</p>\n<p>하지만, 이전 대화를 나중에 다시 사용해야 한다면 데이터베이스에 유저별 테이블 형식으로 저장해두고, 그 기반으로 답변을 할 수 있도록 하는 것이 좋습니다.</p>\n<p> </p>\n<p>흐름부터 정리하겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">invoke() 호출\n    ↓\nconfig에서 user_id, session_id 추출 (history_factory_config 참조)\n    ↓\nget_chat_history(user_id, session_id) 호출 → ChatMessageHistory 반환\n    ↓\nchat_history를 프롬프트에 주입 (history_messages_key 사용)\n    ↓\nquestion을 프롬프트에 주입 (input_messages_key 사용)\n    ↓\nLLM 실행 → 응답 반환\n    ↓\n새 대화(질문+응답)를 ChatMessageHistory에 자동 저장</code></pre></div>\n<p>처음에 질문을 했을 때 config에서 어떠한 데이터베이스에서 어떠한 테이블에서 어떠한 컬럼값을 참고할지를 정합니다.</p>\n<p>이후, 해당 config를 참고하여 이전 대화를 가져온 후 프롬프트에 넣습니다.</p>\n<p>그리고, 처음 입력한 질문을 프롬프트에 넣으면서 LLM이 실행하고 응답을 받습니다.</p>\n<p>마지막에 그 새로운 대화를 해당 데이터베이스의 테이블에 데이터로 저장합니다.</p>\n<p> </p>\n<p>코드로 자세하게 봅시다.</p>\n<ol>\n<li>\n<p>**<code class=\"language-text\">ConfigurableFieldSpec</code>**을 이용해 <strong>DB 조회할 파라미터(컬럼명 등)를 설정</strong>해두어야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>runnables<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> ConfigurableFieldSpec\n\nconfig_fields <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    ConfigurableFieldSpec<span class=\"token punctuation\">(</span>\n        <span class=\"token builtin\">id</span><span class=\"token operator\">=</span><span class=\"token string\">\"user_id\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># ⭐️ get_chat_history 함수에서 사용할 파라미터 ⭐️</span>\n        annotation<span class=\"token operator\">=</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span>\n        name<span class=\"token operator\">=</span><span class=\"token string\">\"User ID\"</span><span class=\"token punctuation\">,</span>\n        description<span class=\"token operator\">=</span><span class=\"token string\">\"Unique identifier for a user.\"</span><span class=\"token punctuation\">,</span>\n        default<span class=\"token operator\">=</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">,</span>\n        is_shared<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    ConfigurableFieldSpec<span class=\"token punctuation\">(</span>\n        <span class=\"token builtin\">id</span><span class=\"token operator\">=</span><span class=\"token string\">\"conversation_id\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># ⭐️ get_chat_history 함수에서 사용할 파라미터 ⭐️</span>\n        annotation<span class=\"token operator\">=</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span>\n        name<span class=\"token operator\">=</span><span class=\"token string\">\"Conversation ID\"</span><span class=\"token punctuation\">,</span>\n        description<span class=\"token operator\">=</span><span class=\"token string\">\"Unique identifier for a conversation.\"</span><span class=\"token punctuation\">,</span>\n        default<span class=\"token operator\">=</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">,</span>\n        is_shared<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">]</span></code></pre></div>\n</li>\n<li>\n<p><strong><code class=\"language-text\">get_chat_history</code></strong> 함수에서 <strong>DB에서 사용할 파라미터명을 인자로 받아</strong>, **<code class=\"language-text\">SQLChatMessageHistory</code>**을 이용해 <strong>DB 메시지 히스토리를 관리</strong>하도록 정의합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_chat_history</span><span class=\"token punctuation\">(</span>user_id<span class=\"token punctuation\">,</span> conversation_id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> SQLChatMessageHistory<span class=\"token punctuation\">(</span>\n        table_name<span class=\"token operator\">=</span>user_id<span class=\"token punctuation\">,</span>          <span class=\"token comment\"># (유저 ID별) 테이블</span>\n        session_id<span class=\"token operator\">=</span>conversation_id<span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 대화 ID 컬럼</span>\n        connection<span class=\"token operator\">=</span><span class=\"token string\">\"sqlite:///sqlite.db\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 데이터베이스 파일명</span>\n    <span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>\n<p><code class=\"language-text\">table_name</code>: (보통 유저 ID별) 테이블명 설정</p>\n</li>\n<li>\n<p><code class=\"language-text\">session_id</code>: 테이블에서 session_id 값에 넣는 대화 세션 ID 값 설정</p>\n</li>\n<li>\n<p><code class=\"language-text\">connection</code>: 데이터베이스 파일명 설정</p>\n</li>\n</ul>\n</li>\n<li>\n<p><code class=\"language-text\">RunnableWithMessageHistory()</code> 객체 생성하면서 <strong>대화 내용을 기록해주는 함수를 기존 체인에 연결</strong>해줍니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>runnables<span class=\"token punctuation\">.</span>history <span class=\"token keyword\">import</span> RunnableWithMessageHistory\n\nchain_with_history <span class=\"token operator\">=</span> RunnableWithMessageHistory<span class=\"token punctuation\">(</span>\n    chain<span class=\"token punctuation\">,</span>\n    get_chat_history<span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 대화 기록을 가져오는 함수를 설정합니다.</span>\n    input_messages_key<span class=\"token operator\">=</span><span class=\"token string\">\"question\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 입력 메시지의 키를 \"question\"으로 설정 (⭐️ chain에서 prompt 템플릿의 입력 변수와 같아야 함 ⭐️)</span>\n    history_messages_key<span class=\"token operator\">=</span><span class=\"token string\">\"chat_history\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 대화 기록 메시지의 키를 \"chat_history\"로 설정 (⭐️ chain에서 prompt 템플릿-메시지플레이스홀더의 입력 변수와 같아야 함 ⭐️)</span>\n    history_factory_config<span class=\"token operator\">=</span>config_fields<span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 대화 기록 조회시 참고할 파라미터를 설정합니다. (⭐️ get_chat_history에서 사용하는 ConfigurableFieldSpece들을 전달하기 위한 것 ⭐️)</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>\n<p><code class=\"language-text\">chain</code>: 기존 체인(프롬프트-LLM-파서 연결)</p>\n</li>\n<li>\n<p><code class=\"language-text\">get_chat_history</code>: DB에서 데이터베이스 파일명-대화 세션-(유저별) 테이블을 찾아가서 메시지 히스토리 관리함</p>\n</li>\n<li>\n<p><code class=\"language-text\">input_messages_key</code>: 기존 체인에서 프롬프트 템플릿에서 사용하는 입력변수 값을 넣음</p>\n</li>\n<li>\n<p><code class=\"language-text\">history_messages_key</code>: 기존 체인에서 프롬프트 템플릿에서 <code class=\"language-text\">MessagesPlaceholder</code>에서 사용하는 변수 값을 넣음</p>\n</li>\n<li>\n<p><code class=\"language-text\">history_factory_config</code>: 대화 기록 조회할 때 참고할 파라미터 정보들 설정</p>\n</li>\n</ul>\n</li>\n<li>\n<p>config에 configurable 키 하위에 get_chat_history에서  <strong>DB 메시지 히스토리를 관리</strong>할 때 사용하는 <strong>파라미터(대화 세션 ID, 테이블명)들을 설정</strong>합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># config 설정</span>\n<span class=\"token triple-quoted-string string\">\"\"\"\nconfigurable 키 하위에 user_id, conversation_id 값을 설정\n(get_chat_history에서 DB 메시지 히스토리 관리할 때 사용하는 값들)\n\"\"\"</span>\nconfig <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"configurable\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"user_id\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"user1\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"conversation_id\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"conversation1\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span></code></pre></div>\n<ul>\n<li>\n<p><strong>table_name 테이블명=user_id=user1</strong></p>\n</li>\n<li>\n<p><strong>session_id 컬럼 값=conversation_id=conversation1</strong></p>\n</li>\n</ul>\n</li>\n<li>\n<p>앞에서 기존 체인과 이전 대화 기록 조회하는 것을 연결한 <strong><code class=\"language-text\">chain_with_history</code>에서 <code class=\"language-text\">invoke()</code> 메서드에 입력변수와 config을 전달하여 호출</strong>합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">chain_with_history<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"question\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"내 이름이 뭐라고?\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> config<span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ol>\n<p> </p>\n<p>굉장히 복잡하고 어려워보이지만<br>갓 테디노트님 강의 들으니 이해하기 쉽더라고요,, 좋슴다…</p>\n<p> </p>\n<p> </p>\n<p>여기서 끄읕.<br>다음은 문서 로드하는 방법인데, 간단히 정리하고 다른 것을 쓰고자 합니다! See ya.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#lcel%EC%97%90%EC%84%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\">LCEL에서 메모리 사용하는 방법</a></li>\n<li><a href=\"#sqlite-db%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%A0%80%EC%9E%A5-%EB%B0%A9%EB%B2%95\">SQLite DB를 이용한 메모리 저장 방법</a></li>\n</ul>\n</div>","frontmatter":{"date":"December 31, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-10","categories":"LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-31_1/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://www.zoomkoding.com","comments":{"utterances":{"repo":"https://github.com/byeonwoojung/byeonwoojung.github.io"}}}}},"pageContext":{"slug":"/26-01-01_1/","nextSlug":"/25-12-31_1/","prevSlug":""}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}