{"componentChunkName":"component---src-templates-blog-template-js","path":"/25-09-09_1/","result":{"data":{"cur":{"id":"510447e4-7933-5a0e-b6aa-8ea9e7284203","html":"<p>참고 : 윤대희 외. (2023). <em>파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습</em>. 위키북스.</p>\n<p>소스코드: <a href=\"https://github.com/wikibook/pytorchtrf\">https://github.com/wikibook/pytorchtrf</a></p>\n<p>위의 교재와 소스코드를 참고하였으며, 대부분의 내용은 직접 찾아보며 학습하였습니다.</p>\n<p> </p>\n<p>자연어 처리에서 토큰화하는 것에 대해 알아보자.</p>\n<p>토큰을 나누는 기준은 공백 분할, 정규표현식 사용, 어휘사전 적용, 머신러닝 활용하는 방법이 있다. <strong>어휘 사전</strong>을 구축할 때, 너무 크게 구축하면 차원의 저주에 빠지고, 너무 작게 구축하면 OOV(Out of Vocab) 존재 가능성이 있으므로 그 크기를 적절히 정해야 하며, <strong>출현빈도는 고려되지만 순서관계는 표현하지 못한다는 점</strong>을 기억하자.</p>\n<p> </p>\n<h2 id=\"토큰화-라이브러리\" style=\"position:relative;\"><a href=\"#%ED%86%A0%ED%81%B0%ED%99%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\" aria-label=\"토큰화 라이브러리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>토큰화 라이브러리</h2>\n<hr>\n<h3 id=\"jamo-라이브러리\" style=\"position:relative;\"><a href=\"#jamo-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\" aria-label=\"jamo 라이브러리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>jamo 라이브러리</h3>\n<ul>\n<li>\n<p>h2j (Hangul to Jamo): 한글 → 자모(자음과 모음 / 한글 글자를 초성,중성,종성 단위로 쪼갬) 변환한다.\n자모 단위로 쪼개었을 때 → 음운단위 학습 가능해져, <strong>희귀단어</strong>나 <strong>신조어 처리</strong>에 유리하다.</p>\n</li>\n<li>\n<p>j2hcj (Jamo to Hangul ConJoining): 자모 → 한글 변환한다. (초성+중성+(종성) 다시 합쳐 완전한 한글글자 만듦)</p>\n</li>\n</ul>\n<p> </p>\n<h3 id=\"konlpy-라이브러리\" style=\"position:relative;\"><a href=\"#konlpy-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\" aria-label=\"konlpy 라이브러리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KoNLPy 라이브러리</h3>\n<ul>\n<li>KoNLPy: 한국어 형태소 분석 라이브러리</li>\n<li>Okt: “Open Korean Text” 형태소 분석기</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> konlpy<span class=\"token punctuation\">.</span>tag <span class=\"token keyword\">import</span> Okt\n\nokt <span class=\"token operator\">=</span> Okt<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"한글 자연어 처리를 공부해봅시다.\"</span>\n\n<span class=\"token comment\"># 형태소 분석</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>okt<span class=\"token punctuation\">.</span>morphs<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['한글', '자연어', '처리', '를', '공부', '해봅시다', '.']</span>\n\n<span class=\"token comment\"># 형태소 + 품사 태깅</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>okt<span class=\"token punctuation\">.</span>pos<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># [('한글', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'),</span>\n<span class=\"token comment\">#  ('를', 'Josa'), ('공부', 'Noun'), ('해봅시다', 'Verb'), ('.', 'Punctuation')]</span>\n\n<span class=\"token comment\"># 명사 추출</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>okt<span class=\"token punctuation\">.</span>nouns<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['한글', '자연어', '처리', '공부']</span>\n\n<span class=\"token comment\"># 구 추출 - 연속된 명사 묶음을 그대로 반환</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span>okt<span class=\"token punctuation\">.</span>phrases<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['한글', '한글 자연어', '한글 자연어 처리', '공부', '자연어', '처리']</span></code></pre></div>\n<ul>\n<li>Kkma (꼬꼬마): 서울대학교 IDS(Intelligent Data Systems) 연구실에서 개발한 한국어 형태소/구문 분석기\n<ul>\n<li>꼬꼬마는 <code class=\"language-text\">Okt</code>보다 <strong>세밀한 품사 태깅</strong>이 가능하다. (예: 구체적인 조사 구분)</li>\n<li>문장 분리(splitting) 기능이 기본 제공된다.</li>\n<li>속도는 Okt보다 느린 편이지만, 정확성은 높다.</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> konlpy<span class=\"token punctuation\">.</span>tag <span class=\"token keyword\">import</span> Kkma\n\nkkma <span class=\"token operator\">=</span> Kkma<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"한글 자연어 처리를 공부해봅시다.\"</span>\n\n<span class=\"token comment\"># 형태소 분석</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>kkma<span class=\"token punctuation\">.</span>morphs<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['한글', '자연어', '처리', '를', '공부', '해보', 'ㅂ시다', '.'] -> ㅂ시다!!!</span>\n\n<span class=\"token comment\"># 형태소 + 품사 태깅</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>kkma<span class=\"token punctuation\">.</span>pos<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># [('한글', 'NNG'), ('자연어', 'NNG'), ('처리', 'NNG'), ('를', 'JKO'),</span>\n<span class=\"token comment\"># ('공부', 'NNG'), ('해보', 'VV'), ('ㅂ시다', 'EFA'), ('.', 'SF')]</span>\n\n\n<span class=\"token comment\"># 명사 추출</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>kkma<span class=\"token punctuation\">.</span>nouns<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['한글', '자연어', '처리', '공부']</span>\n\n<span class=\"token comment\"># 문장 추출</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span>kkma<span class=\"token punctuation\">.</span>sentences<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['한글 자연어 처리를 공부 해봅시다.']</span></code></pre></div>\n<p> </p>\n<h3 id=\"nltknatural-language-toolkit-라이브러리\" style=\"position:relative;\"><a href=\"#nltknatural-language-toolkit-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\" aria-label=\"nltknatural language toolkit 라이브러리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NLTK(Natural Language Toolkit) 라이브러리</h3>\n<ul>\n<li>NLTK 라이브러리는 토큰화(tokenization), 품사 태깅, 파싱, 텍스트 분류, 코퍼스 제공 등 다양한 기능을 지원한다.</li>\n<li>영어 기반이라 한국어 토큰화는 잘 안 된다!! → 한국어는 <code class=\"language-text\">KoNLPy</code>의 <code class=\"language-text\">Okt</code>, <code class=\"language-text\">Kkma</code>, <code class=\"language-text\">Mecab</code> 같은 형태소 분석기 이용하는 것이 더 좋음</li>\n<li>tokenize:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>tokenize <span class=\"token keyword\">import</span> sent_tokenize\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>tokenize <span class=\"token keyword\">import</span> word_tokenize\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>tokenize <span class=\"token keyword\">import</span> RegexpTokenizer\n\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"Hello world. This is NLTK. Let's learn tokenization! 123 test.\"</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>sent_tokenize<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['Hello world.', 'This is NLTK.', \"Let's learn tokenization!\", '123 test.']</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>word_tokenize<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['Hello', 'world', '.', 'This', 'is', 'NLTK', '.',</span>\n<span class=\"token comment\">#  'Let', \"'s\", 'learn', 'tokenization', '!', '123', 'test', '.']</span>\n\n\ntokenizer <span class=\"token operator\">=</span> RegexpTokenizer<span class=\"token punctuation\">(</span><span class=\"token string\">r'\\w+'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 알파벳 대소문자(a-z, A-Z), 숫자(0-9), 아래밑줄(_) 1개이상 포함 정규화</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ['Hello', 'world', 'This', 'is', 'NLTK', 'Let', </span>\n<span class=\"token comment\"># 's', 'learn', 'tokenization', '123', 'test']</span></code></pre></div>\n<p> </p>\n<h3 id=\"spacy-라이브러리\" style=\"position:relative;\"><a href=\"#spacy-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\" aria-label=\"spacy 라이브러리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>spaCy 라이브러리</h3>\n<ul>\n<li>\n<p>사이썬(Cython) 기반으로 개발된 오픈소스 라이브러리</p>\n</li>\n<li>\n<p>NLTK보다 빠른 속도, 높은 정확도를 가짐</p>\n</li>\n<li>\n<p>spaCy는 <code class=\"language-text\">spacy.load(\"en_core_web_sm\")</code>로 모델을 불러와서 모델에 문장을 입력값으로 넣으면, 객체 지향적으로 처리된 결과를 속성값으로 접근할 수 있다.</p>\n</li>\n<li>\n<p><code class=\"language-text\">doc</code> : 전체 문서 객체 (<code class=\"language-text\">spacy.tokens.doc.Doc</code>)</p>\n<p><code class=\"language-text\">token</code> : 문서의 토큰 단위 (<code class=\"language-text\">spacy.tokens.token.Token</code>)</p>\n<p><code class=\"language-text\">span</code> : 특정 구간 (문장, 구 등) (<code class=\"language-text\">spacy.tokens.span.Span</code>)</p>\n<p>→ 각 객체에서 속성들을 접근할 수 있는데, 너무 많으니 생략!!</p>\n</li>\n</ul>\n<p> </p>\n<p> </p>\n<h2 id=\"하위-단어-토큰화-subword-tokenization\" style=\"position:relative;\"><a href=\"#%ED%95%98%EC%9C%84-%EB%8B%A8%EC%96%B4-%ED%86%A0%ED%81%B0%ED%99%94-subword-tokenization\" aria-label=\"하위 단어 토큰화 subword tokenization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>하위 단어 토큰화 (Subword Tokenization)</h2>\n<hr>\n<p>하위 단어 토큰화는 단어(word)를 더 작은 단위인 subword(하위 단어)로 나누는 방법이다. 완전한 형태소 분석보다는 <strong>빈도 기반 분리</strong>에 가깝다. <strong>OOV(Out-Of-Vocabulary, 사전에 없는 단어) 문제를 해결</strong>하기 위해 고안된 것!!</p>\n<p>희귀 단어 처리</p>\n<ul>\n<li>‘돈쭐내다’라는 단어가 사전에 없으면? → 전통적인 토큰화에서는 ‘돈’+‘쭐’+‘내다’로 쪼개서 처리함\n<ul>\n<li>하위단어 토큰화를 적용한다면, 원래 있는 토큰 ‘돈’과 하위단어 ‘쭐’, ‘내’, ‘다’로 쪼개고</li>\n<li>‘돈’+‘쭐내다(subword 패턴 조합)‘로 신조어 패턴을 학습할 수 있음</li>\n</ul>\n</li>\n<li>신조어, 오탈자도 subword로 분리하면 어느 정도 의미 유지 가능하다.</li>\n</ul>\n<p>단어 집합 크기 줄이기</p>\n<ul>\n<li>모든 단어를 사전에 등록하면 수십만~수백만 단어 필요 → 메모리, 연산량 ↑</li>\n<li>subword로 쪼개면 수만 단위의 작은 사전으로 충분해진다.</li>\n</ul>\n<p> </p>\n<h3 id=\"바이트-페어-인코딩-bpe\" style=\"position:relative;\"><a href=\"#%EB%B0%94%EC%9D%B4%ED%8A%B8-%ED%8E%98%EC%96%B4-%EC%9D%B8%EC%BD%94%EB%94%A9-bpe\" aria-label=\"바이트 페어 인코딩 bpe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>바이트 페어 인코딩 (BPE)</h3>\n<ul>\n<li>\n<p>어휘 사전 추가 방법</p>\n<ul>\n<li><strong>병합 점수 = 특정 쌍이 코퍼스(여러 문장)에서 함께 등장한 횟수(빈도)</strong></li>\n<li>가장 빈도가 높은 쌍을 합친 새로운 subword를 사전에 추가한다. (ex. (‘돈’,‘쭐’) 쌍이 가장 많이 등장한 쌍이면 ‘돈쭐’을 어휘 사전에 추가함)</li>\n<li>이 과정을 vocab_size가 될 때까지 반복한다. (ex. subword로 연결된 ‘돈쭐’과 ‘내’ 쌍이 가장 많이 등장하면 ‘돈쭐내’를 어휘 사전에 추가함)</li>\n</ul>\n</li>\n<li>\n<p>sentencepiece 라이브러리</p>\n<ul>\n<li>\n<p>구글이 만든 언어 독립적인 서브워드 토크나이저</p>\n</li>\n<li>\n<p>BPE(Byte Pair Encoding), Unigram LM 방식을 지원한다.</p>\n</li>\n<li>\n<p>띄어쓰기 기반이 아니라 <strong>문자 단위 입력</strong>을 사용하기 때문에, 한국어·중국어·일본어 등 띄어쓰기가 애매한 언어에도 잘 동작한다.</p>\n</li>\n<li>\n<p>SentencePieceTrainer 모듈</p>\n<ul>\n<li>\n<p>토크나이저 학습 모듈 불러옴 → <strong>새로운 subword 토큰화 모델을 학습</strong>할 때 사용함</p>\n</li>\n<li>\n<p>실행 결과\n<code class=\"language-text\">spm.model</code> → 학습된 토크나이저 모델 파일</p>\n<p><code class=\"language-text\">spm.vocab</code> → 서브워드 사전 (토큰과 점수)</p>\n</li>\n<li>\n<p>주요 파라미터\n<code class=\"language-text\">--input</code> : 학습할 텍스트 파일</p>\n<p><code class=\"language-text\">--model_prefix</code> : 출력 모델/사전 파일 이름 prefix</p>\n<p><code class=\"language-text\">--vocab_size</code> : 어휘 크기 (예: 8000, 32000)</p>\n<p><code class=\"language-text\">--model_type</code> : <code class=\"language-text\">unigram</code>(기본), <code class=\"language-text\">bpe</code>, <code class=\"language-text\">char</code>, <code class=\"language-text\">word</code></p>\n<p><code class=\"language-text\">--character_coverage</code> : 문자 커버율 (예: 한국어는 1.0, 일본어/중국어는 0.9995)</p>\n<p><code class=\"language-text\">--input_sentence_size</code> : 학습에 사용할 문장 샘플 크기</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>sentencepiece 라이브러리의 SentencePieceTrainer 모듈로 BPE 모델 학습 &#x26; 적용</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sentencepiece <span class=\"token keyword\">import</span> SentencePieceTrainer\n\n<span class=\"token comment\"># bpe 모델 학습</span>\n<span class=\"token comment\"># input.txt: 학습할 말뭉치 (한 줄 = 하나의 문장)</span>\n<span class=\"token comment\"># character_coverage=1.0: 학습 코퍼스에 등장하는 모든 문자를 vocab에 포함(한국어/한자/이모지 다룰 것)</span>\n<span class=\"token comment\"># unk_id: 어휘 사전에 없는 OOV를 의미하는 unk 토큰의 id (기본값: 0)</span>\n<span class=\"token comment\"># bos_id: 문장이 시작되는 지점을 의미하는 bos 토큰의 id (기본값: 1)</span>\n<span class=\"token comment\"># eos_id: 문장이 끝나는 지점을 의미하는 eos 토큰의 id (기본값: 2)</span>\nSentencePieceTrainer<span class=\"token punctuation\">.</span>Train<span class=\"token punctuation\">(</span>\n    \"<span class=\"token operator\">-</span><span class=\"token operator\">-</span><span class=\"token builtin\">input</span><span class=\"token operator\">=</span><span class=\"token builtin\">input</span><span class=\"token punctuation\">.</span>txt\\\n    <span class=\"token operator\">-</span><span class=\"token operator\">-</span>model_prefix<span class=\"token operator\">=</span>spm\\\n    <span class=\"token operator\">-</span><span class=\"token operator\">-</span>vocab_size<span class=\"token operator\">=</span><span class=\"token number\">8000</span>\\\n    <span class=\"token operator\">-</span><span class=\"token operator\">-</span>model_type<span class=\"token operator\">=</span>bpe\\\n    <span class=\"token operator\">-</span><span class=\"token operator\">-</span>character_coverage<span class=\"token operator\">=</span><span class=\"token number\">1.0</span>\"\n<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sentencepiece <span class=\"token keyword\">import</span> SentencePieceProcessor\n\n<span class=\"token comment\"># 1. 학습된 모델 로드</span>\nsp <span class=\"token operator\">=</span> SentencePieceProcessor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nsp<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">\"spm.model\"</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># Trainer가 만든 모델 파일</span>\n\n<span class=\"token comment\"># 2. 문장을 subword 단위로 토큰화</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"돈쭐내다라는 신조어를 SentencePiece로 실험해봅시다.\"</span>\n\n<span class=\"token comment\"># 서브워드 단위로 분리 (문장 -> 서브워드)</span>\npieces <span class=\"token operator\">=</span> sp<span class=\"token punctuation\">.</span>encode_as_pieces<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Pieces:\"</span><span class=\"token punctuation\">,</span> pieces<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Pieces: ['▁돈', '쭐', '내', '다', '라는', '▁신', '조', '어',</span>\n<span class=\"token comment\"># '를', '▁', 'S', 'e', 'n', 't', 'e', 'n', 'c', 'e',</span>\n<span class=\"token comment\"># 'P', 'i', 'e', 'c', 'e', '로', '▁실', '험', '해', '봅', '시다', '.']</span>\n\n\n<span class=\"token comment\"># 토큰 id로 변환 (문장 -> 정수 인코딩)</span>\nids <span class=\"token operator\">=</span> sp<span class=\"token punctuation\">.</span>encode_as_ids<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"IDs:\"</span><span class=\"token punctuation\">,</span> ids<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># IDs: [189, 3427, 1718, 1622, 165, 111, 1706,</span>\n<span class=\"token comment\"># 1647, 1644, 1620, 2099, 1958, 1997, 1928, 1958,</span>\n<span class=\"token comment\"># 1997, 2047, 1958, 2232, 2009, 1958, 2047, 1958, 1635,</span>\n<span class=\"token comment\"># 136, 1875, 1639, 2029, 743, 1623]</span>\n\n\n<span class=\"token comment\"># 다시 문장으로 복원 (정수 인코딩 -> 문장)</span>\ndecoded_ids <span class=\"token operator\">=</span> sp<span class=\"token punctuation\">.</span>decode_ids<span class=\"token punctuation\">(</span>ids<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Decoded_ids:\"</span><span class=\"token punctuation\">,</span> decoded_ids<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Decoded_ids: 돈쭐내다라는 신조어를 SentencePiece로 실험해봅시다.</span>\n\n\n<span class=\"token comment\"># 다시 문장으로 복원 (서브워드 -> 문장)</span>\ndecoded_pieces <span class=\"token operator\">=</span> sp<span class=\"token punctuation\">.</span>decode_pieces<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Decoded_pieces:\"</span><span class=\"token punctuation\">,</span> decoded_pieces<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Decoded_pieces: 돈쭐내다라는 신조어를 SentencePiece로 실험해봅시다.</span></code></pre></div>\n<ul>\n<li>학습된 어휘사전 확인</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sentencepiece <span class=\"token keyword\">import</span> SentencePieceProcessor\n\ntokenizer <span class=\"token operator\">=</span> SentencePieceProcessor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">\"spm.model\"</span><span class=\"token punctuation\">)</span>\n\nvocab <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>idx<span class=\"token punctuation\">:</span> tokenizer<span class=\"token punctuation\">.</span>id_to_piece<span class=\"token punctuation\">(</span>idx<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>tokenizer<span class=\"token punctuation\">.</span>get_piece_size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">100</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 100개 확인</span>\n<span class=\"token comment\"># [(0, '&lt;unk>'), (1, '&lt;s>'), (2, '&lt;/s>'), (3, '니다'), (4, '▁이'), (5, '▁있'), (6, '습니다'), (7, '▁하'), (8, '▁그'), (9, '▁사'), (10, '▁대'), (11, '으로'), (12, '▁아'), (13, '▁국'), (14, '▁정'), (15, '합니다'), (16, '▁지'), (17, '에서'), (18, '▁수'), (19, '하는'), (20, '▁가'), (21, '하고'), (22, '입니다'), (23, '▁보'), (24, '..'), (25, '▁일'), (26, '▁한'), (27, '▁없'), (28, '▁해'), (29, '▁제'), (30, '▁부'), (31, '▁생'), (32, '▁자'), (33, '▁것'), (34, '▁나'), (35, '▁주'), (36, '▁국민'), (37, '▁안'), (38, '▁다'), (39, '들이'), (40, '▁시'), (41, '▁어'), (42, '▁기'), (43, '▁1'), (44, '▁저'), (45, '다고'), (46, '▁않'), (47, '▁공'), (48, '▁인'), (49, '▁전'), (50, '▁위'), (51, '▁경'), (52, '▁생각'), (53, '▁되'), (54, '▁모'), (55, '▁있는'), (56, '▁2'), (57, '▁청'), (58, '세요'), (59, '▁우'), (60, '▁여'), (61, '▁무'), (62, '지만'), (63, '▁문'), (64, '▁많'), (65, '니까'), (66, '▁사람'), (67, '▁바'), (68, '▁말'), (69, '▁조'), (70, '▁있습니다'), (71, '▁내'), (72, '▁대한'), (73, '에게'), (74, '이라'), (75, '▁고'), (76, '다는'), (77, '▁받'), (78, '▁의'), (79, '▁상'), (80, '들은'), (81, '▁현'), (82, '▁만'), (83, '▁소'), (84, '▁중'), (85, '는데'), (86, '▁더'), (87, '▁불'), (88, '▁합니다'), (89, '하여'), (90, '▁못'), (91, '해서'), (92, '▁우리'), (93, '▁개'), (94, '▁세'), (95, '▁비'), (96, '하게'), (97, '▁미'), (98, '▁법'), (99, '▁도')]</span>\n<span class=\"token comment\"># vocab size : 8000</span></code></pre></div>\n<p> </p>\n<h3 id=\"워드피스-wordpiece\" style=\"position:relative;\"><a href=\"#%EC%9B%8C%EB%93%9C%ED%94%BC%EC%8A%A4-wordpiece\" aria-label=\"워드피스 wordpiece permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>워드피스 (Wordpiece)</h3>\n<ul>\n<li>\n<p>구글 번역(Google Neural Machine Translation)에서 제안됨</p>\n</li>\n<li>\n<p>어휘 사전 추가 방법</p>\n<ul>\n<li>\n<p><strong>병합 점수 = 글자쌍의 등장횟수 / 각 글자의 등장 횟수의 곱</strong></p>\n</li>\n<li>\n<p><strong>초기 상태</strong>: 어휘 사전(vocab)을 문자 단위로 시작 (예: <code class=\"language-text\">['ㄷ', 'ㅗ', 'ㄴ', '쭐', '내', '다']</code>)</p>\n</li>\n<li>\n<p><strong>병합 과정</strong>: 코퍼스에서 가장 자주 등장하는 <strong>subword 쌍</strong>을 합쳐 새로운 토큰으로 추가</p>\n<ul>\n<li><code class=\"language-text\">'ㄷ'</code>,<code class=\"language-text\">'ㅗ'</code>,<code class=\"language-text\">'ㄴ'</code> 의 등장 횟수 대비 <code class=\"language-text\">\"돈\"</code>이 자주 나오면 `‘ㄷ’+‘ㅗ’+‘ㄴ’ → ‘돈’“</li>\n</ul>\n</li>\n<li>\n<p><strong>사전 확장</strong>: 원하는 vocab_size (예: 30k)까지 반복</p>\n<ul>\n<li>많이 쓰이는 건 그대로 하나의 토큰, 드물게 쓰이는 건 작은 subword 단위로 쪼개짐</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>허깅페이스 Tokenizer 라이브러리</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tokenizers <span class=\"token keyword\">import</span> Tokenizer\n<span class=\"token keyword\">from</span> tokenizers<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> WordPiece\n<span class=\"token keyword\">from</span> tokenizers<span class=\"token punctuation\">.</span>normalizers <span class=\"token keyword\">import</span> Sequence<span class=\"token punctuation\">,</span> NFD<span class=\"token punctuation\">,</span> Lowercase\n<span class=\"token keyword\">from</span> tokenizers<span class=\"token punctuation\">.</span>pre_tokenizers <span class=\"token keyword\">import</span> Whitespace\n\n<span class=\"token comment\"># 1. WordPiece 모델 정의</span>\n<span class=\"token comment\"># - WordPiece 알고리즘을 기반으로 토크나이저를 생성</span>\n<span class=\"token comment\"># - unk_token=\"[UNK]\" : 사전에 없는 토큰(OOV)을 처리할 때 사용되는 특별 토큰</span>\ntokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">(</span>WordPiece<span class=\"token punctuation\">(</span>unk_token<span class=\"token operator\">=</span><span class=\"token string\">\"[UNK]\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 정규화기(Normalizer) 설정</span>\n<span class=\"token comment\"># - 입력 텍스트를 표준화하는 과정</span>\n<span class=\"token comment\"># - NFD() : 유니코드 정규화 클래스 (예: é -> e + ´)</span>\n<span class=\"token comment\"># - Lowercase() : 모든 문자를 소문자로 변환하는 정규화 클래스</span>\n<span class=\"token comment\"># - 정규화 클래스는 NFKD, NFC, NFKC, strip 등이 더 존재함</span>\n<span class=\"token comment\"># - Sequence([...]) : 여러 정규화기를 순서대로 적용</span>\ntokenizer<span class=\"token punctuation\">.</span>normalizer <span class=\"token operator\">=</span> Sequence<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>NFD<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> Lowercase<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 3. PreTokenizer 설정 (사전 토큰화 클래스)</span>\n<span class=\"token comment\"># - 본격적인 subword 학습 전에 단어 단위로 먼저 나누는 과정</span>\n<span class=\"token comment\"># - Whitespace() : 공백 단위로 먼저 토큰 분리하는 클래스</span>\n<span class=\"token comment\"># - 사전 토큰화 클래스는 Sequence(), CharDelimiterSplit(), Digits() 등이 더 존재함</span>\ntokenizer<span class=\"token punctuation\">.</span>pre_tokenizer <span class=\"token operator\">=</span> Whitespace<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 4. WordPiece 토크나이저 학습</span>\n<span class=\"token comment\"># - \"../datasets/corpus.txt\" : 학습에 사용할 말뭉치 파일</span>\n<span class=\"token comment\"># - 파일 안의 텍스트를 기반으로 WordPiece subword 사전을 학습</span>\ntokenizer<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"../datasets/corpus.txt\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 5. 학습된 토크나이저 저장</span>\n<span class=\"token comment\"># - \"../models/petition_wordpiece.json\" : 토크나이저 설정 및 vocab이 포함된 JSON 파일</span>\n<span class=\"token comment\"># - 나중에 이 파일을 불러와서 같은 토크나이저를 재사용 가능</span>\ntokenizer<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token string\">\"../models/petition_wordpiece.json\"</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tokenizers <span class=\"token keyword\">import</span> Tokenizer\n<span class=\"token keyword\">from</span> tokenizers<span class=\"token punctuation\">.</span>decoders <span class=\"token keyword\">import</span> WordPiece <span class=\"token keyword\">as</span> WordPieceDecoder\n\n<span class=\"token comment\"># 1. 학습된 모델 로드</span>\ntokenizer <span class=\"token operator\">=</span> Tokenizer<span class=\"token punctuation\">.</span>from_file<span class=\"token punctuation\">(</span><span class=\"token string\">\"../models/petition_wordpiece.json\"</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>decoder <span class=\"token operator\">=</span> WordPieceDecoder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. 문장을 subword 단위로 토큰화</span>\ntext <span class=\"token operator\">=</span> <span class=\"token string\">\"돈쭐내다라는 신조어를 SentencePiece로 실험해봅시다.\"</span>\n\n<span class=\"token comment\"># 인코딩</span>\nencoded <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 인코딩 타입</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"encoded_type:\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>encoded<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># encoded_type: &lt;class 'tokenizers.Encoding'></span>\n\n<span class=\"token comment\"># 문장 토큰화</span>\ntokens <span class=\"token operator\">=</span> encoded<span class=\"token punctuation\">.</span>tokens\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"tokens:\"</span><span class=\"token punctuation\">,</span> tokens<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># tokens: ['돈', '##ᄍ', '##ᅮᆯ', '##내', '##다', '##라는', '신',</span>\n<span class=\"token comment\"># '##조', '##어를', 'se', '##nt', '##en', '##ce', '##p', '##i'</span>\n<span class=\"token comment\">#  '##ec', '##e', '##로', '실험', '##해보', '##ᆸ시다', '.']</span>\n\n<span class=\"token comment\"># 정수 인코딩</span>\nids <span class=\"token operator\">=</span> encoded<span class=\"token punctuation\">.</span>ids\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ids:\"</span><span class=\"token punctuation\">,</span> ids<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># ids: [7917, 4280, 7521, 7731, 7478, 7906,</span>\n<span class=\"token comment\"># 7755, 7649, 11207, 26328, 23624, 10456, 21575,</span>\n<span class=\"token comment\"># 4295, 4263, 10611, 4288, 7495, 12904, 9150, 9008, 13]</span>\n\n<span class=\"token comment\"># 디코딩 (정수 인코딩 -> 문장 변환)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"decoded:\"</span><span class=\"token punctuation\">,</span> tokenizer<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>ids<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># decoded: 돈쭐내다라는 신조어를 sentencepiece로 실험해봅시다.</span></code></pre></div>\n<p> </p>\n<p>토큰화하는 방법은 여러가지 존재한다.</p>\n<p>번역/챗봇에서는 subword 방식(WordPiece, SentencePiece), 한국어 문법 분석에서는 형태소 기반(Okt, Kkma, Mecab), 소셜미디어 신조어 처리에서는 subword + 자모 단위 보완하도록 토큰화를 진행할 수 있다. 또한, 어떤 단어/서브워드를 어휘 사전에 포함시킬지에 따라 모델 성능이 크게 달라진다.</p>\n<p>결국, <strong>프로젝트 목적에 맞는 토큰화 선택이 중요하고, 어휘사전 구축이 핵심이다.</strong></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%ED%86%A0%ED%81%B0%ED%99%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">토큰화 라이브러리</a></p>\n<ul>\n<li><a href=\"#jamo-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">jamo 라이브러리</a></li>\n<li><a href=\"#konlpy-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">KoNLPy 라이브러리</a></li>\n<li><a href=\"#nltknatural-language-toolkit-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">NLTK(Natural Language Toolkit) 라이브러리</a></li>\n<li><a href=\"#spacy-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">spaCy 라이브러리</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%ED%95%98%EC%9C%84-%EB%8B%A8%EC%96%B4-%ED%86%A0%ED%81%B0%ED%99%94-subword-tokenization\">하위 단어 토큰화 (Subword Tokenization)</a></p>\n<ul>\n<li><a href=\"#%EB%B0%94%EC%9D%B4%ED%8A%B8-%ED%8E%98%EC%96%B4-%EC%9D%B8%EC%BD%94%EB%94%A9-bpe\">바이트 페어 인코딩 (BPE)</a></li>\n<li><a href=\"#%EC%9B%8C%EB%93%9C%ED%94%BC%EC%8A%A4-wordpiece\">워드피스 (Wordpiece)</a></li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"참고 : 윤대희 외. (2023). 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습. 위키북스. 소스코드: https://github.com/wikibook/pytorchtrf 위의 교재와 소스코드를 참고하였으며, 대부분의 내용은 직접 찾아보며 학습하였습니다.   자연어 처리에서 토큰화하는 것에 대해 알아보자. 토큰을 나누는 기준은 공백 분할, 정규표현식 사용, 어휘사전 적용, 머신러닝 활용하는 방법이 있다. 어휘 사전을 구축할 때, 너무 크게 구축하면 차원의 저주에 빠지고, 너무 작게 구축하면 OOV(Out of Vocab) 존재 가능성이 있으므로 그 크기를 적절히 정해야 하며, 출현빈도는 고려되지만 순서관계는 표현하지 못한다는 점을 기억하자.   토큰화 라이브러리 jamo 라이브러리 h2j (Hangul to Jamo): 한글 → 자모(자음과 모음 / 한글 글자를 초성,중성,종성 단위로 쪼갬) 변환한다.\n자모 단위로 쪼개었을 때 → 음운단위 학습 가능해져, 희귀단…","frontmatter":{"date":"September 09, 2025","title":"[NLP] 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습 -토큰화","categories":"NLP","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-09-09_1/"}},"next":{"id":"1dd3c51a-53a4-5cbb-9e43-79be0b45f237","html":"<p>GitHub 레포: <a href=\"https://github.com/byeonwoojung/youtube-playlist-MLproject\">https://github.com/byeonwoojung/youtube-playlist-MLproject</a></p>\n<p> </p>\n<h2 id=\"들어가며\" style=\"position:relative;\"><a href=\"#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0\" aria-label=\"들어가며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>들어가며(?)</h2>\n<hr>\n<p>2025년 7월 3일, 생애 처음으로 학회에서 논문 발표를 했다.</p>\n<p>”<strong>섬네일·제목·오디오 기반 통합적 유튜브 플레이리스트 조회수 예측</strong>“이라는 주제로 <strong>한국디지털콘텐츠학회 하계종합학술대회</strong>에 참가했다. ASAC 교육과정에서 발표를 위해 1개월 정도 준비했고, 이후 딥러닝 프로젝트, 기업연계 프로젝트와 병행하면서(진짜 너무 힘듦 ㅠ) 약 5~6주 동안 디벨롭해 <strong>학회 논문 발표</strong>를 진행했다.. 프로젝트 2개씩 병행하니 잠을 거의 못 자며 준비했었다,,,</p>\n<p>정말 발표 때가 오니, 약간 긴장도 되고 설레기도 했다.(사실 긴장 안함🥶)</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 35%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAAA60lEQVQoz3WR2QqEMAxF+/9fJSoIgogPCr6IKG64gPvuHRKmQxlmCpeG3OY0bcTzPFBF6zgOFEXBStMUSZKgLEvEccwxqaoq3vM85zPk13UNgT9Lwr/jf9593yzRNA10XYdt27AsC77vIwxDlmEYnHccB67rsq9pGjzPQxRFnA+CAKZpous6BovzPDGOI+Z5xrqurH3fuf0sy9hblgXTNGEYhre/fXLbtrGIw8BfTyUg3UiShXQh7QSUxb9qhRwGQWgYBGjb9gPo+547I6kxdXVdF/+bOlShTpZg8gkk+QVqjkSXS4/qZHcEfAGFdxip8varbgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"conference_1\" title=\"conference_1\" src=\"/static/7ec63d2f135124a02ca6609b96c38ab9/37523/conference_1.png\" srcset=\"/static/7ec63d2f135124a02ca6609b96c38ab9/e9ff0/conference_1.png 180w,\n/static/7ec63d2f135124a02ca6609b96c38ab9/f21e7/conference_1.png 360w,\n/static/7ec63d2f135124a02ca6609b96c38ab9/37523/conference_1.png 720w,\n/static/7ec63d2f135124a02ca6609b96c38ab9/302a4/conference_1.png 1080w,\n/static/7ec63d2f135124a02ca6609b96c38ab9/3fca6/conference_1.png 1112w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p>진짜 마감 직전까지 수정한…ㅠ 학회 논문은 내겐 아픈 손가락이다.. 더 잘할 수 있었을 텐데 아쉬움이 많이 남는다..\n(다른 분들은 원치 않으실 수 있으니 가렸음다,,ㅎ)</p>\n<p> </p>\n<h2 id=\"프로젝트를-시작한-계기\" style=\"position:relative;\"><a href=\"#%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EB%A5%BC-%EC%8B%9C%EC%9E%91%ED%95%9C-%EA%B3%84%EA%B8%B0\" aria-label=\"프로젝트를 시작한 계기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>프로젝트를 시작한 계기</h2>\n<hr>\n<p>평소 유튜브 플레이리스트 영상을 자주 봤다. 퇴근길 듣는 감성 노래, 잠들기 전에 듣는 음악, 공부할 때 틀어놓는 장작 타는 소리 등… 그러다 보니 자연스럽게 “어떤 플레이리스트가 조회수가 높을까?”라는 궁금증이 생겼다.</p>\n<p>찾아보니 플레이리스트 콘텐츠는 유튜브에서 인기 있는 카테고리인데, 관련 연구는 거의 없없다. 특히 유튜브 조회수 예측 관련해서 <strong>섬네일, 제목, 오디오</strong>를 통합적으로 분석한 연구는 찾기 어려웠다.</p>\n<p>그래서 직접 해보기로 했다. 이미지, 텍스트, 오디오의 비정형 데이터를 활용해서 조회수를 예측하고, 플레이리스트 제작자들에게 도움이 될 만한 인사이트를 찾아보자는 목표로 시작했다.</p>\n<p> </p>\n<p> </p>\n<h2 id=\"사용한-기술들\" style=\"position:relative;\"><a href=\"#%EC%82%AC%EC%9A%A9%ED%95%9C-%EA%B8%B0%EC%88%A0%EB%93%A4\" aria-label=\"사용한 기술들 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>사용한 기술들</h2>\n<hr>\n<p>이 프로젝트에서는 여러 기술을 활용했다.</p>\n<ul>\n<li><strong>ML/DL</strong>: PyTorch, TensorFlow, Scikit-learn</li>\n<li><strong>이미지</strong>: YuNet (정면얼굴 검출), Google Vision API (텍스트 추출), OpenCV, KMeans (색상 분석), OpenAI API (SAM, YOLOv8, mediapipe 등 실험하면서 더 써봄,,)</li>\n<li><strong>텍스트</strong>: OpenAI API(오감자극표현 여부, 관심 유도 표현 정도), TfidfVectorizer (감성·일상 테마 분류)</li>\n<li><strong>오디오</strong>: Wav2Vec2, Librosa (음향 특징 추출)</li>\n<li><strong>Data Processing</strong>: Pandas, Numpy</li>\n</ul>\n<p>섬네일에서는 감성·일상 테마 정합도, 정면얼굴, 색상 분포, 텍스트 면적 비율, 주요소(동물, 사람, 애니메이션 등), 명암 분산도 등을 추출했고, 제목에서는 오감자극 표현이 있는지, 관심유도하는 표현의 정도는 어떠한지 등에 집중했고, 오디오에서는 정량적 특징(centroid, bpm 등)과 감성적인 특징(7가지)을 분석했다.</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB80lEQVQoz32TzWsTURTFZ6W4889wqRuLO3ErLgoWKhVRVzWC1GKxmCwUWqGSglLQgko2oi1qqZOalMZp/MhUAxmhRDERlEqLDXGSTGkm85GkP5lXE5O0euDyHu+de9653PukarVKZ7iu+zccBxeogtgbhoGu6+iFgliLxSK6/ot8Pk+5XEbqFHI8AdeFeh1qNTw4agLrdVzst/6c1Wo1wes0I7WKeatIAtZLJdZ0nZxp8i2R4OeH96xvblIwDNZWV7FtR3A7K5IaB96LlmWRyWRYTqdZCIdRwnMsxWIsRudZTmmk1CVSySTT01No2key2azIaTXUJmiaJvF4HE3T+J5MknwhkwrLpGZn0CJzpJUYSvQlyts3fM1mUVUVs1JpK3/Xkj3MKDI3xvuJPL3C0O0Al4J+BoLXuDAyiLwYafJac9sEGxeViimID0LXuew7wKuHPdwL9jA2epLg2CnO+7p4NDUuOJZV2dEYqbNLHsnDZGiUYyf2Mnz1MCOBIwSGu/BdPMiho/uYmPQLjm1bTcG2prSGbW8LPnnsp7dbov/cHs72SZzplTjdt5/j3RL3Q0M7BOv1+u4OvTn08OWzSvT5LRT5DpFnN0UszN9FDk+Q/vRuez4dp+muVCqJjkv/+ilb/B8NZxsbBrlcjpUfK2JKfgOkrRLWerBxQgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"presentation_1\" title=\"presentation_1\" src=\"/static/abe9f89b94e47b6d1a8e45f746b8759e/37523/presentation_1.png\" srcset=\"/static/abe9f89b94e47b6d1a8e45f746b8759e/e9ff0/presentation_1.png 180w,\n/static/abe9f89b94e47b6d1a8e45f746b8759e/f21e7/presentation_1.png 360w,\n/static/abe9f89b94e47b6d1a8e45f746b8759e/37523/presentation_1.png 720w,\n/static/abe9f89b94e47b6d1a8e45f746b8759e/302a4/presentation_1.png 1080w,\n/static/abe9f89b94e47b6d1a8e45f746b8759e/07a9c/presentation_1.png 1440w,\n/static/abe9f89b94e47b6d1a8e45f746b8759e/720e3/presentation_1.png 1962w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p> </p>\n<p> </p>\n<h2 id=\"발표-준비-과정\" style=\"position:relative;\"><a href=\"#%EB%B0%9C%ED%91%9C-%EC%A4%80%EB%B9%84-%EA%B3%BC%EC%A0%95\" aria-label=\"발표 준비 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>발표 준비 과정</h2>\n<hr>\n<h3 id=\"발표-자료-만들기\" style=\"position:relative;\"><a href=\"#%EB%B0%9C%ED%91%9C-%EC%9E%90%EB%A3%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0\" aria-label=\"발표 자료 만들기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>발표 자료 만들기</h3>\n<p>우리의 이야기를 담기엔 발표시간 10분은 너무 적다. 발표시간이 적은 ‘적은 만큼 다 덜어내면 되겠지?’ 했는데, 무엇을 덜어내야 할지, 피처에 대한 설명은 어느정도로 해야할지 등 발표 내용을 압축하는 것이 너무 어려웠다.</p>\n<p>처음엔 모든 실험 결과를 다 넣으려고 했는데, 그러다 보니 발표 자료가 너무 길어졌다. 결국 우리 발표의 핵심인  남기고 과감하게 덜어냈다. **“왜 이 연구를 했는지”, “어떻게 했는지”, “결과는 어땠는지”**에 집중했다.</p>\n<p>발표 자료를 만들면서 프로젝트를 다시 정리하는 시간을 가질 수 있었다. 뭐가 중요한지, 뭘 강조해야 하는지 명확해졌다.</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 51.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABcElEQVQoz31SXXOCQAzk//8Cf5DT0SedwlRn9ARbRAXk4w4BEU1nV4/aPvRhLySXTTbhnNDzpB+N5H0ykbfpVHRZShiGEkWRdNer9H3/C9c/Meufz2cpikKc+XgsH0rJ6XTiRV3X0jQNcblcaBGz8f9igFNoLRulxHVdUUqxS57nVDibz8QPAinLkrHdbifL5ZJ3yAOCIJDFYiHRfv9QiAMEJCdJIlVVyfF4pD0cDryD+izLGIvjmOPBRxOttcTxgwffAQEJxlTcB2SnacJxkFQ39UAGOEGRszC+jTHDBEma/ijMnt0AKEUcygAoho9mbdsS8NM0HUaHKHCdIs+HglBSas1E/bTw0R2KsJbVeiVqs5Ht9pOvwfU8Wa3X3CH4LyMbWhTA7r7CUHzfZyFbFGoxVtO2bIiV3O93vo7b7UZhHBmBruu4QxS1P4VLN+ahvCxJRg7IdlWWC8uCOOzuAPhAZQxVozB8FLD31trYK/cbNpP3yIjEbG8AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"presentation_2\" title=\"presentation_2\" src=\"/static/1da28bf9f9ffba0a6c8957368ab8dfd3/37523/presentation_2.png\" srcset=\"/static/1da28bf9f9ffba0a6c8957368ab8dfd3/e9ff0/presentation_2.png 180w,\n/static/1da28bf9f9ffba0a6c8957368ab8dfd3/f21e7/presentation_2.png 360w,\n/static/1da28bf9f9ffba0a6c8957368ab8dfd3/37523/presentation_2.png 720w,\n/static/1da28bf9f9ffba0a6c8957368ab8dfd3/302a4/presentation_2.png 1080w,\n/static/1da28bf9f9ffba0a6c8957368ab8dfd3/07a9c/presentation_2.png 1440w,\n/static/1da28bf9f9ffba0a6c8957368ab8dfd3/720e3/presentation_2.png 1962w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p> </p>\n<h3 id=\"발표-연습\" style=\"position:relative;\"><a href=\"#%EB%B0%9C%ED%91%9C-%EC%97%B0%EC%8A%B5\" aria-label=\"발표 연습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>발표 연습</h3>\n<p>발표 시간은 딱 10분이었다. 시간을 맞추는 게 생각보다 까다로웠다.</p>\n<p>집에서 혼자 타이머 켜놓고 여러 번 연습했다. 처음엔 12분 넘게 걸렸는데, 불필요한 설명을 줄이고 핵심만 말하는 연습을 했다. 발표 당일 아침에도 한 번 더 리허설을 했다.</p>\n<p>예상 질문도 미리 정리했다. “왜 이 주제를 선택했나요?”, “데이터는 어떻게 수집했나요?”, “모델 성능은 어떤가요?” 같은 질문들을 생각해두고 답변을 준비했다.\n(근데, 생각했던 질문 거의 안 나왔네요… ㅎㅎ)</p>\n<p> </p>\n<p> </p>\n<h2 id=\"발표-당일\" style=\"position:relative;\"><a href=\"#%EB%B0%9C%ED%91%9C-%EB%8B%B9%EC%9D%BC\" aria-label=\"발표 당일 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>발표 당일</h2>\n<hr>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 35%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABU0lEQVQozzVRyW6DUAzk/6/9kF4r5dhDUlqVQ5QUsrHDWyDAA7EEwVR2myeNLFkz9nie5fs+ttstPM+DlBJaa2itkAuBTEjkQnJfKcU1TVMcDgfsdjvcbjfW2bYN13URhiGs+/2OoixQliXmecayLABWqMrg5e0TH+cEWGaEUYQ0y5jzxDRNIH2e59BFwT0eWFUV6rpG3/csHMcRgW7w+nWGfc0x/guNMcwLggDny4VdkybLMjRNg9YYWGEY4Hj8gR8EeDweLKRN70cfG8fFxjnhGqWQUiCKYx5KYhrcdR3iOEacJGxkGAZYdCrlRpUaSiv0w4BlniDSBH3Xgl6SJJzhuq5ojOHBZMC0hk2QhiKwnudSpQaRCFVVQyoNXZQsIA4tHsaReXQqGWjbFkIKdjuSw5PnYb/f82898yQ8f5tyItAFFP6340AIwWcT/4/3B9L9Ann0DGKPinNnAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"conference_2\" title=\"conference_2\" src=\"/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/37523/conference_2.png\" srcset=\"/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/e9ff0/conference_2.png 180w,\n/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/f21e7/conference_2.png 360w,\n/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/37523/conference_2.png 720w,\n/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/302a4/conference_2.png 1080w,\n/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/07a9c/conference_2.png 1440w,\n/static/3d84eafdf3c6deee6fdec5bd5eaf9f3c/e4ee8/conference_2.png 1496w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 10%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAXElEQVQI11WMMQ7AIAwD+f87CSShgQEYYHFFVFXtcJLlkx2IIlJKKKUgEoGFYWbIOUNEoKruRRVW7e3WWth7/zi7cOSc02FmP26tuay1OteTv33v3Rlj+NnJmRk3tp2Xh4fEpoEAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"conference_3\" title=\"conference_3\" src=\"/static/1790154d17ebf1049f8eee1806bad9b8/37523/conference_3.png\" srcset=\"/static/1790154d17ebf1049f8eee1806bad9b8/e9ff0/conference_3.png 180w,\n/static/1790154d17ebf1049f8eee1806bad9b8/f21e7/conference_3.png 360w,\n/static/1790154d17ebf1049f8eee1806bad9b8/37523/conference_3.png 720w,\n/static/1790154d17ebf1049f8eee1806bad9b8/302a4/conference_3.png 1080w,\n/static/1790154d17ebf1049f8eee1806bad9b8/5a791/conference_3.png 1248w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p> </p>\n<p>ASAC 교육과정에서 딥러닝 프로젝트와 기업연계 프로젝트를 함께 준비하느라,\n도저히 제주도를 갈 수 없어서 비대면으로 했습니다^^ (너무 아쉬운…)</p>\n<p> </p>\n<h3 id=\"발표-후-질의응답\" style=\"position:relative;\"><a href=\"#%EB%B0%9C%ED%91%9C-%ED%9B%84-%EC%A7%88%EC%9D%98%EC%9D%91%EB%8B%B5\" aria-label=\"발표 후 질의응답 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>발표 후, 질의응답</h3>\n<p>발표가 끝나고 5분간 질의응답 시간이 있었다.</p>\n<p>첫 번째 질문은 <strong>실제 활용 가능성</strong>에 관한 것이었다. 현재 모델로도 어느 정도 가능성이 있지만, 피처를 더 세밀하게 구성하면 실제로 활용할 수 있을 것 같다고 답했다.</p>\n<p>두 번째 질문이 인상 깊었다.</p>\n<p><em>“유튜브는 조회수보다 <strong>시청시간</strong>이 수익에 더 중요한데, 유튜브 API에서는 시청시간 데이터를 제공하지 않는다. 이 부분에 대해 고민해봤는지, 어려움은 없었는지?”</em></p>\n<p>정말 핵심을 찌르는 질문이었다. 프로젝트 진행하면서 이 부분이 늘 아쉬웠는데, 질문으로 나오니 솔직하게 답할 수 있었다.</p>\n<p><em>“맞다. 시청시간 데이터를 못 구한 게 가장 아쉬웠다. 만약 유튜브 제작자와 직접 협력해서 시청시간이나 조회수 변화량 같은 데이터를 받을 수 있다면, 모델 성능을 훨씬 더 높일 수 있을 것 같다. 지금보다 더 세밀한 분석이 필요하다고 생각한다.”</em></p>\n<p>답변하면서도 ‘다음엔 이 부분을 보완해야겠다’는 생각이 들었다.</p>\n<p>아, 그리고 ASAC에서 발표했을 때도 그렇지만, <strong>주변에서 주제 선정 배경부터 피처 선정, 생성, … , 최종 결과까지 정말 매끄럽고 자연스럽다는 피드백 받아서 기분이 좋아요^^,,👍🏻</strong></p>\n<p> </p>\n<h2 id=\"느낀-점\" style=\"position:relative;\"><a href=\"#%EB%8A%90%EB%82%80-%EC%A0%90\" aria-label=\"느낀 점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>느낀 점</h2>\n<hr>\n<h3 id=\"배운-점--아쉬운-점\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EC%9A%B4-%EC%A0%90--%EC%95%84%EC%89%AC%EC%9A%B4-%EC%A0%90\" aria-label=\"배운 점  아쉬운 점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배운 점 + 아쉬운 점</h3>\n<p>학회 발표는 프로젝트를 정리하고 되돌아보는 좋은 기회였다.<br>실 준비 기간 2~3개월동안 하나의 스토리로 이루어지기 위한 그 노력들이 떠올랐다.</p>\n<ol>\n<li>\n<p>질의응답을 준비하면서, 질의응답을 하면서 아쉬웠던 점들이 더욱 명확하게 느껴졌다. 현 모델을 곧바로 활용하기에 부족한 감이 있었다. 가장 중요한 것은 <strong>실제 활용 가능한가</strong>인데, 이는 모델 성능에 귀결되는 것 같다.</p>\n</li>\n<li>\n<p>모델 성능을 높이기 위해 ’<strong>섬네일과 제목과의 상호작용을 좀 더 딥하게 파보면 어떨까</strong>‘라는 생각도 해보았다.</p>\n</li>\n<li>\n<p>또, 현재 연구는 ’<strong>제작한 콘텐츠를 업로드 직전</strong>‘에 즉, 조회수나 좋아요수 등의 <strong>‘사후 지표’를 이용하지 않은 ‘사전 정보’만으로 조회수 예측을 하는 모델 연구</strong>였다. <strong>그러한 콘셉트를 유지하고자 사후지표를 완전히 배제한 채 연구했다.</strong></p>\n<p>하지만, <strong>그게 좀 아쉬웠던 판단</strong>이었던 것 같다.</p>\n<p><strong>콘텐츠 업로드 후 섬네일 또는 제목이 변경된 콘텐츠들을 수집해서 이들을 깊이 분석해보는 것도 굉장히 피처 정의에 도움이 많이 됐을 것이라 생각했다.</strong></p>\n</li>\n</ol>\n<p>그리고, 유튜브 영상에서 사람들이 ’<strong>가장 많이 봤던 장면 부분</strong>‘과 ’<strong>적게 봤던 부분</strong>‘에 대해 프레임들을 가져와 분석해보면 어떨까 하는 생각도 있었다. (굉장히 많은 리소스가 들것으로 보이긴 하지만!)</p>\n<p> </p>\n<h3 id=\"다음-목표\" style=\"position:relative;\"><a href=\"#%EB%8B%A4%EC%9D%8C-%EB%AA%A9%ED%91%9C\" aria-label=\"다음 목표 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>다음 목표</h3>\n<p>이번 경험을 바탕으로 다음 단계를 생각해봤다.</p>\n<ul>\n<li>유튜브 크리에이터와 협력해서 <strong>더 많은 데이터</strong> 수집하기</li>\n<li><strong>업로드 후 섬네일/제목 변경으로 인한 A/B 테스트 가능한 콘텐츠를 분석</strong>해서 피처 추가 정의하기</li>\n<li>실제 크리에이터가 활용할 수 있는 <strong>웹 서비스</strong> 만들기</li>\n</ul>\n<p> </p>\n<p> </p>\n<h2 id=\"마무리이이\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EB%AC%B4%EB%A6%AC%EC%9D%B4%EC%9D%B4\" aria-label=\"마무리이이 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마무리이이</h2>\n<hr>\n<p>비록, 논문지 게재는 아닌 학회 논문 발표였지만 그 과정에서 얻은 경험이 충분히 너무나 값졌다.</p>\n<p>주제와 방향성을 정한 후, ‘논문 리서치 + 피처 정의 + 피처 추출 모델 선정 논의 + EDA + 모델링 + 검증’을 반복하는 과정이 정말 힘들었지만,</p>\n<p>내 안의 열정을 불태워서 그런지, 음… 행복하고 즐거웠다.😢</p>\n<p>특히, 마감 기한이 다가오지만 특정 피처 추출이 잘 진행되지 않을 때, 우리 팀원들 간에 ’<strong>현 상황을 진단하고 어떻게 대처할지에 대해 서로 열렬히 논의했던 그 소중한 경험</strong>‘이 떠오른다.</p>\n<p>그리고… 마음 같아서는 대학원 가서 더 깊이 연구도 하고 싶다. 재밌다. 끝.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0\">들어가며(?)</a></p>\n</li>\n<li>\n<p><a href=\"#%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EB%A5%BC-%EC%8B%9C%EC%9E%91%ED%95%9C-%EA%B3%84%EA%B8%B0\">프로젝트를 시작한 계기</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%82%AC%EC%9A%A9%ED%95%9C-%EA%B8%B0%EC%88%A0%EB%93%A4\">사용한 기술들</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B0%9C%ED%91%9C-%EC%A4%80%EB%B9%84-%EA%B3%BC%EC%A0%95\">발표 준비 과정</a></p>\n<ul>\n<li><a href=\"#%EB%B0%9C%ED%91%9C-%EC%9E%90%EB%A3%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0\">발표 자료 만들기</a></li>\n<li><a href=\"#%EB%B0%9C%ED%91%9C-%EC%97%B0%EC%8A%B5\">발표 연습</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B0%9C%ED%91%9C-%EB%8B%B9%EC%9D%BC\">발표 당일</a></p>\n<ul>\n<li><a href=\"#%EB%B0%9C%ED%91%9C-%ED%9B%84-%EC%A7%88%EC%9D%98%EC%9D%91%EB%8B%B5\">발표 후, 질의응답</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%8A%90%EB%82%80-%EC%A0%90\">느낀 점</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EC%9A%B4-%EC%A0%90--%EC%95%84%EC%89%AC%EC%9A%B4-%EC%A0%90\">배운 점 + 아쉬운 점</a></li>\n<li><a href=\"#%EB%8B%A4%EC%9D%8C-%EB%AA%A9%ED%91%9C\">다음 목표</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%A7%88%EB%AC%B4%EB%A6%AC%EC%9D%B4%EC%9D%B4\">마무리이이</a></p>\n</li>\n</ul>\n</div>","frontmatter":{"date":"July 04, 2025","title":"[ASAC 회고] 한국디지털콘텐츠학회 논문 발표 후기 (2025.07.03)","categories":"ASAC","author":"변우중","emoji":"📝"},"fields":{"slug":"/25-07-04_1/"}},"prev":{"id":"66d916a8-fe23-5bc2-a843-b9c1486dac94","html":"<p>참고 : 테디노트의 RAG 비법노트 (<a href=\"https://fastcampus.co.kr/data_online_teddy\">https://fastcampus.co.kr/data_online_teddy</a>)</p>\n<p>소스코드: <a href=\"https://github.com/teddylee777/langchain-kr\">https://github.com/teddylee777/langchain-kr</a></p>\n<p> </p>\n<p>지난 번 토큰화 관련해서 공부하고 블로그도 썼지만, 관련 내용 이해와 내면화 사이의 괴리가 있었다.<br>\n그런데, 갓 테디노트님 강의 들으면서 그 괴리를 좁혀나갈 수 있었던 것 같다^^&#x3C; 굳👍🏻</p>\n<p>(오늘 게시글은 진짜 두서없이 깨알 Tip 넣을 것임다… 너무 뜬금 없어도 이해부탁…)</p>\n<p> </p>\n<h2 id=\"서브워드-기반-토큰화-왜-하는-것인가\" style=\"position:relative;\"><a href=\"#%EC%84%9C%EB%B8%8C%EC%9B%8C%EB%93%9C-%EA%B8%B0%EB%B0%98-%ED%86%A0%ED%81%B0%ED%99%94-%EC%99%9C-%ED%95%98%EB%8A%94-%EA%B2%83%EC%9D%B8%EA%B0%80\" aria-label=\"서브워드 기반 토큰화 왜 하는 것인가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>서브워드 기반 토큰화 왜 하는 것인가?</h2>\n<hr>\n<p><strong>문자 기반 토큰화</strong>(한국어는 자모 단위로 토큰화)는 너무 쪼개니까 편집거리를 이용해 검색어 추천, 오타 교정 정도에 활용할 수 있는데,</p>\n<p><strong>텍스트 문장 생성에서는 너무 많이 분절해놓으니 이렇게 쪼갠 것을 모델이 학습하고 또 생성해내기가 어려워진다.(모델이 생성해야하는 묶음이 더 많아짐!!)<br>\n(우리한테 자모음들을 막 뿌려 주고, 텍스트 문장 만들라고 시켜보면 얼마나 힘든가…)</strong></p>\n<p>단어 기반 토큰화는 텍스트 문장 생성에 효과적일 수 있다지만,<br>지난 블로그 글에 남겼듯이 자주 사용하는 단어들이 있잖아..?(요.)</p>\n<p>고런 것들도 고려하려면 <strong>서브워드 기반 토큰화</strong>를 통해서 <strong>단어 사전 만드는 것</strong>이 좋지~~(요.)</p>\n<p>그. 이. 후. 그 토큰별로 벡터화 시키는 것이다!!! -> 그 하나가 1토큰임<br>-> 즉, <strong>토큰화 잘하면 토큰 사용량 줄일 수 있다</strong>!!!!</p>\n<p> </p>\n<h2 id=\"한국어-버전-토큰화-vs-영어-버전-토큰화\" style=\"position:relative;\"><a href=\"#%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%B2%84%EC%A0%84-%ED%86%A0%ED%81%B0%ED%99%94-vs-%EC%98%81%EC%96%B4-%EB%B2%84%EC%A0%84-%ED%86%A0%ED%81%B0%ED%99%94\" aria-label=\"한국어 버전 토큰화 vs 영어 버전 토큰화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>한국어 버전 토큰화 vs 영어 버전 토큰화</h2>\n<hr>\n<h3 id=\"한국어-버전\" style=\"position:relative;\"><a href=\"#%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%B2%84%EC%A0%84\" aria-label=\"한국어 버전 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>한국어 버전</h3>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 57.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAACHUlEQVQoz42TSU8bQRCF5/+fIgVErBgwhC0SipJbpOQQFMEBCzwe7xgbkjFeZvWMx559vqjbznYhKenpVZe6q0vvdSuapqGqKrVaDc/z+FcURSE5CALSNEUsRS3Pc8nK09MY07LQRyOm0xm2beM4DrZl4dg2i2Al4bpzVmFEkuYkaUYUxcRJulmva3leoPy8WdywWi7xfV/mf8YyTHD8JXGckSQxYZQRJhmRaBwnpHFCGKXynPLm5JyD43NKeyeUykeU908p7R2zXTrg8OQdrytvqdVbdPrf+dJqoDYvuOncUet26bWHdFWdW62J9liFHJRy5ZSXO/tslw7ZKR9xfPae3coZW6UDtl5VeLG1S/W2RrNxzderGpcXN1xcf+Kqeol69YHPapWPnQb1QQcKUFzHZjadYFsmxmyKaRrYtsncdSRc1ybLcmamiznxMMZzxmMHfWIzMoeYnkEURazCeK2hYZiMRiN0XUfXR0wmE+aeRygMSFLiOF4LXkAm3CyQkO7mQntIs0Ia85cpz0WSpnj+An+xxPMD/EXwiyX8gEWwXDcUzojkN29QrFm6vArpdO8kGs023V6fZquzqfVk3r8fCglRxGN8DiLCKOJ+8CAbiYMi7931abW79PsD2p0uw4dvcq/yP78iimM5RV1rbKZsoTVaqHVNNhP5YPi4bvg/E6Zphmk5TKYGhmFhWjaGaWM7LlGUSIfDKJEa/gA+4IDnaYFF5AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"img 1\" title=\"img 1\" src=\"/static/88295fffbab9d7c05b261c6581cca526/37523/img_1.png\" srcset=\"/static/88295fffbab9d7c05b261c6581cca526/e9ff0/img_1.png 180w,\n/static/88295fffbab9d7c05b261c6581cca526/f21e7/img_1.png 360w,\n/static/88295fffbab9d7c05b261c6581cca526/37523/img_1.png 720w,\n/static/88295fffbab9d7c05b261c6581cca526/302a4/img_1.png 1080w,\n/static/88295fffbab9d7c05b261c6581cca526/07a9c/img_1.png 1440w,\n/static/88295fffbab9d7c05b261c6581cca526/5ab15/img_1.png 2446w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<h3 id=\"영어-버전\" style=\"position:relative;\"><a href=\"#%EC%98%81%EC%96%B4-%EB%B2%84%EC%A0%84\" aria-label=\"영어 버전 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>영어 버전</h3>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 59.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAACIklEQVQoz4WTTWsTURSG57+KC61CUaQglS5cFNy5ceFSf4AIulCqRis1YiptMpmZzCSTpEmbz/nKZJLM5yNzp2mDSD3w8N47F86cc99zJVmWKZVKlMtlVFUljyRJrkjTVLCOLMuExnGM53nFeZaRphlJkiIFQcB8HjDzffr9c0ajEd5shut5OI6L7/ssViGOW+zDKLkkJorTjX2BtPn3PLlIsFiwGVGS4i+WeMGSMIxYLROWy4xVGLKMI1ZhQriKiKIIyWx3qas6umFycipTq6tUawpqw6DT7WO2z9BbXXr9CT9O+5yoP5G1dxw0DCrKMVq1iqq0qWgyQ6uL9OLla27decT9h3vc3d5lZ3efx3vP2Nre5d6DJ9ze2uHp/nMqSp23n79yePSRo2+f+FD6zqvjCgfl37z/dciXozd0u8dIeXuj4ZDxeCzuL2c6nWLb1pXmMXUsdMNgoNU5UwZ01AnjiYXrzjj3XC6csfBCsm0bXdfpdDq0Wi2azSamaQrt9Xpind9rmkHub7pBkhZkCeRWxLnL67HIxyET9qf/ZB4scD2fmT/HExrgzxdi788DwSqMkNZzVSS8nDWudX0+GI6pyQqapgut1upUazJyXRXk3yzbQRJDeRNpUbntuOhGC1XTMZomzVYbRdFo6IZYaw2DycRC4j+xrjBvs9Pt0WyZXAxGov2p5YiqLNstDPJmRcs3UySN4uvXkL+QnDjJ/iLlD/iHgTUfW3h+AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"img 2\" title=\"img 2\" src=\"/static/45bdf123ade94eda3c30734a1d02df1f/37523/img_2.png\" srcset=\"/static/45bdf123ade94eda3c30734a1d02df1f/e9ff0/img_2.png 180w,\n/static/45bdf123ade94eda3c30734a1d02df1f/f21e7/img_2.png 360w,\n/static/45bdf123ade94eda3c30734a1d02df1f/37523/img_2.png 720w,\n/static/45bdf123ade94eda3c30734a1d02df1f/302a4/img_2.png 1080w,\n/static/45bdf123ade94eda3c30734a1d02df1f/07a9c/img_2.png 1440w,\n/static/45bdf123ade94eda3c30734a1d02df1f/b0bb3/img_2.png 2344w\" sizes=\"(max-width: 720px) 100vw, 720px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n<p>그림에서 오른쪽 형관펜으로 칠한 만큼 각각 한 토큰이다. 같은 문장이지만, 영어 버전이 토큰이 적게 든다.<br>\n즉, 비용을 줄이긴 위해선 영어로 바꿔서 하는 것이 좋다.</p>\n<p>그리고, 모델에 한국어 서브워드를 학습시킨다면 더 좋을 수 있지 않겠는가,,,,<br>\n(학습 비용이 더 들 수는 있는데, LLM Post Training이 필요한 프로젝트가 있다면 반드시 고려해볼만 하다.)</p>\n<p>한국어 버전에서 확인할 수 있듯이 ‘안녕하세요’는 ‘안’+‘녕하세요’ 2개 토큰으로 인식하고 있다. ‘열심히’도 ‘열’ + ‘심’ + ‘히’ 3개 토큰으로 인식하고 있다.<br>\n(요즘은 예전보다는 gpt의 한국어 토큰 최적화 잘 되어 있다고 하지만, 어쨌든 토큰 최적화를 위해 이정도는 알아 두자,, 지금은 GPT-5.1까지 나왔는데 나중에 확인 ㄱㄱ)</p>\n<p>서브워드의 중요성 인식 완.</p>\n<p> </p>\n<blockquote>\n<h3 id=\"깨알-tip\" style=\"position:relative;\"><a href=\"#%EA%B9%A8%EC%95%8C-tip\" aria-label=\"깨알 tip permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>💡깨알 Tip</h3>\n<p>멀티모달 GPT 사용시 LangChain의 HumanMessage를 이용한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># HumanMessage 안에 이미지와 텍스트를 같이 넣는 방법</span>\n<span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> ChatOpenAI\n<span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>messages <span class=\"token keyword\">import</span> HumanMessage\n\n<span class=\"token comment\"># GPT-4o / GPT-4.1 등 멀티모달 지원 모델</span>\nllm <span class=\"token operator\">=</span> ChatOpenAI<span class=\"token punctuation\">(</span>\n    model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4o-mini\"</span><span class=\"token punctuation\">,</span> \n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 예시 이미지</span>\nimage_url <span class=\"token operator\">=</span> <span class=\"token string\">\"https://example.com/cat.png\"</span>\n\nmessages <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n    HumanMessage<span class=\"token punctuation\">(</span>\n        content<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>\n            <span class=\"token punctuation\">{</span><span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"text\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"text\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"이 이미지에 무엇이 보여?\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">{</span>\n                <span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"image_url\"</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">\"image_url\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"url\"</span><span class=\"token punctuation\">:</span> image_url<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span></code></pre></div>\n</blockquote>\n<p> </p>\n<h2 id=\"lcellangchain-expression-language\" style=\"position:relative;\"><a href=\"#lcellangchain-expression-language\" aria-label=\"lcellangchain expression language permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LCEL(LangChain Expression Language)</h2>\n<hr>\n<p>chain = prompt | model | output_parser<br>\n가장 기본적인 chain 형태이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> ChatOpenAI\n<span class=\"token comment\"># from langchain_teddynote.messages import stream_response  # teddynote 스트리밍 출력 (for문 사용하지 않고도 쉽게 출력되도록 되어 있음)</span>\n<span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>prompts <span class=\"token keyword\">import</span> PromptTemplate\n\nprompt <span class=\"token operator\">=</span> PromptTemplate<span class=\"token punctuation\">.</span>from_template<span class=\"token punctuation\">(</span><span class=\"token string\">\"{topic}에 대해 {how} 설명해주세요.\"</span><span class=\"token punctuation\">)</span>\noutput_parser <span class=\"token operator\">=</span> StrOutputParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> ChatOpenAI<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nchain <span class=\"token operator\">=</span> prompt <span class=\"token operator\">|</span> model <span class=\"token operator\">|</span> output_parser</code></pre></div>\n<p>chain을 출력해서 보면</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">PromptTemplate(input_variables=['how', 'topic'], input_types={}, partial_variables={}, template='{topic} 에 대해 {how} 설명해주세요.')\n| ChatOpenAI(client=&lt;openai.resources.chat.completions.completions.Completions object at 0x1751c8650>, async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x175156110>, root_client=&lt;openai.OpenAI object at 0x15ffe7b90>, root_async_client=&lt;openai.AsyncOpenAI object at 0x1751c9290>, model_name='gpt-4.1-nano', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n| StrOutputParser()</code></pre></div>\n<p>이렇게 연결됨을 알 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token builtin\">input</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"topic\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"인공지능 모델의 학습 원리\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"how\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"5살짜리 어린 아이에게도 이해할 수 있도록 쉽게\"</span><span class=\"token punctuation\">}</span>\n\nchain<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>input에서 딕셔너리 형태로 변수 값을 넣어주고, chain에 invoke() 함수에 던져주면 값을 한번에 생성해서 보여줍니다.<br>\n(참고1: 변수가 1개일 때는 딕셔너리가 아닌 문자열로 던져줘도 가능하다)<br>\n(참고2: stream() 함수에 넣어주고 for문으로 출력하면 실시간으로 출력 토큰을 출력가능하다)</p>\n<blockquote>\n<p>이때는 <strong>모든 변수의 값을 넣어주어야 오류가 나지 않는다</strong>.</p>\n</blockquote>\n<p>그리고, 출력 결과는 chain = prompt | model | output_parser 이므로<br>\n<strong>model에서 나온 ⭐️ output의 content !!!!⭐️ 부분</strong>을 string 형태로 파싱하여 나온다!!<br>\n(<strong>참고3: chain = prompt | model 의 결과는 model 객체가 나와서 content, response_metadata 등 클래스 속성들 모두 결과로 갖는다</strong>)</p>\n<p> </p>\n<blockquote>\n<h3 id=\"깨알-tip-1\" style=\"position:relative;\"><a href=\"#%EA%B9%A8%EC%95%8C-tip-1\" aria-label=\"깨알 tip 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>💡깨알 Tip</h3>\n<p>중괄호 {{}}를 겹쳐 쓰면 변수로 인식하지 않고 중괄호 2쌍으로 인식한다.<br>\n(f-string 방식은 중괄호 겹쳐 쓰면 중괄호 1쌍으로 인식하는데 prompt 템플릿에 넣으면 두쌍으로 인식됨… 왜 그런지는 이해가 안가요..)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">template <span class=\"token operator\">=</span> <span class=\"token string\">\"{{question}}과 같이 중괄호 겹쳐 쓰면 변수로 인식하지 않는다.\"</span>\ntemplate\n<span class=\"token comment\"># '{{question}}과 같이 중괄호 겹쳐 쓰면 변수로 인식하지 않는다.'</span></code></pre></div>\n</blockquote>\n<p> </p>\n<p>아, invoke() 출력과 stream() 출력 방법 비교도 해보자.</p>\n<h3 id=\"invoke-출력-답변-완성-후-출력\" style=\"position:relative;\"><a href=\"#invoke-%EC%B6%9C%EB%A0%A5-%EB%8B%B5%EB%B3%80-%EC%99%84%EC%84%B1-%ED%9B%84-%EC%B6%9C%EB%A0%A5\" aria-label=\"invoke 출력 답변 완성 후 출력 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>invoke() 출력: 답변 완성 후 출력</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">answer <span class=\"token operator\">=</span> llm<span class=\"token punctuation\">.</span>stream<span class=\"token punctuation\">(</span><span class=\"token string\">\"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3 id=\"stream-출력-실시간-출력\" style=\"position:relative;\"><a href=\"#stream-%EC%B6%9C%EB%A0%A5-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EC%B6%9C%EB%A0%A5\" aria-label=\"stream 출력 실시간 출력 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>stream() 출력: 실시간 출력</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">### 갓 테디노트님이 만드신 것 활용</span>\n<span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> ChatOpenAI\n<span class=\"token keyword\">from</span> langchain_teddynote<span class=\"token punctuation\">.</span>messages <span class=\"token keyword\">import</span> stream_response\n\n<span class=\"token comment\"># 객체 생성</span>\nllm <span class=\"token operator\">=</span> ChatOpenAI<span class=\"token punctuation\">(</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 창의성 (0.0 ~ 2.0)</span>\n    model_name<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4.1-nano\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 모델명</span>\n<span class=\"token punctuation\">)</span>\nanswer <span class=\"token operator\">=</span> llm<span class=\"token punctuation\">.</span>stream<span class=\"token punctuation\">(</span><span class=\"token string\">\"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 최종 결과물 토큰별 누적하면서 실시간 출력</span>\nfinal_ansewer2 <span class=\"token operator\">=</span> stream_response<span class=\"token punctuation\">(</span>answer<span class=\"token punctuation\">,</span> return_output<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">### 국룰 방법</span>\n<span class=\"token keyword\">from</span> langchain_openai <span class=\"token keyword\">import</span> ChatOpenAI\n\n<span class=\"token comment\"># 객체 생성</span>\nllm <span class=\"token operator\">=</span> ChatOpenAI<span class=\"token punctuation\">(</span>\n    temperature<span class=\"token operator\">=</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 창의성 (0.0 ~ 2.0)</span>\n    model_name<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4.1-nano\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 모델명</span>\n<span class=\"token punctuation\">)</span>\n\nanswer <span class=\"token operator\">=</span> llm<span class=\"token punctuation\">.</span>stream<span class=\"token punctuation\">(</span><span class=\"token string\">\"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 최종 결과물 토큰별 누적하면서 실시간 출력</span>\nfinal_answer <span class=\"token operator\">=</span> <span class=\"token string\">\"\"</span>  <span class=\"token comment\"># 최종 결과물 토큰 누적시킬 변수</span>\n<span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> answer<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># token.content: 각 토큰 별로 담긴 내용</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">,</span> end<span class=\"token operator\">=</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">,</span> flush<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    final_answer <span class=\"token operator\">+=</span> token<span class=\"token punctuation\">.</span>content</code></pre></div>\n<p><strong>.stream() 한다고 바로 호출되는 것이 아니고, for문을 통해 실시간으로 호출되면서 응답을 받는다.</strong></p>\n<p> </p>\n<p> </p>\n<p>일단 여기서 끝.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#%EC%84%9C%EB%B8%8C%EC%9B%8C%EB%93%9C-%EA%B8%B0%EB%B0%98-%ED%86%A0%ED%81%B0%ED%99%94-%EC%99%9C-%ED%95%98%EB%8A%94-%EA%B2%83%EC%9D%B8%EA%B0%80\">서브워드 기반 토큰화 왜 하는 것인가?</a></p>\n</li>\n<li>\n<p><a href=\"#%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%B2%84%EC%A0%84-%ED%86%A0%ED%81%B0%ED%99%94-vs-%EC%98%81%EC%96%B4-%EB%B2%84%EC%A0%84-%ED%86%A0%ED%81%B0%ED%99%94\">한국어 버전 토큰화 vs 영어 버전 토큰화</a></p>\n<ul>\n<li><a href=\"#%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%B2%84%EC%A0%84\">한국어 버전</a></li>\n<li><a href=\"#%EC%98%81%EC%96%B4-%EB%B2%84%EC%A0%84\">영어 버전</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#lcellangchain-expression-language\">LCEL(LangChain Expression Language)</a></p>\n<ul>\n<li><a href=\"#invoke-%EC%B6%9C%EB%A0%A5-%EB%8B%B5%EB%B3%80-%EC%99%84%EC%84%B1-%ED%9B%84-%EC%B6%9C%EB%A0%A5\">invoke() 출력: 답변 완성 후 출력</a></li>\n<li><a href=\"#stream-%EC%B6%9C%EB%A0%A5-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EC%B6%9C%EB%A0%A5\">stream() 출력: 실시간 출력</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"December 11, 2025","title":"[LLM] 테디노트의 RAG 비법노트 끄적끄적-1","categories":"NLP LLM","author":"변우중","emoji":"☀️"},"fields":{"slug":"/25-12-11_1/"}},"site":{"siteMetadata":{"siteUrl":"https://www.zoomkoding.com","comments":{"utterances":{"repo":"https://github.com/byeonwoojung/byeonwoojung.github.io"}}}}},"pageContext":{"slug":"/25-09-09_1/","nextSlug":"/25-07-04_1/","prevSlug":"/25-12-11_1/"}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}